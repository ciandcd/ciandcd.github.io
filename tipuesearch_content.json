{"pages":[{"title":"about","tags":"pages","url":"http://ciandcd.github.io/pages/about.html","text":"LOGO build 状态 关于ciandcd.com 本网站使用github pages技术维护，自动抓取国外最新的软件持续集成和持续发布相关文章。 投稿（目前不接受投稿，投稿可以直接到ciandcd.com） fork本网站源码:https://github.com/ciandcd/ciandcd.github.io.git 将你的文章放到content/category/下对应的子目录中, 如果是原创文章可以放到content/category/原创. 文章的格式可以为markdown，restructuredtext或html。 提交你的文章。 你也可以通过QQ群来提供相关的rss feed。 .rst格式 My super title \\############## :date: 2010-10-03 10:20 :modified: 2010-10-04 18:40 :tags: thats, awesome :category: yeah :slug: my-super-post :authors: Alexis Metaireau, Conan Doyle :summary: Short version for index and feeds This is the content of my super blog post. .md格式 Title : My super title Date : 2010 - 12 - 03 10 : 20 Modified : 2010 - 12 - 05 19 : 30 Category : Python Tags : pelican , publishing Slug : my - super - post Authors : Alexis Metaireau , Conan Doyle Summary : Short version for index and feeds This is the content of my super blog post . .html格式 <html> <head> <title> My super title </title> <meta name= \"tags\" content= \"thats, awesome\" /> <meta name= \"date\" content= \"2012-07-09 22:28\" /> <meta name= \"modified\" content= \"2012-07-10 20:14\" /> <meta name= \"category\" content= \"yeah\" /> <meta name= \"authors\" content= \"Alexis Métaireau, Conan Doyle\" /> <meta name= \"summary\" content= \"Short version for index and feeds\" /> </head> <body> This is the content of my super blog post. </body> </html> 联系 email: itech001@126.com QQ群：172758282 567940397 567931165 感谢 https://www.python.org/ https://pythonhosted.org/feedparser/index.html https://github.com/codelucas/newspaper https://github.com/getpelican/pelican http://twitter.github.com/bootstrap 开发环境 prod环境请参考.travis.yml. 本地测试环境 make clean && make html make devserver, or 'cd output && python -m http.server 8000'. 自动deploy 所有的修改会被trivs CI自动build和deploy到github pages. 问题 file issues. pull requests. License MIT License for this source code Creative Commons Attribution 3.0 Unported License for the content of www.ciandcd.com which includes all blogs, pages and articles"},{"title":"Awesome","tags":"pages","url":"http://ciandcd.github.io/pages/awesome.html","text":"ciandcd Continuous Integration and Continuous Delivery Written by ciandcd.com A curated list of awesome tools for: continuous integration continuous delivery software integration devops ciandcd Theory Build And Release System Online Build System Infrastructure dev env Source Code Management Code Review Build Static Check Dynamic Check Performance Analysis Coverage Testing Package Deploy Delivery Provisioning Tools Web Server Applications And Container OS And Shell Database Version Control Useful Sites Conference And Submit Other Awesome Lists Contact Theory The theory for continuous integration and continuous deliver continuousIntegration continuousdelivery software integration devopsdays * ci cheatsheet Build And Release System The system for software build and release, continuous integration and continuous delivery Jenkins An extendable open source continuous integration server BuildForge Automate and accelerate build and release processes ElectricCommander ElectricCommander gives distributed teams shared control and visibility into infrastructure, tool chains and processes. It accelerates and automates the software delivery process to enable agility, predictability and security across many build-test-deploy pipelines Teamcity Ready to work, extensible and developer-friendly build server out of the box bamboo Bamboo does more than just run builds and tests. It connects issues, commits, test results, and deploys so the whole picture is available to your entire product team go Automate and streamline the build-test-release cycle for worry-free, continuous delivery of your product hudson the previous one of Jenkins openbuildservice The Open Build Service (OBS) is a generic system to build and distribute binary packages from sources in an automatic, consistent and reproducible way. You can release packages as well as updates, add-ons, appliances and entire distributions for a wide range of operating systems and hardware architectures buildbot Buildbot is a continuous integration system designed to automate the build/test cycle. By automatically rebuilding and testing the tree each time something has changed, build problems are pinpointed quickly, before other developers are inconvenienced by the failure Parabuild Parabuild is an enterprise software build and release management system that helps software teams to release on time by providing them practically unbreakable release builds and Continuous Integration FinalBuilder Automating your Build process is simple with FinalBuilder. With FinalBuilder you don't need to edit xml, or write scripts. Visually define and debug your build scripts, then schedule them with windows scheduler, or integrate them with Continua CI, Jenkins or any other CI Server VisualBuild Visual Build enables developers and build masters to easily create an automated, repeatable build process cruisecontrol CruiseControl.NET, an Automated Continuous Integration server, implemented using the .NET Framework continuum Apache Continuum™ is an enterprise-ready continuous integration server with features such as automated builds, release management, role-based security, and integration with popular build tools and source control management systems quickbuild GitHub integration. Perforce shelve support. Coverity report rendering. Subversion external change retrieval. Resource access info. Display reasons for waiting steps. Custom build and request columns. Favorite dash board list. Inheritable environment variables.And much more... rexify perl Deployment & Configuration Management Online Build System Online build release system travis-ci ci server for github and bitbuckets cloudbees the Enterprise Jenkins Company elasticbox A DevOps approach that focuses on reusable application components as a service, and enables operations to provide IT as a Service coveralls Track your project's code coverage over time, changes to files, and badge your GitHub repo shippable Hosted continuous integration and deployment service built on docker circleci Continuous Integration for web apps. buildbox Simple self-hosted Continuous Integration drone Open source continuous integration platform built on Docker appveyor Continuous Integration and Deployment service for busy Windows snap-ci Easy builds, deployed when you want codeship Continuous Integration and Delivery made simple solanolabs Hosted continuous integration and deployment githost Painless GitLab CE & CI Hosting testling Automatic browser tests on every push magnum-ci Hosted Continuous Integration and Delivery Platform for private repositories wercker Test and deploy your applications with ease coveralls Track your project's code coverage over time, changes to files, and badge your GitHub repo ship.io Simple, powerful CI for iOS and Android. Re-build, Re-test, Re-deploy. GitLab CI - Based off of ruby. They also provide GitLab, which manages git repositories. IBM DevOps Services - Develop, track, plan, and deploy software onto the IBM Bluemix cloud platform. Infrastructure The hardware,virtual machines, fram management, docker GridWiki wiki page for Grid UGE Univa workload management solutions maximize the value of existing computing resources by efficiently sharing workloads across thousands of servers SGE Grid Engine is typically used on a computer farm or high-performance computing (HPC) cluster and is responsible for accepting, scheduling, dispatching, and managing the remote and distributed execution of large numbers of standalone, parallel or interactive user jobs. It also manages and schedules the allocation of distributed resources such as processors, memory, disk space, and software licenses LSF Platform Load Sharing Facility (or simply LSF) is a workload management platform, job scheduler, for distributed HPC environments. It can be used to execute batch jobs on networked Unix and Windows systems on many different architectures vmwarevshpere VMware vSphere (formerly VMware Infrastructure 4) is VMware's cloud computing virtualization operating system ctrixserver XenServer is the best server virtualization platform for public and private clouds, powering 4 of the 5 largest hosting provider clouds. Built with scale, security and multi-tenancy in mind, XenServer allows for even greater flexibility and cost efficiency miscrosofthyperv microsoft virtualization amazon Scalable, pay-as-you-go compute capacity in the cloud Dev env boxstarter Repeatable, reboot resilient windows environment installations made easy using Chocolatey packages. vagrantup Create and configure lightweight, reproducible, and portable development environments. veewee Easing the building of vagrant boxes Source Code Management Version control and source code management tools git Git is a free and open source distributed version control system designed to handle everything from small to very large projects with speed and efficiency perforce Industry's most reliable and proven platform for versioning code, images, documents... everything clearcase IBM Rational ClearCase is a software configuration management solution that provides version control, workspace management, parallel development support, and build auditing mercurial Mercurial is a free, distributed source control management tool. It efficiently handles projects of any size and offers an easy and intuitive interface svn Subversion is an open source version control system gitlab Open source self-hosted Git management software github Powerful collaboration, review, and code management for open source and private development projects. bitbuckets Plant your code in the cloud. Watch it grow. teamfoundationservice Visual Studio Online, based on the capabilities of Team Foundation Server with additional cloud services, is the online home for your development projects. Get up and running in minutes on our cloud infrastructure without having to install or configure a single server. Visual Studio Online connects to Visual Studio, Eclipse, Xcode, and other Git clients to support development for a variety of platforms and languages phabricator Phabricator is a collection of open source web applications that help software companies build better software. * IBM DevOps Services - Store, manage, edit, and collaborate on your source code. Then deploy onto the IBM Bluemix cloud platform. Code Review Code review tools codecollaborator Collaborator helps development, testing and management teams work together to produce high quality code crucible Code reviews = quality code. Review code, discuss changes, share knowledge, and identify defects with Crucible's flexible review workflow. It's code review made easy for Subversion, CVS, Perforce, and more reviewboard Review Board takes the pain out of code review, saving you time, money, and sanity so you can focus on making great software codestriker Codestriker is an open-sourced web application which supports online code reviewing. Traditional document reviews are supported, as well as reviewing diffs generated by an SCM (Source Code Management) system and plain unidiff patches getbarkeep a fast, fun way to review code gerrit Gerrit is a web based code review system, facilitating online code reviews for projects using the Git version control system * Codebrag Codebrag is a simple code review tool that makes the process work for your team. Build Build tools gnumake GNU Make is a tool which controls the generation of executables and other non-source files of a program from the program's source files gnuautoconf Autoconf is an extensible package of M4 macros that produce shell scripts to automatically configure software source code packages mozillabuildtools The Mozilla build system, like the rest of the Mozilla codebase, is cross-platform. It uses traditional Unix-style autoconf and make tools to build the various applications (even on non-unix operating systems) scons SCons is an Open Source software construction tool—that is, a next-generation build tool. Think of SCons as an improved, cross-platform substitute for the classic Make utility with integrated functionality similar to autoconf/automake and compiler caches such as ccache. In short, SCons is an easier, more reliable and faster way to build software cmake cmake offers robust, cross-platform software development solutions. Find out how we can help your team efficiently manage the build, test, and package process for your software project msbuild The Microsoft Build Engine is a platform for building applications. This engine, which is also known as MSBuild, provides an XML schema for a project file that controls how the build platform processes and builds software. Visual Studio uses MSBuild, but it doesn't depend on Visual Studio. By invoking msbuild.exe on your project or solution file, you can orchestrate and build products in environments where Visual Studio isn't installed ant Ant can be used to pilot any type of process which can be described in terms of targets and tasks. The main known usage of Ant is the build of Java applications. maven Apache Maven is a software project management and comprehension tool. Based on the concept of a project object model (POM), Maven can manage a project's build, reporting and documentation from a central piece of information. gradle Gradle is build automation evolved. Gradle can automate the building, testing, publishing, deployment and more of software packages or other types of projects such as generated static websites, generated documentation or indeed anything else. ElectricacCelerator Continuous Delivery isn't continuous if builds and tests take too long to complete. ElectricAccelerator speeds up builds and tests by up to 20X, improving software time to market, infrastructure utilization and developer productivity Static Check Software static check tools static tools wiki wiki page coverity Continually measure and improve code quality and security across your development organization fxcop FxCop is an application that analyzes managed code assemblies (code that targets the .NET Framework common language runtime) and reports information about the assemblies, such as possible design, localization, performance, and security improvements cpd Duplicate code can be hard to find, especially in a large project. But PMD's Copy/Paste Detector (CPD) can find it for you sonar SonarQube is an open platform to manage code quality findbugs Find Bugs in Java Programs * checkstyle Checkstyle is a development tool to help programmers write Java code that adheres to a coding standard Dynamic Check Software dynamic check tools * dynamic tools wiki wiki page Performance Analysis Software performance analysis tools * performance tools wiki wiki page Coverage Software testing coverage tools * code coverage wiki wiki page Testing Software testing framework and tools Testingautomation test automation wiki page softwareqatest FAQ page qaforums SQA Forums opensourcetesting open source software testing tools, news and disccussions * selenium Selenium automates browsers Package The tools for software package and installation installshield World's #1 Software Installation Solution-Build Reliable MSI Installers for Windows Applications NSIS NSIS (Nullsoft Scriptable Install System) is a professional open source system to create Windows installers. It is designed to be as small and flexible as possible and is therefore very suitable for internet distribution rpm The RPM Package Manager (RPM) is a powerful command line driven package management system capable of installing, uninstalling, verifying, querying, and updating computer software packages yum Yum is an automatic updater and package installer/remover for rpm systems fpm Effing package management! Build packages for multiple platforms (deb, rpm, etc) with great ease and sanity. wix The most powerful set of tools available to create your Windows installation experience. * packer Packer is a tool for creating identical machine images for multiple platforms from a single source configuration. Deploy The tools for web site deploy jfrog s the first Binary Repository Management solution, Artifactory has changed the way binaries are controlled, stored and managed throughout the software release cycle xl-deploy Agentless, Model-based App Deployment Jenkinsdeployplugin deploy to tomcat bintray The fastest and most reliable way to automate the distribution of your software releases Delivery The tools for software delivery sl-release Orchestrate your Continuous Delivery pipelines. Simple. Flexible. End-to-End archiva Apache Archiva™ is an extensible repository management software that helps taking care of your own personal or enterprise-wide build artifact repository. It is the perfect companion for build tools such as Maven, Continuum, and ANT nexus The use of repository managers (also known as component managers) is helping software development teams achieve simple gains in speed, efficiency, and quality of their operations chocolatey Chocolatey NuGet is a Machine Package Manager, somewhat like apt-get, but built with Windows in mind pulp Pulp is a platform for managing repositories of content, such as software packages, and pushing that content out to large numbers of consumers. herd A single-command bittorrent distribution system, based on Twitter's Murder * murder Large scale server deploys using BitTorrent and the BitTornado library from twitter.com Provisioning Tools Provision tools Puppet Build, destroy and rebuild servers on any public or private cloud Chef Fast, scalable and flexible software for data center automation SaltStack Radically simple configuration-management, application deployment, task-execution, and multi-node orchestration engine ansible Web Server Common used web server apache Apache httpd has been the most popular web server on the Internet since April 1996 nginx A high performance free open source web server powering busiest sites on the Internet tomcat An open source software implementation of the Java Servlet and JavaServer Pages technologies jetty Jetty provides a Web server and javax.servlet container, plus support for SPDY, WebSocket, OSGi, JMX, JNDI, JAAS and many other integrations * HAProxy - Software based load Balancing, SSL offloading and performance optimization, compression, and general web routing. OS And Shell Linux shell, perl, python awesome-shell awesome-python awesome-perl awesome-sysadmin Applications And Container VM application and container docker Docker - An open platform for distributed applications for developers and sysadmins suseapplication tools to create suse applications Database Version Control Database version control system liquibase source control for your database flywaydb Database Migrations Made Easy nextep NeXtep Softwares provides software solutions for the industrialization of your database developments and deployments. Our goal is to increase the productivity of your development teams by taking control of your developments' lifecycle and by automating your deployment and test processes dbdeploy dbdeploy is a Database Change Management tool. It's for developers or DBAs who want to evolve their database design – or refactor their database – in a simple, controlled, flexible and frequent manner * dbmaestro Controlled Database Continuous Delivery is Our Business Useful Sites Other useful pages infoq stackoverflow Conference And Submit Conference and submit * devops submit Other Awesome Lists Other amazingly awesome lists can be found in awesome awesome-awesome awesome-awesomeness sysadmin Contact To add new items about continuous integration and continuous delivery: 1) pull; 2) add issue; 3) send me by email itech001@126.com; 4) qq group 172758282;"},{"title":"A closer look at Europe · GitHub","tags":"scm","url":"http://ciandcd.github.io/a-closer-look-at-europe-github.html","text":"From: https://github.com/blog/2023-a-closer-look-at-europe Last week we opened our first international office in Japan . This week we thought we'd take a closer look at Europe, which happens to be the largest demographic of GitHub users around the world, representing 36% of site traffic. Around 32 million people visit GitHub each month, and most of this traffic comes from outside of the United States (74% in fact!). The most active countries in Europe are Germany, the United Kingdom, and France, but if we look at users per capita we see a different story -- Sweden, Finland, and the Netherlands lead the way. London, Paris and Stockholm top the list of European cities most active on GitHub. The goals of building better software are universal, and several European organizations are setting the example. Companies like SAP and XS4ALL are driving innovation with software, while The UK Government Digital Services and dozens of other European government agencies and services are developing new ways to serve citizens. Today, around 10% of GitHub employees are based in Europe, with a dozen new faces in the last year alone -- many of whom are focused solely on helping our European customers build great software. A few of us are here in the UK for London Tech Week and EnterConf in Belfast. There will be plenty more meetups ahead if we don't see you there."},{"title":"A new GitLab Logo","tags":"scm","url":"http://ciandcd.github.io/a-new-gitlab-logo.html","text":"From: https://www.gitlab.com/2015/05/18/a-new-gitlab-logo/ A new GitLab Logo We hear you: Gitlab seems like a cool service, but my god that logo is scary — Matt Bachmann (@MattBachmann) March 11, 2015 We have a scary, angry looking raccoon dog logo. this creepy human/racoon hybrid that is the @gitlab logo is starting to really freak me out pic.twitter.com/HJarlbRNOo — hatewell (@hatwell) January 16, 2015 We figured we could use a better representation of GitLab. Update May 20th: After careful consideration we have decided that this is our new logo: We like the way it looks in GitLab: And compared to the old logo: The options we didn't pick: If you have a better suggestion than one of the ones above, a certain preference or opinion, we'd love to hear it. The final choice of our new logo rests with Dmitriy . Install GitLab on your own server in 2 minutes Let's do it! Install GitLab on your own server in 2 minutes → Browse all posts @gitlab on Twitter. For the latest and most detailed news followon Twitter. Get every GitLab blog post and stay up to date. Please enable JavaScript to view the <a href=\"http://disqus.com/?ref_noscript\">comments powered by Disqus.</a>"},{"title":"An updated header, just for you · GitHub","tags":"scm","url":"http://ciandcd.github.io/an-updated-header-just-for-you-github.html","text":"From: https://github.com/blog/2022-an-updated-header-just-for-you Navigating what's most important to you on GitHub.com just got a little easier with our updated site header. The new header gives you faster access to your pull requests and issues dashboards from anywhere on the site. If you're unfamiliar with them, these dashboards list all of your open pull requests and issues—as well as those you've been mentioned in or are assigned to—in one place. Use them to stay up to date on what needs to be done across your projects. Lastly, clicking your avatar now opens a new dropdown menu with links to your profile, account settings, and more. As a small bonus, we've also included a new Your stars link for easy access to your starred repositories. Enjoy!"},{"title":"Announcing Atom 1.0 · GitHub","tags":"scm","url":"http://ciandcd.github.io/announcing-atom-10-github.html","text":"From: https://github.com/blog/2031-announcing-atom-1-0 GitHub is pleased to announce that version 1.0 of the Atom text editor is now available from atom.io . Read the full behind the scenes story over on the Atom blog . The entire Atom team is attending CodeConf this week and will be presenting a session all about Atom 1.0 featuring Chris Wanstrath , Ben Ogle , and Daniel Hengeveld . Watch along tomorrow, June 26th, at 11AM EDT: https://live-stream.github.com"},{"title":"Announcing GitHub Japan · GitHub","tags":"scm","url":"http://ciandcd.github.io/announcing-github-japan-github.html","text":"From: https://github.com/blog/2017-announcing-github-japan GitHub <3s Japan, and today we're excited to announce the formation of GitHub Japan G.K., a subsidiary of GitHub, Inc. Our new office in Tokyo is our first official office outside of the United States. The Japanese developer community GitHub couldn't exist without the Japanese open source community — after all, our site is built on Rails , which is built on Ruby , an open source project started in Japan . Japan has historically been one of the most active countries on GitHub, ranking in the top 10 countries visiting github.com since GitHub was founded in 2008. The thriving software community in Japan keeps growing; in 2014, activity on github.com from Japan increased more than 60 percent from the previous year. GitHub Enterprise in Japan In addition to an active local open source community, Japanese businesses including Hitachi Systems , CyberAgent and GREE are collaborating and building the best software with GitHub Enterprise. To that end, we're also announcing that we'll be partnering locally to provide Japanese language technical support for GitHub Enterprise users, as well as the ability to pay in Japanese Yen in Japan. Stay up to date Keep tabs on everything happening in our Tokyo office by following @GitHubJapan on Twitter and checking out github.co.jp . We'd also love to see you at our meetup in Osaka on June 6 . Yoroshiku-Onegaiitashimasu! 初めましてGitHub Japanです！ GitHub <3s Japan, 本日、私達はGitHub, Inc.の子会社である、GitHub Japan合同会社の設立の発表ができる事をとても光栄に思っております 。東京にオープンした新しいオフィスは、米国外でオープンする初のオフィスになります。 〜日本のデベロッパー・コミュニティにむけて〜 GitHubは、日本で生まれたオープンソース・プロジェクトのRubyで作られたRailsというフレームワークによって開発されており、日本のオープンソース・コミュニティーなしではGitHubは存在しえないと言っては過言ではない程、日本とGitHubは深いつながりがあります。 また、2008年のGitHub設立当初から、日本からgithub.comへのアクセス数は上位10ヶ国に入り続けてきました。そして、日本のユーザーは現在も増加し続けており、2014年の日本ユーザーのGitHub上でのアクティビティは、前年比60％も増加しました。 ～「GitHub Enterprise」の日本展開～ GitHubは広く開かれた開発を支援するオープンソース・プラットフォーム以外にも、全世界で企業向けに「GitHub Enterprise」を提供して参りました。これまで「GitHub Enterprise」は、英語でのサポートのみだったにもかかわらず、日本国内では、 株式会社日立システムズ 、 ヤフー株式会社 、 株式会社サイバーエージェント や グリー株式会社 などの大手企業をはじめとして、多くの先進的な企業にご活用頂いて参りました。そして今回、さらに迅速できめ細かいサービスやサポートを提供するため、GitHubは大手代理店と業務提携を行い、日本語による「GitHub Enterprise」の法人向け導入サポートも開始しました。この販売パートナー提携により、円建て決済や日本語のテクニカルサポートも可能になります。 GitHub の最新の情報を得よう 東京オフィスで何が起こっているか知る為にはTwitterで @GitHubJapan をフォローするか、 github.co.jp にアクセスしてくださいね。そして 大阪で開催されるuser meetup にも是非お越しください！ お待ちしております！. よろしくお願い致します！"},{"title":"Atom at CodeConf 2015 · GitHub","tags":"scm","url":"http://ciandcd.github.io/atom-at-codeconf-2015-github.html","text":"From: https://github.com/blog/2018-atom-at-codeconf-2015 CodeConf is coming June 25 & 26 to Nashville and will feature the best that the open source community has to offer. We're excited to share that there will be several talks about the Atom ecosystem presented for your enjoyment and edification, kicked off by GitHub CEO Chris Wanstrath . Speakers will include: @bolinfest talking about Nuclide @lee-dohm talking about the Atom community @paulcbetts talking about Slack and Electron We will also be hosting an Atom workshop led by Nathan Sobo , and a lounge where you will be able to meet with the core team and hack on Atom together. Grab your CodeConf and workshop tickets now and we'll see you there in Nashville!"},{"title":"Changes to Enterprise Edition subscription pricing","tags":"scm","url":"http://ciandcd.github.io/changes-to-enterprise-edition-subscription-pricing.html","text":"From: https://www.gitlab.com/2015/06/12/price_changes/ Changes to Enterprise Edition subscription pricing Today we are announcing two changes to GitLab Enterprise Edition subscription pricing. The changes are intended to better reflect the value of each offering and ensure our subscription options cater to the needs of different organizations. In short, our basic subscription is now $19,10 more expensive, but in 10-user packs. Our Plus subscription is now $100 more affordable. Standard and terms remain unchanged. As of today (June 12, 2015) the following will take affect: Basic Subscriptions will cost $390 per year for a 10-user pack ($39 per user / per year). Current Basic Subscribers will be offered a 25% discount on this new pricing at their next renewal. However, new pricing will apply to subsequent renewals and any additional user packs. Basic subscriptions are now available in 10-user packs, making it slightly more affordable for small teams. Plus Subscriptions will cost $14,900 for a 100-user pack ($149 per user / per year). Current Plus subscribers will receive a prorated refund on the price differece. There are no changes in the software features or service level of Basic or Plus subscriptions, which you can view on our website here . Standard Subscription pricing will also remain unchanged at $4,900 per year for each 100-user pack ($49 per user / per year). All current quotes will be honored until their expiration (60 days from issue date) but the new pricing will apply to any subsequent orders, including renewals. Our goal is to keep GitLab the most affordable enterprise grade development platform available. These changes should not have any significant effect on our ability to achieve that. We felt our Basic plan was underpriced and Plus plan was overpriced. These changes reduce the price difference between them. If you have questions about the changes or about pricing in general, please contact our sales team at sales@gitlab.com. Install GitLab on your own server in 2 minutes Let's do it! Install GitLab on your own server in 2 minutes → Browse all posts @gitlab on Twitter. For the latest and most detailed news followon Twitter. Get every GitLab blog post and stay up to date. Please enable JavaScript to view the <a href=\"http://disqus.com/?ref_noscript\">comments powered by Disqus.</a>"},{"title":"Did you install GitLab from source? Check your Git version","tags":"scm","url":"http://ciandcd.github.io/did-you-install-gitlab-from-source-check-your-git-version.html","text":"From: https://www.gitlab.com/2015/06/12/did-you-install-gitlab-from-source-recently-check-your-git-version/ Did you install GitLab from source? Check your Git version Although the preferred way to install GitLab is to use our omnibus packages , you can also install GitLab Community Edition or Enterprise Edition ‘from source'. If you used this installation method, and if you compiled Git from source in the process then please check whether your Git version defends against Git vulnerability CVE-2014-9390. This issue does not apply to our Omnibus packages (DEB or RPM). Although GitLab itself is not affected by CVE-2014-9390, a GitLab server may be used to deliver ‘poisoned' Git repositories to users on vulnerable systems. Upgrading Git on your GitLab server stops users from pushing poisoned repositories to your GitLab server. Due to an oversight, the guide for installing GitLab from source still contained instructions telling administrators to install Git 2.1.2 if the version of Git provided by their Linux distribution was too old. Git 2.1.2 does not defend against CVE-2014-9390. If your GitLab server uses /usr/local/bin/git please check your Git version using the instructions in this upgrade guide . Install GitLab on your own server in 2 minutes Let's do it! Install GitLab on your own server in 2 minutes → Browse all posts @gitlab on Twitter. For the latest and most detailed news followon Twitter. Get every GitLab blog post and stay up to date. Please enable JavaScript to view the <a href=\"http://disqus.com/?ref_noscript\">comments powered by Disqus.</a>"},{"title":"Feature Highlight: Approve Merge Request","tags":"scm","url":"http://ciandcd.github.io/feature-highlight-approve-merge-request.html","text":"From: https://www.gitlab.com/2015/06/16/feature-highlight-approve-merge-request/ Feature Highlight: Approve Merge Request With less than a week until GitLab 7.12, we've got a nice preview for you today: Merge Request Approvals in GitLab EE. Usually you accept a merge request the moment it is ready and reviewed. But in some cases you want to make sure that every merge request is reviewed and signed off by several people before merging it. With GitLab Enterprise Edition 7.12, you can enforce such a workflow that requires multiple reviewers with the new Merge Request Approval feature. To enable approvals, go to project settings page and set the \"Approvals required\" field to a numeric value. For example, if you set it to 3 each merge request has to receive 3 approvals from different people before it can be merged through the user interface. After setting the approval, you will see an Approve button on merge requests, rather than an Accept button. Once the merge request has enough approvals, you will be able to merge it as usual. We'd love to hear what you think of this new feature in the comments below. Install GitLab on your own server in 2 minutes Let's do it! Install GitLab on your own server in 2 minutes → Browse all posts @gitlab on Twitter. For the latest and most detailed news followon Twitter. Get every GitLab blog post and stay up to date. Please enable JavaScript to view the <a href=\"http://disqus.com/?ref_noscript\">comments powered by Disqus.</a>"},{"title":"Filter Pull Requests by Status · GitHub","tags":"scm","url":"http://ciandcd.github.io/filter-pull-requests-by-status-github.html","text":"From: https://github.com/blog/2014-filter-pull-requests-by-status When we shipped the new GitHub Issues , we made it easy to scope lists of Issues and Pull Requests with filters like author, date, mentions, and team mentions. With the new status: filter you can now filter the Pull Requests in your repositories by combined status . If you're taking advantage of the Status API , or using an integration that does, try out the new filters: status:success Only pull requests with all successful statuses status:failure Only pull requests that have statuses in the failure or error state status:pending Only pull requests with no statuses or at least one status in the pending state"},{"title":"Finding the ‘Needle in a Haystack' with Helix Threat Detection","tags":"scm","url":"http://ciandcd.github.io/finding-the-needle-in-a-haystack-with-helix-threat-detection.html","text":"From: http://www.perforce.com/blog/150617/finding-%E2%80%98needle-haystack%E2%80%99-helix-threat-detection Software development projects in bigger companies typically involve large teams collaborating across multiple locations. A large corporation may employ tens of thousands of developers working on thousands of projects over a span of many years. For many companies, developer access to older software projects and files may continue long after the project has been completed, sometimes because of lax processes and stagnant access control policies. Yet, these projects can represent valuable IP worth tens of millions of dollars. In light of the ramifications of a competitor getting ahold of these files, what can companies do to better protect their crown jewels from theft? The answer might be found in the source code management (SCM) or version control tools companies use to drive their development workflows. SCM tools typically track access to key projects and files via audit logs. However, the sheer volume of these logs can overwhelm security teams. A month of log data might yield millions of different interactions with files and projects, making it virtually impossibe to find important clues. Done the right way, however, this approach can bring the real threats to the surface. A recent Fortune article entitled Using Log Data and Machine Learning to Weed out the Bad Guys shares how a large company applied our Helix Threat Detection capabilities to quickly identify data theft. Likening this approach to ‘finding a needle in a haystack,' the article describes how effective it can be to apply behavioral analytics to the audit logs in our Helix Versioning Engine. Leveraging Machine Learning to Establish a Baseline Conventional security tools (e.g., SIEMs) are often rule-based and require time-consuming manual setting of thresholds and iterative tuning of multiple parameters in order to identify anomalous behavior. Yet manually setting alerts to trigger when developers access an arbitrary number of files may be problematic for large projects and can inundate security teams with too many false positives. A better approach is to use machine-learning algorithms and risk-based-behavior-analytics models to audit logs to first establish a baseline understanding of normal behavior. It's possible to create cluster models that group similar users based on their past activities. Continuous self-learning more accurately identifies high-risk events, like someone accessing a project he or she doesn't normally work on, putting a spotlight on threats to an organization's most sensitive assets. Identifying High-Risk Behaviors Once you've establised what's normal behavior, the next step is to apply advanced mathematical models that generate a behavioral risk score. This score represents multiple factors, including the importance of an asset or file, the method of access, the activity (e.g., volume or type), and the user. These behavioral analytics models can then be used to find anomalies by: Comparing access patterns, data usage patterns and data movement patterns against historic behavior Determining similar user patterns across the environment and comparing behavioral patterns between users and groups of users Detecting dissimilar patterns among members of the same project group or job role Comparing individuals against the entire user group To learn more about the behavioral analytics models used in Helix Threat Detection, download the white paper Helix Threat Detection: IP Security and Risk Analytics. To learn more download our white paper: A Unified Approach to Securing and Protecting IP. READ NOW"},{"title":"Focus on your changes in GitHub for Windows · GitHub","tags":"scm","url":"http://ciandcd.github.io/focus-on-your-changes-in-github-for-windows-github.html","text":"From: https://github.com/blog/2015-focus-on-your-changes-in-github-for-windows GitHub for Windows now makes it even easier to see everything local to your machine, whether it's uncommitted changes or commits you haven't synced yet. One of the things you'll notice when creating commits is the new, compact list of changed files in your working directory. GitHub for Windows shows the number of files that a commit changed and lets you drill down to see what changed in a given file. The updated branch selector now groups your recently used branches so that you can jump straight back in to what you were doing before that pesky hotfix distracted you. We've given branch creation a dedicated place in the toolbar. As a bonus, you can pick which branch to base the new one off. Finally, you can collapse the repository list to reclaim some screen space. If you have GitHub for Windows installed it will automatically update to the latest version. If you don't have it installed, download GitHub for Windows from windows.github.com ."},{"title":"GitLab 7.10.4 released","tags":"scm","url":"http://ciandcd.github.io/gitlab-7104-released.html","text":"From: https://www.gitlab.com/2015/05/11/gitlab-7-dot-10-dot-4-released/ GitLab 7.10.4 released Last week we had to pull our 7.10.2 release as in a small number of installations the migrations would fail because of a uniqueness constraint on tags. We did not release GitLab 7.10.3, as we improved a migration after creating the 7.10.3 version tag and wanted to include that in our patch release. Today we release GitLab 7.10.4 which solves the issues with the migrations and contains all fixes also present in 7.10.2. If you've already successfully upgraded to 7.10.2, you do not need to update at this time. The fixes in this patch: Fix migrations broken in 7.10.2 Add missing indices to tags for some installations Make tags for GitLab installations running on MySQL case sensitive And the following were fixed with 7.10.2, also included here: A bug when using the Gitorious importer A bug that prevented adding group members through the admin screen Broken links on the merge request page leading to CI services A 500 error when trying to search in the wiki A 500 error when trying to add new tags to a project A bug where commit data would not appear in some subdirectories due to escaped slashes A bug where branches with escaped characters in their names would not always work in the compare view Upgrade barometer There is a migration that loops through all tags. This can take a while for larger installations. The upgrade can be performed online. Theoretically, there is a small chance that if a tag is created during the migration of that specific tag, the tag counter gets a value that is slightly higher or lower than its actual value. We do not believe this is reason to schedule downtime and recommend performing the upgrade online. Updating To update, check out our update page . Enterprise Edition Omnibus packages for GitLab Enterprise Edition 7.10.4 are available for subscribers here . For installations from source, use this guide . Interested in GitLab Enterprise Edition? For an overview of feature exclusive to GitLab Enterprise Edition please have a look at the features exclusive to GitLab EE . Access to GitLab Enterprise Edition is included with a subscription . No time to upgrade GitLab yourself? A subscription also entitles to our upgrade and installation services. Install GitLab on your own server in 2 minutes Let's do it! Install GitLab on your own server in 2 minutes → Browse all posts @gitlab on Twitter. For the latest and most detailed news followon Twitter. Get every GitLab blog post and stay up to date. Please enable JavaScript to view the <a href=\"http://disqus.com/?ref_noscript\">comments powered by Disqus.</a>"},{"title":"GitLab 7.10.5 released","tags":"scm","url":"http://ciandcd.github.io/gitlab-7105-released.html","text":"From: https://www.gitlab.com/2015/05/27/gitlab-7-dot-10-dot-5-released/ GitLab 7.10.5 released In GitLab 7.11 we have introduced the requirement of a license key for users of GitLab Enterprise Edition. This can cause a moment of downtime when upgrading, as you will need to upload the license key before being able to push to the GitLab instance. With this patch release we're adding a license upload functionality that allows you to upload your license in GitLab 7.10.5, preventing downtime when upgrading to GitLab 7.11 Enterprise Edition. This patch release also includes a fix for GitLab Annex and patches a MySQL vulnerability in GitLab CI. If you are not using GitLab Enterprise Edition, you can skip this patch and go straight to GitLab 7.11 . Upgrade barometer This is a minor update, without any migrations. No downtime is necessary. Updating To update, check out our update page . As Enterprise Edition user, if you want to update to 7.10.5 rather than straight to 7.11, download and install the Omnibus package at the old download location, here. . For installations from source, use this guide . Enterprise Edition Interested in GitLab Enterprise Edition? For an overview of feature exclusive to GitLab Enterprise Edition please have a look at the features exclusive to GitLab EE . Access to GitLab Enterprise Edition is included with a subscription . No time to upgrade GitLab yourself? A subscription also entitles to our upgrade and installation services. Install GitLab on your own server in 2 minutes Let's do it! Install GitLab on your own server in 2 minutes → Browse all posts @gitlab on Twitter. For the latest and most detailed news followon Twitter. Get every GitLab blog post and stay up to date. Please enable JavaScript to view the <a href=\"http://disqus.com/?ref_noscript\">comments powered by Disqus.</a>"},{"title":"GitLab 7.11 released with Two-factor Authentication and a publicly viewable Enterprise Edition","tags":"scm","url":"http://ciandcd.github.io/gitlab-711-released-with-two-factor-authentication-and-a-publicly-viewable-enterprise-edition.html","text":"From: https://www.gitlab.com/2015/05/22/gitlab-7-11-released/ It's the 22nd of the month, so we have a new GitLab release ready! GitLab 7.11 brings more improvements to the look and feel of GitLab, two-factor authentication, a version check and more! Of course we're also releasing GitLab CI 7.11, with a new backup and restore utility, improvements in the UI and other new features. This month's MVP is James Newton (newton on IRC)! James is very active on our #gitlab IRC channel, often helping people out with issues or helping people getting started with GitLab. We're very happy to have James supporting the community and believe that is deserving of a MVP award! Thanks James! Better looking sidebar We changed the look of the sidebar to reflect its function better and make it look more pretty: Clean project dashboard The project dashboard was a good example of design by committee, one GitLab contributor noted. We broomed through it and cleaned it up: Two-factor authentication Keep your code more secure and start using two-factor authentication (2FA)! GitLab has built-in 2FA in both CE and EE now and makes use of the convenient Google Authenticator. All you have to do is go to your Profile > Account and scan the QR code using Google's app. From now on, on login you'll be required to provide the code the app gives you for GitLab. Two-factor authentication only works with the web-UI for now. User roles in comments Now you know who's who in your favorite project. On comments you will see the role of the person in that project: Task lists everywhere Want a task list in the comments? Now you can! Version Check GitLab releases a new version every single month on the 22nd, so we understand that people are not always up to date. We wanted to give you some help with this, so from now on you can quickly see which version of GitLab you have running by visiting the Help or Admin page. It will show if you are up to date and if there is a security release you should have installed. Read more about the version check in our blog post about it. You can turn off the version check under Admin > Settings. License keys for Enterprise Edition GitLab Enterprise Edition used to live in a private repository, which was fine up until now. However, with the addition of our package server, we want to make it easier to start using GitLab Enterprise Edition. Rather than locking up the package repository of GitLab EE, we decided to open up all the code and packages and start using license keys. The code is still proprietary, but now is publicly viewable . This has several advantages. The installation of GitLab EE becomes as easy as installing GitLab CE. You no longer needs access to specific repositories, rather you can download it using the same methods as CE (including AWS/Azure templates, Docker images, etc). In addition, the code for Enterprise Edition is now becoming open to inspect for everyone. This will make it easier to send enhancements and makes it easier to do a trial of Enterprise Edition. Getting organizations to purchase a subscription after their trial expires or at renewal time sometimes took a substantial effort from us. We don't want to raise prices for customers that renew without prompting because we need to invest more time in unresponsive customers. Therefore we decided to introduce license keys that prompt customers automatically. We regret the inconvenience that license keys introduce but we think it is the best solution to keep prices low. True-up model for subscriptions The worst thing about license keys is that they can be very inflexible. Most GitLab installations quickly grow in popularity within the organization. Having to purchase a new license key every time this happens is very inefficient. Also, we noticed that the majority of our customers didn't have a compliant subscription, for us this indicates that having to renew the subscription multiple times a year is very inconvenient. Therefore we will switch to a true-up model that allows you to grow now and pay later. When you get a new license you should get it for your current number of active users. For users that are added during the year you pay half price when you renew. So if you have 100 active users today you get a 100 user subscription. Suppose that when you renew a year from now you have 300 active users. You pay for a 300 user subscription and pay half a year for the 200 users that you added during the year. Getting the license key If you are currently a GitLab customer, you should have received your license key already at the email you registered with your payment. You can also email sales at gitlab dot com to request it at any time. New subscribers will receive their license key automatically. Installing the license key To install the license, vist /admin/license in your GitLab instance as an admin. Here you can upload your .gitlab-license file, which will instantly unlock GitLab Enterprise Edition. You can also download and review your current license here. Please note that we will release GitLab 7.10.5 soon, that will allow you to upload the license key to your GitLab instance before upgrading, to avoid unnecessary downtime. Two-Factor Authentication for LDAP / Active Directory (EE-only) Want to use two-factor authentication together with your LDAP or Active Directory integration? With GitLab Enterprise Edition you can. New GitLab CI Features With the release of GitLab 7.11, we also updated GitLab CI to 7.11. Some changes worth mentioning are an improved runners page, public accessible build and commit pages for public projects , a new backup/restore utility that will backup your CI database and HipChat notifications! Other awesome changes in GitLab CE We can never cover all the new stuff in each GitLab release, but these are worth to have a quick look at as well: Quick quote-reply You can now reply with a quotation by simply selecting text in an issue or merge request and pressing r . It will set the focus to the editing window and have the quoted text already in it! Atom feeds for all! There is now an atom feed for each project! Settings in admin UI We moved default project and snippet visibility settings to the admin web interface. Improved UI for mobile GitLab is now better viewable on mobile! WIP your MRs! If you add WIP or [WIP] (work in progress) to the start of the title of a merge request, it will be protected from merging now. This release has more improvements, including security fixes, please check out the Changelog to see the all named changes. Upgrade barometer Coming from 7.10, the migrations in 7.11 are pretty fast (under 1 minute), but one of them is tricky: we rename any existing users with names ending in a period (‘.'). This migration updates both the database and the filesystem and previous versions of this migration have proven to be fragile. If you have no user namespaces with paths ending in ‘.' in your database and if you trust your users not to create any until after you upgrade to GitLab 7.11 you can perform this upgrade online. If not, we recommend to take downtime (this is what we did for gitlab.com). You can find the current number of affected database records with the following command: 1 sudo gitlab-rails runner \"puts Namespace.where(type: nil).where(%q{path LIKE '%.'}).count\" Installation If you are setting up a new GitLab installation please see the installing GitLab page . Updating Check out our update page . Please note that cookbook-omnibus-gitlab, our Chef cookbook that installs/manages GitLab omnibus packages, does not yet support packages.gitlab.com. See this issue . Enterprise Edition The mentioned EE-only features and things like LDAP group support can be found in GitLab Enterprise Edition. For a complete overview please have a look at the feature list of GitLab EE . Access to GitLab Enterprise Edition is included with a subscription . No time to upgrade GitLab yourself? A subscription also entitles you to our upgrade and installation services. Install GitLab on your own server in 2 minutes Let's do it! Install GitLab on your own server in 2 minutes → Browse all posts @gitlab on Twitter. For the latest and most detailed news followon Twitter. Get every GitLab blog post and stay up to date. Please enable JavaScript to view the <a href=\"http://disqus.com/?ref_noscript\">comments powered by Disqus.</a>"},{"title":"GitLab 7.11.4 released","tags":"scm","url":"http://ciandcd.github.io/gitlab-7114-released.html","text":"From: https://www.gitlab.com/2015/05/28/gitlab-7-dot-11-dot-4-released/ GitLab 7.11.4 released We've released GitLab 7.11.4 for GitLab CE, EE and CI. It includes the following fixes for CE and EE: Fix rendering of list bullets Force a rel=\"nofollow\" attribute on all external links in markdown For GitLab Enterprise Edition this patch release also fixes a bug in git-annex. This fix was also included in the (unannounced) 7.11.3 patch. Upgrade barometer This is a minor update, without any migrations. No downtime is necessary. Updating To update, check out our update page . Enterprise Edition Interested in GitLab Enterprise Edition? For an overview of feature exclusive to GitLab Enterprise Edition please have a look at the features exclusive to GitLab EE . Access to GitLab Enterprise Edition is included with a subscription . No time to upgrade GitLab yourself? A subscription also entitles to our upgrade and installation services. Install GitLab on your own server in 2 minutes Let's do it! Install GitLab on your own server in 2 minutes → Browse all posts @gitlab on Twitter. For the latest and most detailed news followon Twitter. Get every GitLab blog post and stay up to date. Please enable JavaScript to view the <a href=\"http://disqus.com/?ref_noscript\">comments powered by Disqus.</a>"},{"title":"GitLab, Gitorious, and Free Software","tags":"scm","url":"http://ciandcd.github.io/gitlab-gitorious-and-free-software.html","text":"From: https://www.gitlab.com/2015/05/20/gitlab-gitorious-free-software/ GitLab, Gitorious, and Free Software This is a guest post by Mike Gerwitz , a free software hacker and activist, and author of GNU ease.js . In early March of this year, it was announced that GitLab would acquire Gitorious and shut down gitorious.org by 1 June, 2015. Reactions from the community were mixed, and understandably so: while GitLab itself is a formidable alternative to wholly proprietary services, its acquisition of Gitorious strikes a chord with the free software community that gathered around Gitorious in the name of software freedom . After hearing that announcement, as a free software hacker and activist myself , I was naturally uneasy. Discussions of alternatives to Gitorious and GitLab ensued on the libreplanet-discuss mailing list. Sytse Sijbrandij (GitLab B.V. CEO) happened to be present on that list; I approached him very sternly with a number of concerns, just as I would with anyone that I feel does not understand certain aspects of the free software philosophy . To my surprise, this was not the case at all. Sytse has spent a lot of time accepting and considering community input for both the Gitorious acquisition and GitLab itself. He has also worked with me to address some of the issues that I had raised. And while these issues won't address everyone's concerns, they do strengthen GitLab's commitment to software freedom , and are commendable. I wish to share some of these details here; but to do so, I first have to provide some background to explain what the issues are, and why they are important. Free Software Ideology Gitorious was (and still is) one of the most popular Git repository hosts, and largely dominated until the introduction of GitHub. But even as users flocked to GitHub's proprietary services , users who value freedom continued to support Gitorious, both on gitorious.org and by installing their own instances on their own servers. Since Gitorious is free software , users are free to study, modify, and share it with others. But software freedom does not apply to Services as a Software Substitute (SaaSS) or remote services—you cannot apply the four freedoms to something that you do not yourself possess—so why do users still insist on using gitorious.org despite this? The matter boils down to supporting a philosophy: The GNU General Public License (GPL) is a license that turns copyright on its head: rather than using copyright to restrict what users can do with a program, the GPL instead ensures users' freedoms to study, modify, and share it. But that isn't itself enough: to ensure that the software always remains free (as in freedom), the GPL ensures that all derivatives are also licensed under similar terms. This is known as copyleft , and it is vital to the free software movement. Gitorious is licensed under the GNU Affero General Public License Version 3 (AGPLv3) —this takes the GPL and adds an additional requirement: if a modified version of the program is run on a sever, users communicating with the program on that server must have access to the modified program's source code. This ensures that modifications to the program are available to all users ; they would otherwise be hidden in private behind the server, with others unable to incorporate, study, or share them. The AGPLv3 is an ideal license for Gitorious, since most of its users will only ever interact with it over a network. GitLab is also free software: its Expat license (commonly referred to ambiguously as the \"MIT license\") permits all of the same freedoms that are granted under the the GNU GPL. But it does so in a way that is highly permissive: it permits relicensing under any terms, free or not. In other words, one can fork GitLab and derive a proprietary version from it, making changes that deny users their freedoms and cannot be incorporated back into the original work. This is the issue that the free software community surrounding Gitorious has a problem with: any changes contributed to GitLab could in turn benefit a proprietary derivative. This situation isn't unique to GitLab: it applies to all non-copyleft (\"permissive\") free software licenses . And this issue is realized by GitLab itself in the form of its GitLab Enterprise Edition (GitLab EE): a proprietary derivative that adds additional features atop of GitLab's free Community Edition (CE). For this reason, many free software advocates are uncomfortable contributing to GitLab, and feel that they should instead support other projects; this, in turn, means not supporting GitLab by using and drawing attention to their hosting services. The copyleft vs. permissive licensing debate is one of the free software movement's most heated. I do not wish to get into such a debate here. One thing is clear: GitLab Community Edition (GitLab CE) is free software. Richard Stallman (RMS) responded directly to the thread on libreplanet-discuss , stating plainly: We have a simple way of looking at these two versions. The free version is free software, so it is ethical. The nonfree version is nonfree software, so it is not ethical. Does GitLab CE deserve attention from the free software community? I believe so. Importantly, there is another strong consideration: displacing proprietary services like GitHub and Bitbucket, which host a large number of projects and users. GitLab has a strong foothold, which is an excellent place for a free software project to be in. If we are to work together as a community, we need to respect GitLab's free licensing choices just as we expect GitLab to respect ours. Providing respect does not mean that you are conceding: I will never personally use a non-copyleft license for my software; I'm firmly rooted in my dedication to the free software philosophy , and I'm sure that many other readers are too. But using a non-copyleft license, although many of us consider it to be a weaker alternative, is not wrong . Free JavaScript As I mentioned above, software freedom and network services are separate issues —the four freedoms do not apply to interacting with gitlab.com purely over a network connection, for example, because you are not running its software on your computer. However, there is an overlap: JavaScript code downloaded to be executed in your web browser. Non-free JavaScript is a particularly nasty concern: it is software that is downloaded automatically from a server—often without prompting you—and then immediately executed. Software is now being executed on your machine, and your four freedoms are once again at risk. This, then, is the primary concern for any users visiting gitlab.com : not only would this affect users that use gitlab.com as a host, but it would also affect any user that visits the website. That would be a problem, since hosting your project there would be inviting users to run proprietary JavaScript. As I was considering migrating my projects to GitLab, this was the first concern I brought up to Sytse . This problem arises because gitlab.com uses a GitLab EE instance: if it had used only its Community Edition (GitLab CE)—which is free software—then all served JavaScript would have been free. But any scripts served by GitLab EE that are not identical to those served by GitLab CE are proprietary, and therefore unethical. This same concern applies to GitHub, Bitbucket, and other proprietary hosts that serve JavaScript. Sytse surprised me by stating that he would be willing to freely license all JavaScript in GitLab EE , and by offering to give anyone access to the GitLab EE source code who wants to help out. I took him up on that offer. Initially, I had submitted a patch to merge all GitLab EE JavaScript into GitLab CE, but Sytse came up with another, superior suggestion, that ultimately provided even greater reach. I'm pleased to announce that Sytse and I were able to agree on a license change (with absolutely no friction or hesitation on his part) that liberates all JavaScript served to the client from GitLab EE instances. There are two concerns that I had wanted to address: JavaScript code directly written for the client, and any code that produced JavaScript as output. In the former case, this includes JavaScript derived from other sources: for example, GitLab uses CoffeeScript, which compiles into JavaScript. The latter case is important: if there is any code that generates fragments of JavaScript—e.g. dynamically at runtime—then that code must also be free, or users would not be able to modify and share the resulting JavaScript that is actually being run on the client. Sytse accepted my change verbatim, while adding his own sentence after mine to disambiguate. At the time of writing this post, GitLab EE's source code isn't yet publicly visible, so here is the relevant snippet from its LICENSE file: The above copyright notices applies only to the part of this Software that is not distributed as part of GitLab Community Edition (CE), and that is not a file that produces client-side JavaScript, in whole or in part. Any part of this Software distributed as part of GitLab CE or that is a file that produces client-side JavaScript, in whole or in part, is copyrighted under the MIT Expat license. Further Discussion My discussions with Sytse did not end there: there are other topics that have not been able to be addressed before my writing of this post that would do well to demonstrate commitment toward software freedom . The license change liberating client-side JavaScript was an excellent move. To expand upon it, I wish to submit a patch that would make GitLab LibreJS compliant ; this provides even greater guarantees, since it would allow for users to continue to block other non-free JavaScript that may be served by the GitLab instance, but not produced by it. For example: a website/host that uses GitLab may embed proprietary JavaScript, or modify it without releasing the source code. Another common issue is the user of analytics software; gitlab.com uses Google Analytics. If you would like to help with LibreJS compliance, please contact me . I was brought into another discussion between Sytse and RMS that is unrelated to the GitLab software itself, but still a positive demonstration of a commitment to software freedom —the replacement of Disqus on the gitlab.com blog with a free alternative. Sytse ended up making a suggestion, saying he'd be \"happy to switch to\" Juvia if I'd help with the migration. I'm looking forward to this, as it is an important discussion area (that I honestly didn't know existed until Sytse told me about it, because I don't permit proprietary JavaScript!). He was even kind enough to compile a PDF of comments for one of our discussions, since he was cognizant ahead of time that I would not want to use Disqus. (Indeed, I will be unable to read and participate in the comments to this guest post unless I take the time to freely read and reply without running Disqus' proprietary JavaScript.) Considering the genuine interest and concern expressed by Sytse in working with myself and the free software community, I can only expect that GitLab will continue to accept and apply community input. Actions Speak Louder Than Words It is not possible to address the copyleft issue without a change in license, which GitLab is not interested in doing. So the best way to re-assure the community is through action. To quote Sytse : I think the only way to prove we're serious about open source is in our actions, licenses or statements don't help. There are fundamental disagreements that will not be able to be resolved between GitLab and the free software community—like their \"open core\" business model . But after working with Sytse and seeing his interactions with myself, RMS, and many others in the free software community, I find his actions to be very encouraging. Are you interested in helping other websites liberate their JavaScript? Consider joining the FSF's campaign , and please liberate your own ! This post is licensed under the Creative Commons Attribution-ShareAlike 3.0 Unported License . Install GitLab on your own server in 2 minutes Let's do it! Install GitLab on your own server in 2 minutes → Browse all posts @gitlab on Twitter. For the latest and most detailed news followon Twitter. Get every GitLab blog post and stay up to date. Please enable JavaScript to view the <a href=\"http://disqus.com/?ref_noscript\">comments powered by Disqus.</a>"},{"title":"GitLab.com and Logjam","tags":"scm","url":"http://ciandcd.github.io/gitlabcom-and-logjam.html","text":"From: https://www.gitlab.com/2015/06/17/gitlab-com-and-logjam/ GitLab.com and Logjam We've previously announced security advisory for Logjam vulnerability . In that announcement we've mentioned that GitLab.com is using 1024-bit DH groups to retain compatibility with older Java-based clients. We've updated the default/recommended SSL ciphers for all GitLab installations and implemented new ciphers on GitLab.com. After some reasearch and testing we've decided to change the SSL cipher suite served by the web server/load balancer. This decision was made after weighing on the trade-offs between having the stronger DH params and denying access to Java 6 based clients. Using 2048-bit DHE params Generating the 2048-bit DHE params was advised to help against the Logjam vulnerability. While this is a way to go for most servers, with GitLab.com we have to keep in mind that we have users using older Java-based clients. Adopting the stronger params suites would prevent those users using GitLab.com completely. Although the number of these users is not high, denying them access does not seem like an option. Removing DHE suites DHE suites have a couple of issues: DHE is slow Not all browsers support all the necessary suites One advantage of having DHE together with ECDHE suites is that this allows forward secrecy to all clients. We then turned to investigating how others are handling this issue and we found out that, for example, Google sites mostly do not have DHE suites in their configuration . With this in mind we've tried removing the DHE suites and the result was as follows: All major browsers and clients retain forward secrecy using ECDHE SSL labs score went from B to A There is no forward secrecy for Android 2.3.7, Java 6 and OpenSSL 0.9.8 After considering the trade-offs, we've decided to remove the DHE suites from our cipher suite on GitLab.com. Forward secrecy is now denied for Android 2.3.7, Java 6 and OpenSSL 0.9.8 but we suspect that number of users affected will be extremely low. We have also updated the recommended configurations for omnibus-gitlab packages and GitLab installation from source. Install GitLab on your own server in 2 minutes Let's do it! Install GitLab on your own server in 2 minutes → Browse all posts @gitlab on Twitter. For the latest and most detailed news followon Twitter. Get every GitLab blog post and stay up to date. Please enable JavaScript to view the <a href=\"http://disqus.com/?ref_noscript\">comments powered by Disqus.</a>"},{"title":"GitLab.com outage on 2015-05-29","tags":"scm","url":"http://ciandcd.github.io/gitlabcom-outage-on-2015-05-29.html","text":"From: https://www.gitlab.com/2015/06/04/gitlab-dot-com-outage-on-2015-05-29/ GitLab.com outage on 2015-05-29 GitLab.com suffered an outage from 2015-05-29 01:00 to 2015-05-29 02:34 (times in UTC). In this blog post we will discuss what happened, why it took so long to recover the service, and what we are doing to reduce the likelihood and impact of such incidents. Background GitLab.com is provided and maintained by the team of GitLab B.V., the company behind GitLab. On 2015-05-02 we performed a major infrastructure upgrade, moving GitLab.com from a single server to a small cluster of servers, consisting of a load balancer (running HAproxy), three workers (NGINX/Unicorn/Sidekiq/gitlab-shell) and a backend server (PostgreSQL/Redis/NFS). This new infrastructure configuration improved the responsiveness of GitLab.com, at the expense of having more moving parts. GitLab.com is backed up using Amazon EBS snapshots. To protect against inconsistent snapshots our backup script ‘freezes' the filesystem on the backend server with fsfreeze prior to making EBS snapshots, and ‘unfreezes' the filesystem immediately after. Timeline Italic comments below are written with the knowledge of hindsight 1:00 The GitLab.com backup script is activated by Cron on the backend server. For unknown reasons, the backup script hangs/crashes before or during the ‘unfreeze' of the filesystem holding all user data. 1:07 Our on-call engineer is paged by Pingdom . The on-call engineer tries to diagnose the issue on the worker servers but is unable to diagnose the problem. The issue was on the backend server, not on the workers. 1:30 The on-call engineer decides to call in more help. The other team members with access and knowledge to resolve the issue are all in Europe at this time, where it is 3:30/4:30am. 1:45 A second engineer in Europe has been woken up and takes the lead on the investigation of the outage. More workers are rebooted because they appear to be stuck. It becomes apparent that the workers cannot mount the NFS share which holds all Git repository data. 1:51 One of the engineers notices that the load on the backend server is more than 150. A normal value would be less than 5. 2:10 The engineers give up on running commands on the workers to bring the NFS share back, and start investigating the backend server. The engineers discuss whether they should reboot the backend server but they are unsure if it is safe given that this setup is fairly new. 2:21 The engineers reboot the backend server. The reboot is taking a long time. The AWS ‘reboot' command first tries a soft reboot, and only does a hard reboot after a 4-minute timeout. The soft reboot probably hung when it tried to shut down services that were trying to write to the ‘frozen' disk. 2:30 The backend server has rebooted and the engineers regain SSH access to it. The worker servers are able to mount the NFS share now but GitLab.com is still not functioning because the Postgres database server is not responding. One of the engineers restarts Postgres on the backend server. It may have been that Postgres was still busy performing crash recovery. 2:34 Gitlab.com is available again. Root causes Although we cannot explain what went wrong with the backup script it is hard to come to another conclusion that something did go wrong with it. The length of the outage was caused by insufficient training and documentation for our on-call engineers following the infrastructure upgrade rolled out on May 2nd. Next steps We have removed the freeze/unfreeze steps from our backup script. Because this (theoretically) increases the risk of occasional corrupt backups we have added a second backup strategy for our SQL data. In the future we would like to have automatical validation of our GitLab.com backups. The day before this incident we decided the training was our most important priority. We have started to do regular operations drills in one-on-one sessions with all of our on-call engineers. Install GitLab on your own server in 2 minutes Let's do it! Install GitLab on your own server in 2 minutes → Browse all posts @gitlab on Twitter. For the latest and most detailed news followon Twitter. Get every GitLab blog post and stay up to date. Please enable JavaScript to view the <a href=\"http://disqus.com/?ref_noscript\">comments powered by Disqus.</a>"},{"title":"Helix DVCS - How to Initialize Like a Pro","tags":"scm","url":"http://ciandcd.github.io/helix-dvcs-how-to-initialize-like-a-pro.html","text":"From: http://www.perforce.com/blog/150610/helix-dvcs-initialization-tips-tricks We are all very excited about the new distributed version control system (DVCS) capabilities of Perforce Helix. Here are a few tips for getting started. Keep in mind that in order to use Helix DVCS, you need to have the 2015.1 version of both Helix client P4 and Helix server P4D installed. Some of the commands (e.g., init and clone ) are implemented in P4, so you need the latest version of both executables. The first thing you need to do when you want to use a local Helix server (called a personal server) is to run \"p4 init\". This command will create the personal server for you (in a subdirectory called .p4root) and set up the P4CONFIG and P4IGNORE files, as well. \"p4 init\" also turns your current directory into the client workspace root for your new personal server , which is useful if you already have some files and realize it might be a good thing to version them: p4 init p4 rec p4 submit -n \"Initial checkin\" In the above, \"rec\" is a handy alias for \"reconcile\" to save you typing . If you start a new project from scratch and want to place it in another directory instead, use the \"-d\" option like such: p4 –d path-to-new-project init Case and Unicode Let's take a closer look at the output of the \"p4 init\" command: Matching server configuration from ‘wayfarer-p4d:1666': case-sensitive (-C0), non-unicode (-n) Server sknop-dvcs-1429629213 saved. One might ask: what is case-sensitive and Unicode about? Because the Helix versioning engine supports many platforms, both case sensitive and insensitive, you can choose how your personal server handles case. By default, the Helix versioning engine adopts the case policy of the platform you run it on: insensitive on Mac and Windows, sensitive on Linux and other Unix platforms. Also by default, the Helix versioning engine does no Unicode translation and simply accepts any encoding for file content and metadata. For cross-platform development it is better to put a shared server into Unicode mode. For a personal server you may not care at first what these settings are, but what if you want to push your changes to another server at a later stage? The settings of your personal server have to match the settings on the destination server or there could be chaos, as the destination server will refuse the push if the settings do not match. It is cumbersome to change case sensitivity and Unicode settings after the Helix versioning engine is populated, so it is important to get this right up front. \"p4 init\" will \"guess\" what the standard settings within your enterprise are by connecting to and inquiring with the Helix versioning engine specified by the P4PORT environment variable (or \"perforce:1666\" if that is not set). If you'd rather inquire with a particular server when initializing a personal server, use the \"-p\" option: p4 init –p myserver:1666 Alternately, you can also explicitly set case and Unicode support with the following options: Option Meaning -C1 Case insensitive -C0 Case sensitive -n No Unicode support -xi Unicode support Server and User NameNote well: if you have P4CHARSET defined in your environment and not set to \"none\", a new personal server will automatically be initialized as a Unicode-enabled server. So what is the story with the server and user name? The name of your personal server and client workspace coincide. Although in principle you can have more than one workspace against your personal server, in practice there is rarely any need for it. Locally the name does not matter, but when you push your changes into another server, the changes are linked to your local workspace name. An automatically generated name like \"sknop-dvcs-1429629213\" is highly likely do be unique, but you are free to choose a different name if you so wish by using the \"-c\" option. The same is true for your user name: locally it does not matter and will typically coincide with either your OS user name and/or whatever P4USER is set to, but when pushing to another server the user name becomes important. Take the Perforce workshop for example: my local user name is always \"sknop\", but for the workshop I use \"sven_erik_knop\". If I create a local DVCS server under the user name \"sknop\", submit my changes, set up a remote to the workshop, and push, I'll receive only an error message. Fortunately, the solution is very simple. I add another user to my local server and update my local protection table: p4 user –f sven_erik_knop p4 protect Now I can push my changes under the new user name (I might have to log into the target server first): p4 –u sven_erik_knop push Conclusion A simple \"p4 init\" will create you a new personal server to which you can submit changes, but if you want to push these changes to another server, it makes sense to pay attention to case sensitivity, Unicode support, and workspace and user name. Let me know if you are using our new DVCS features and how you are getting on. My Twitter handle is @p4sven. For a live technical overview of DVCS features in the Helix Versioning engine sign up for our DevTalk Webinar on June 26th."},{"title":"Helix Swarm 2015.1 Released","tags":"scm","url":"http://ciandcd.github.io/helix-swarm-20151-released.html","text":"From: http://www.perforce.com/blog/150608/helix-swarm-20151-released Swarm is two years old this month! It's rewarding to think that just two years ago, our collaboration engine was only just getting into the hands of our customers. Fast-forward to today, where Swarm plays a big part in the daily workflows of so many innovative companies. With much of the functionality now matured, we wanted to expand Swarm beyond just our English-speaking customers. With the latest release of Helix Swarm, we've translated the product into Japanese. It's available through our exclusive partner in Japan, the TOYO Corporation . TOYO provides expert consulting and support to our Japanese customers, and Swarm joins the Helix Versioning Engine and our popular visual client, P4V , in the suite of Perforce products available in Japanese. Localization Support for Swarm The Swarm team spent the last couple of months creating a localization framework and translating the product and documentation into Japanese. The next languages on our list are Korean and Simplified Chinese. If there's a language you'd like us to add to our list of localizations, please send us your request via the Perforce Swarm Forums or by emailing support and we'll put it on our radar. Aside from the localization support , other new functionalities include: Files and folders are downloadable as ZIP archives Swarm now limits the number of files to display in a committed change to a configurable default of 1000 Configurable timeout sets thresholds for large commits More details can be found in the What's new in 2015.1 section of our user guide."},{"title":"How GitLab uses Unicorn and unicorn-worker-killer","tags":"scm","url":"http://ciandcd.github.io/how-gitlab-uses-unicorn-and-unicorn-worker-killer.html","text":"From: https://www.gitlab.com/2015/06/05/how-gitlab-uses-unicorn-and-unicorn-worker-killer/ How GitLab uses Unicorn and unicorn-worker-killer We just wrote some new documentation on how Gitlab uses Unicorn and unicorn-worker-killer, available on doc.gitlab.com but also included below. We would love to hear from the community if you have other questions so we can improve this documentation resource! Update 19:29 CEST: made link to doc.gitlab.com more specific. Understanding Unicorn and unicorn-worker-killer Unicorn GitLab uses Unicorn , a pre-forking Ruby web server, to handle web requests (web browsers and Git HTTP clients). Unicorn is a daemon written in Ruby and C that can load and run a Ruby on Rails application; in our case the Rails application is GitLab Community Edition or GitLab Enterprise Edition. Unicorn has a multi-process architecture to make better use of available CPU cores (processes can run on different cores) and to have stronger fault tolerance (most failures stay isolated in only one process and cannot take down GitLab entirely). On startup, the Unicorn ‘master' process loads a clean Ruby environment with the GitLab application code, and then spawns ‘workers' which inherit this clean initial environment. The ‘master' never handles any requests, that is left to the workers. The operating system network stack queues incoming requests and distributes them among the workers. In a perfect world, the master would spawn its pool of workers once, and then the workers handle incoming web requests one after another until the end of time. In reality, worker processes can crash or time out: if the master notices that a worker takes too long to handle a request it will terminate the worker process with SIGKILL (‘kill -9'). No matter how the worker process ended, the master process will replace it with a new ‘clean' process again. Unicorn is designed to be able to replace ‘crashed' workers without dropping user requests. This is what a Unicorn worker timeout looks like in unicorn_stderr.log . The master process has PID 56227 below. 1 2 3 4 [2015-06-05T10:58:08.660325 #56227] ERROR -- : worker=10 PID:53009 timeout (61s > 60s), killing [2015-06-05T10:58:08.699360 #56227] ERROR -- : reaped #<Process::Status: pid 53009 SIGKILL (signal 9)> worker=10 [2015-06-05T10:58:08.708141 #62538] INFO -- : worker=10 spawned pid=62538 [2015-06-05T10:58:08.708824 #62538] INFO -- : worker=10 ready Tunables The main tunables for Unicorn are the number of worker processes and the request timeout after which the Unicorn master terminates a worker process. See the omnibus-gitlab Unicorn settings documentation if you want to adjust these settings. unicorn-worker-killer GitLab has memory leaks. These memory leaks manifest themselves in long-running processes, such as Unicorn workers. (The Unicorn master process is not known to leak memory, probably because it does not handle user requests.) To make these memory leaks manageable, GitLab comes with the unicorn-worker-killer gem . This gem monkey-patches the Unicorn workers to do a memory self-check after every 16 requests. If the memory of the Unicorn worker exceeds a pre-set limit then the worker process exits. The Unicorn master then automatically replaces the worker process. This is a robust way to handle memory leaks: Unicorn is designed to handle workers that ‘crash' so no user requests will be dropped. The unicorn-worker-killer gem is designed to only terminate a worker process in between requests, so no user requests are affected. This is what a Unicorn worker memory restart looks like in unicorn_stderr.log. You see that worker 4 (PID 125918) is inspecting itself and decides to exit. The threshold memory value was 254802235 bytes, about 250MB. With GitLab this threshold is a random value between 200 and 250 MB. The master process (PID 117565) then reaps the worker process and spawns a new ‘worker 4' with PID 127549. 1 2 3 4 5 [2015-06-05T12:07:41.828374 #125918] WARN -- : #<Unicorn::HttpServer:0x00000002734770>: worker (pid: 125918) exceeds memory limit (256413696 bytes > 254802235 bytes) [2015-06-05T12:07:41.828472 #125918] WARN -- : Unicorn::WorkerKiller send SIGQUIT (pid: 125918) alive: 23 sec (trial 1) [2015-06-05T12:07:42.025916 #117565] INFO -- : reaped #<Process::Status: pid 125918 exit 0> worker=4 [2015-06-05T12:07:42.034527 #127549] INFO -- : worker=4 spawned pid=127549 [2015-06-05T12:07:42.035217 #127549] INFO -- : worker=4 ready One other thing that stands out in the log snippet above, taken from Gitlab.com, is that ‘worker 4' was serving requests for only 23 seconds. This is a normal value for our current GitLab.com setup and traffic. The high frequency of Unicorn memory restarts on some GitLab sites can be a source of confusion for administrators. Usually they are a red herring . Install GitLab on your own server in 2 minutes Let's do it! Install GitLab on your own server in 2 minutes → Browse all posts @gitlab on Twitter. For the latest and most detailed news followon Twitter. Get every GitLab blog post and stay up to date. Please enable JavaScript to view the <a href=\"http://disqus.com/?ref_noscript\">comments powered by Disqus.</a>"},{"title":"How to Archive a Review in Helix Swarm","tags":"scm","url":"http://ciandcd.github.io/how-to-archive-a-review-in-helix-swarm.html","text":"From: http://www.perforce.com/blog/150624/how-archive-review-helix-swarm We get a lot of positive feedback about Helix Swarm, but some customers are not sure about all of the workflow steps, so here's a quick review. The code review in Helix Swarm can be in one of the following states: Needs Review - The review has started and the changes need to be reviewed. Needs Revisions - The changes have been reviewed and the reviewer has indicated that further revisions are required. Approved - The review has completed. The changes need to be committed. Rejected - The review has completed. The changes are undesirable and should not be committed. Archived - The review has completed for now. However, it is neither rejected nor approved; it is simply put aside in case it is needed in the future. Most of these states are self-explanatory with the exception of \"Archived\" state, which is where my fellow customers feel a bit lost. They wonder how they can archive and restore reviews for future consideration. Which can also mean that they inadvertently created a review for something they didn't intend to. That said, the review in question can be archived, restored, updated with new set of files and then routed through the workflow. Let's look at how to \"Archive\" the review: When you open a review in Swarm, there is a drop-down at the top right corner of the review. Select the \"Archive\" option if you think the review needs to be deffered for future consideration. Once you set the state of the review to \"Archive\", the review disappears from the list of open reviews. Now, at a later stage, the project team decides that the review should be restored for immediate consideration. Let's look at how to restore a previously archived review: The previously archived review can be found under, \"Closed\" review tab. To narrow down your search further, the \"Archived\" review icon can be clicked to list only archived reviews. Select the review you wish to schedule for consideration by clicking on the hyperlinked review ID. Again, go to the drop-down at the top right corner of the review. Simply click \"Needs Review\" and the review will reappear in the list of Opened reviews. Are you using Helix Swarm? We invite you to try it for free–it is included in Perforce Helix's free 20–user edition. Download now and tell us what you think!"},{"title":"How to undo (almost) anything with Git · GitHub","tags":"scm","url":"http://ciandcd.github.io/how-to-undo-almost-anything-with-git-github.html","text":"From: https://github.com/blog/2019-how-to-undo-almost-anything-with-git One of the most useful features of any version control system is the ability to \"undo\" your mistakes. In Git, \"undo\" can mean many slightly different things. When you make a new commit, Git stores a snapshot of your repository at that specific moment in time; later, you can use Git to go back to an earlier version of your project. In this post, I'm going to take a look at some common scenarios where you might want to \"undo\" a change you've made and the best way to do it using Git. Undo a \"public\" change Scenario: You just ran git push , sending your changes to GitHub, now you realize there's a problem with one of those commits. You'd like to undo that commit. Undo with: git revert <SHA> What's happening: git revert will create a new commit that's the opposite (or inverse) of the given SHA. If the old commit is \"matter\", the new commit is \"anti-matter\"—anything removed in the old commit will be added in the new commit and anything added in the old commit will be removed in the new commit. This is Git's safest, most basic \"undo\" scenario, because it doesn't alter history—so you can now git push the new \"inverse\" commit to undo your mistaken commit. Fix the last commit message Scenario: You just typo'd the last commit message, you did git commit -m \"Fxies bug #42\" but before git push you realized that really should say \"Fixes bug #42\". Undo with: git commit --amend or git commit --amend -m \"Fixes bug #42\" What's happening: git commit --amend will update and replace the most recent commit with a new commit that combines any staged changes with the contents of the previous commit. With nothing currently staged, this just rewrites the previous commit message. Undo \"local\" changes Scenario: The cat walked across the keyboard and somehow saved the changes, then crashed the editor. You haven't committed those changes, though. You want to undo everything in that file—just go back to the way it looked in the last commit. Undo with: git checkout -- <bad filename> What's happening: git checkout alters files in the working directory to a state previously known to Git. You could provide a branch name or specific SHA you want to go back to or, by default, Git will assume you want to checkout HEAD , the last commit on the currently-checked-out branch. Keep in mind: any changes you \"undo\" this way are really gone. They were never committed, so Git can't help us recover them later. Be sure you know what you're throwing away here! (Maybe use git diff to confirm.) Reset \"local\" changes Scenario: You've made some commits locally (not yet pushed), but everything is terrible, you want to undo the last three commits—like they never happened. Undo with: git reset <last good SHA> or git reset --hard <last good SHA> What's happening: git reset rewinds your repository's history all the way back to the specified SHA. It's as if those commits never happened. By default, git reset preserves the working directory. The commits are gone, but the contents are still on disk. This is the safest option, but often, you'll want to \"undo\" the commits and the changes in one move—that's what --hard does. Redo after undo \"local\" Scenario: You made some commits, did a git reset --hard to \"undo\" those changes (see above), and then realized: you want those changes back! Undo with: git reflog and git reset or git checkout What's happening: git reflog is an amazing resource for recovering project history. You can recover almost anything—anything you've committed—via the reflog. You're probably familiar with the git log command, which shows a list of commits. git reflog is similar, but instead shows a list of times when HEAD changed. Some caveats: HEAD changes only. HEAD changes when you switch branches, make commits with git commit and un-make commits with git reset , but HEAD does not change when you git checkout -- <bad filename> (from an earlier scenario—as mentioned before, those changes were never committed, so the reflog can't help us recover those. git reflog doesn't last forever. Git will periodically clean up objects which are \"unreachable.\" Don't expect to find months-old commits lying around in the reflog forever. Your reflog is yours and yours alone. You can't use git reflog to restore another developer's un-pushed commits. So... how do you use the reflog to \"redo\" a previously \"undone\" commit or commits? It depends on what exactly you want to accomplish: If you want to restore the project's history as it was at that moment in time use git reset --hard <SHA> If you want to recreate one or more files in your working directory as they were at that moment in time, without altering history use git checkout <SHA> -- <filename> If you want to replay exactly one of those commits into your repository use git cherry-pick <SHA> Once more, with branching Scenario: You made some commits, then realized you were checked out on master . You wish you could make those commits on a feature branch instead. Undo with: git branch feature , git reset --hard origin/master , and git checkout feature What's happening: You may be used to creating new branches with git checkout -b <name> —it's a popular short-cut for creating a new branch and checking it out right away—but you don't want to switch branches just yet. Here, git branch feature creates a new branch called feature pointing at your most recent commit, but leaves you checked out to master . Next, git reset --hard rewinds master back to origin/master , before any of your new commits. Don't worry, though, they are still available on feature . Finally, git checkout switches to the new feature branch, with all of your recent work intact. Branch in time saves nine Scenario: You started a new branch feature based on master , but master was pretty far behind origin/master . Now that master branch is in sync with origin/master , you wish commits on feature were starting now, instead of being so far behind. Undo with: git checkout feature and git rebase master What's happening: You could have done this with git reset (no --hard , intentionally preserving changes on disk) then git checkout -b <new branch name> and then re-commit the changes, but that way, you'd lose the commit history. There's a better way. git rebase master does a couple of things: First it locates the common ancestor between your currently-checked-out branch and master . Then it resets the currently-checked-out branch to that ancestor, holding all later commits in a temporary holding area. Then it advances the currently-checked-out-branch to the end of master and replays the commits from the holding area after master 's last commit. Mass undo/redo Scenario: You started this feature in one direction, but mid-way through, you realized another solution was better. You've got a dozen or so commits, but you only want some of them. You'd like the others to just disappear. Undo with: git rebase -i <earlier SHA> What's happening: -i puts rebase in \"interactive mode\". It starts off like the rebase discussed above, but before replaying any commits, it pauses and allows you to gently modify each commit as it's replayed. rebase -i will open in your default text editor, with a list of commits being applied, like this: The first two columns are key: the first is the selected command for the commit identified by the SHA in the second column. By default, rebase -i assumes each commit is being applied, via the pick command. To drop a commit, just delete that line in your editor. If you no longer want the bad commits in your project, you can delete lines 1 and 3-4 above. If you want to preserve the contents of the commit but edit the commit message, you use the reword command. Just replace the word pick in the first column with the word reword (or just r ). It can be tempting to rewrite the commit message right now, but that won't work— rebase -i ignores everything after the SHA column. The text after that is really just to help us remember what 0835fe2 is all about. When you've finished with rebase -i , you'll be prompted for any new commit messages you need to write. If you want to combine two commits together, you can use the squash or fixup commands, like this: squash and fixup combine \"up\"—the commit with the \"combine\" command will be merged into the commit immediately before it. In this scenario, 0835fe2 and 6943e85 will be combined into one commit, then 38f5e4e and af67f82 will be combined together into another. When you select squash , Git will prompt us to give the new, combined commit a new commit message; fixup will give the new commit the message from the first commit in the list. Here, you know that af67f82 is an \"ooops\" commit, so you'll just use the commit message from 38f5e4e as is, but you'll write a new message for the new commit you get from combining 0835fe2 and 6943e85 . When you save and exit your editor, Git will apply your commits in order from top to bottom. You can alter the order commits apply by changing the order of commits before saving. If you'd wanted, you could have combined af67f82 with 0835fe2 by arranging things like this: Fix an earlier commit Scenario: You failed to include a file in an earlier commit, it'd be great if that earlier commit could somehow include the stuff you left out. You haven't pushed, yet, but it wasn't the most recent commit, so you can't use commit --amend . Undo with: git commit --squash <SHA of the earlier commit> and git rebase --autosquash -i <even earlier SHA> What's happening: git commit --squash will create a new commit with a commit message like squash! Earlier commit . (You could manually create a commit with a message like that, but commit --squash saves you some typing.) You can also use git commit --fixup if you don't want to be prompted to write a new commit message for the combined commit. In this scenario, you'd probably use commit --fixup , since you just want to use the earlier commit's commit message during rebase . rebase --autosquash -i will launch an interactive rebase editor, but the editor will open with any squash! and fixup! commits already paired to the commit target in the list of commits, like so: When using --squash and --fixup , you might not remember the SHA of the commit you want to fix—only that it was one or five commits ago. You might find using Git's &#94; and ~ operators especially handy. HEAD&#94; is one commit before HEAD . HEAD~4 is four commits before HEAD - or, altogether, five commits back. Stop tracking a tracked file Scenario: You accidentally added application.log to the repository, now every time you run the application, Git reports there are unstaged changes in application.log . You put *.log in the .gitignore file, but it's still there—how do you tell git to to \"undo\" tracking changes in this file? Undo with: git rm --cached application.log What's happening: While .gitignore prevents Git from tracking changes to files or even noticing the existence of files it's never tracked before, once a file has been added and committed, Git will continue noticing changes in that file. Similarly, if you've used git add -f to \"force\", or override, .gitignore , Git will keep tracking changes. You won't have to use -f to add it in the future. If you want to remove that should-be-ignored file from Git's tracking, git rm --cached will remove it from tracking but leave the file untouched on disk. Since it's now being ignored, you won't see that file in git status or accidentally commit changes from that file again. That's how to undo anything with Git. To learn more about any of the Git commands used here, check out the relevant documentation:"},{"title":"Implementing .gitlab-ci.yml","tags":"scm","url":"http://ciandcd.github.io/implementing-gitlab-ciyml.html","text":"From: https://www.gitlab.com/2015/06/08/implementing-gitlab-ci-dot-yml/ Implementing .gitlab-ci.yml We wrote about why we're replacing GitLab CI jobs with a .gitlab-ci.yml file. As we've started on implementing this large change, we wanted to share the details of that process with you and would love to hear what you think. To recap the previous article : currently you are required to write out your CI jobs in GitLab CI's interface. We're replacing this with a single file .gitlab-ci.yml , that you place in the root of your repository. Schema change Currently, on a push to GitLab, GitLab sends a web-hook to the CI Coordinator. The coordinator creates a build based on the jobs that are defined in its UI, which can then be executed by the connected Runners. In the new schema, GitLab sends the web-hook and the .gitlab-ci.yml contents to the CI Coordinator, which creates builds based on the yml file. In turn, these builds are executed by the Runners as before. Migrating to new style Keeping two different ways of doing things would be a strain on development and support, not to mention confusing. So we're not just deprecating the old style of defining jobs, we're removing it entirely and will migrate existing jobs. Upon upgrading your existing jobs defined in the GitLab CI Coordinator will be converted into a YAML file with the new syntax. You can download this file at any time from the project settings. When the GitLab webhook triggers and doesn't transmit the content from .gitlab-ci.yml , the coordinator will use the converted YAML file instead. This makes migrating to the new style very easy. You can start by simply copy-pasting the contents of the converted YAML file to the root of your repository. Existing projects will continue to build successfully, yet new projects do not have the option to use anything else. An example .gitlab-ci.yml To get an idea of how the .gitlab-ci.yml will look, we've prepared an example for a Ruby on Rails project (such as GitLab itself). Of course, this is due to change as we're still working on this. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 # Refs to skip skip_refs: \"deploy*\" # Run before each script # Refs to skip skip_refs: \"deploy*\" # Run before each script before_script: - export PATH=$HOME/bin:/usr/local/bin:/usr/bin:/bin - gem install bundler - cp config/database.yml.mysql config/database.yml - cp config/gitlab.yml.example config/gitlab.yml - touch log/application.log - touch log/test.log - bundle install --without postgres production --jobs $(nproc) - \"bundle exec rake db:create RAILS_ENV=test\" # Parallel jobs, each line is a parallel build jobs: - script: \"rake spec\" runner: \"ruby,postgres\" name: \"Rspec\" - script: \"rake spinach\" runner: \"ruby,mysql\" name: \"Spinach\" tags: true branches: false # Parallel deploy jobs on_success: - \"cap deploy production\" - \"cap deploy staging\" UPDATE Dmitriy and Sytse spend some time thinking about file syntax. Scripting should be simple and memorable. Thats why we come with better proposal: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 before_script: - gem install bundler - bundle install - bundle exec rake db:create rspec: test: \"rake spec\" tags: - ruby - postgres only: - branches spinach: test: \"rake spinach\" tags: - ruby - mysql except: - tags staging: deploy: \"cap deploy stating\" tags: - capistrano - debian except: - stable production: deploy: - cap deploy production - cap notify tags: - capistrano - debian only: - master - /&#94;deploy-.*$/ Contribute GitLab is nothing without its community. Contribute or follow the development in the GitLab CI repository . Install GitLab on your own server in 2 minutes Let's do it! Install GitLab on your own server in 2 minutes → Browse all posts @gitlab on Twitter. For the latest and most detailed news followon Twitter. Get every GitLab blog post and stay up to date. Please enable JavaScript to view the <a href=\"http://disqus.com/?ref_noscript\">comments powered by Disqus.</a>"},{"title":"Improved organization permissions · GitHub","tags":"scm","url":"http://ciandcd.github.io/improved-organization-permissions-github.html","text":"From: https://github.com/blog/2020-improved-organization-permissions Organizations have always been the best way for teams to work together and collaborate on code. We're happy to announce major improvements to GitHub organization permissions . These improvements include new customizable member privileges, fine-grained team permissions, and more open communication. The improved permissions system gives your organization the flexibility to work the way you want. Here are just a few highlights: (Opt-in) Members can view and mention all teams, even when they're not on those teams. (Opt-in) Members can create repositories without help from an owner. Members can create new teams to self-organize with the people they work with. Owners can give just the right amount of access to contractors and interns by adding them to repositories without giving them the privileges of organization members. And many more! Learn about GitHub's improved organization permissions . All of these new features give your organization the ability to work together seamlessly without everyone needing to be an owner. Once these features launch, organization owners will be able to turn on new permissions as needed. Simply opt-in when you're ready. Early access We're rolling out our improved permissions system to a select group of users who will be asked to provide feedback over a short survey as part of the program. If you're interested in being one of the first to try it out on GitHub.com, sign your organization up for early access . In the next few months, every organization on GitHub.com will have the improved permissions system."},{"title":"Living la Vida Helix: Submitting Without Fear","tags":"scm","url":"http://ciandcd.github.io/living-la-vida-helix-submitting-without-fear.html","text":"From: http://www.perforce.com/blog/150609/living-la-vida-helix-submitting-without-fear One of the common complaints I hear about centralized version control systems is that they are scary. With every commit being immediately visible there is a feeling that you may screw up everything for your co-workers. What's worse is that you generally don't have the power to clean up after yourself. How many of us have had to sheepishly go ask the admin to obliterate something? With P4D (which we now call Helix Versioning Engine) becoming a proper DVCS, you now can manipulate history that has not yet been shared with other people. Than means you can commit to your heart's content, and then sweep through later to keep just the interesting commits. It also means that if you accidentally submit something you can deal with it. Just recently while doing some cleanup work in the Workshop I had just one of these cases. I'd like to walk you through what happened so that you can see how unsubmit and resubmit will help you. Setting the scene A user had reported that a number of files that I had added the day before had all of their line endings mangled. The files were already in the shared server, so I didn't want to run p4 unsubmit there, and anyway I feel it is important for my failures to remain on display for all to see. So I got to work updating the files. p4 fetch Everyting was up-to-date. Next to find the files with the bad line endings. grep -lIUr --color \"&#94;R\" I was lucky and it was just a handful of files. Thankfully turning Windows line endings into Unixones is a piece of cake with P4D. p4 client -o | sed s/LineEnd: local/LineEnd: share | p4 client -i Now to get the files synced with the correct line endings and submitted: p4 sync -f p4 submit -d \"Fixing up some busted line endings that snuck in\" All was well and good until I realized that in my excitement I'd mangled some solution files which probably wanted those '\\R's. Thankfully I hadn't pushed, so I could quickly clean up my mess. p4 changes -m1 p4 unsubmit @ 12345 I identified my last change number, and then unsubmitted it. At this point I had all of my changed files in a shelf. In this case I had only one changelist, but I still decided to use p4 resubmit to apply the change. p4 resubmit makes it easy to reapply the changes in order. p4 resubmit This kicks me into interactive mode. Because there is a lot you can do with resubmit and I always forget the options, I hit '?' to see the list. Specify next action ( l/m/e/c/r/R/s/d/b/v/V/a/q ) or ? for help: ? The following actions are available: c Modify the change description for this change m Merge this change, then submit if no conflicts e Merge this change, then exit for further editing r Interactively resolve this change, then submit if no conflicts a Add (squash) this change into the next unsubmitted change s Skip this change and move on to the next d Delete this change without submitting it b Begin again from the earliest remaining change l List the changes remaining to be processed v View the current change in short form V View the current change with full diffs R Display the status of resolved and unresolved merges q Quit the resubmit operation< ? Display this help. In this case I wanted to resubmit all of the files except the solution files, so I selected e That merged my change back in, but then dropped me back to the command prompt so I could further mangle the files. A quick revert got rid of the changed solution files, and then I used p4 resubmit -Re to resume the resubmit process. p4 revert ....sln p4 resubmit -Re P4D submitted the change again, and cleaned up the shelf for me since I no longer needed it. With that tidied up I was ready to push and share my changes with the community. p4 push Sharing that broken change wouldn't have been the end of the world, but I felt so much more in control being able to clean up those .sln files before pushing out my change. Ever wish you could undo a merge between branches? With p4 unsubmit you can. Helix Versioning Engine gives you a way to safely experiment, modifying history as need be to make sure the changes your coworkers see are the ones you want them to see. Interested in trying it yourself? You're just a download of our Helix Versioning Engine and p4 init away from being able to try this all yourself. If you'd like to push to a shared server the Workshop has been running 2015.1 since beta, and Helix Cloud is also using it. As always we're here to help, so if you have questions, just shout!"},{"title":"Meet & Greet and Workshop Tickets · GitHub","tags":"scm","url":"http://ciandcd.github.io/meet-greet-and-workshop-tickets-github.html","text":"From: https://github.com/blog/2027-codeconf-updates-meet-greet-and-workshop-tickets CodeConf is next week, and I couldn't be more excited to bring the open source community together to exchange ideas and have some fun in Nashville. There are a few updates I'd like to share: On June 24, the day before the conference, we'll be hosting a meet & greet for attendees who would like to register early. This event is free and open to the public, so if you aren't attending CodeConf but live in the Nashville area and would like to stop by, grab a ticket here . We'll be congregating on the second floor of Acme Feed & Seed downtown beginning at 5:30pm The workshop schedule has been updated, and I have opened up more space in each session for those interested. If you'd like to snag one of the newly available tickets, go for it! There's still time to grab a CodeConf ticket. Take a look at the website for the full schedule of sessions, workshops, and sponsors. I hope to see you in Nashville."},{"title":"Merging Without a Base in Perforce Helix","tags":"scm","url":"http://ciandcd.github.io/merging-without-a-base-in-perforce-helix.html","text":"From: http://www.perforce.com/blog/150623/merging-without-base-perforce-helix I recently had a question about merging files in Perforce Helix that have no direct lineage. For example, let's say you want to merge file //depot/main/foo.c (source) to //depot/dev/foo.c (target), which is fine, but you realize the target file was not branched from the source and there is no base (common ancestor). To determine the base, Helix uses integration history created by previous integration commands to know which file revisions to integrate. However, since dev/foo.c was not branched from main/foo.c there is no integration history between these two paths: a baseless merge. To handle this scenario Helix does a two-way merge for the best results. All the diffs are considered conflicts. Why? Because in the absence of a base, we do not have any way to determine what differences are \"changes\" relative to the base, which is how we normally determine whether diffs are conflicting or not. We found the best and most accurate way to handle this scenario is scheduling a two-way merge. In the past, Helix would not allow baseless merges without the use of p4 integ command flags; -i/-I uses the first revision of the source as the base for baseless merges, rather than an empty file. With the old \"-i\" behavior, an arbitrarily chosen base can lead to lost changes as seen in the figure below. This is why we changed the behavior. If you are stuck in the past, these flags are preserved for backward compatibility but are deprecated. To recap, with the current integration behavior, no flags are needed to handle baseless merges. The Helix integration engine does the right thing for a more realistic and accurate result! Happy merging!"},{"title":"Mobile File Finder · GitHub","tags":"scm","url":"http://ciandcd.github.io/mobile-file-finder-github.html","text":"From: https://github.com/blog/2032-mobile-file-finder The GitHub File Finder is now available on your mobile device. Just click the \"Jump to file\" link on any repository."},{"title":"News from InfoSecurity 2015","tags":"scm","url":"http://ciandcd.github.io/news-from-infosecurity-2015.html","text":"From: http://www.perforce.com/blog/150610/news-infosecurity-2015-0 I've just returned from my first visit to InfoSecurity 2015 in London . With the launch earlier this year of Perforce Helix Threat Detection this was a great opportunity to review the state of the cyber-security world, hear about the key challenges facing governments, businesses and individuals and review some of the solutions being offered. This couldn't be more timely as, right when I'm writing this, it has been announced that the U.S. government suffered a serious attack earlier this year which compromised the personal details of thousands of federal employees. My particular area of interest is the emerging role of security in DevOps. There are a few key aspects to consider: As a developer what do you need to do and how does that fit with agile and development processes? As a Release Manager/Operations Specialist/DevOps Engineer what do you need to know to roll out and manage secure applications? As a Chief Information Security Officer or Risk Manager what is going on in the development and operations areas that I ought to be concerned about? I haven't got space here to cover all of these topics, but here are a few highlights from the conversations I had at the conference. Development Managers and DevOps specialists are increasingly aware of the need for secure applications. They are concerned that as release cycle times reduce with the adoption of Continuous Delivery they don't reduce security nor slow down deliveries. Some companies are working out how to do this by involving security experts in the earliest stages of sprint planning and ensuring security stories are \"groomed\" to ensure they are properly positioned for priority in their backlogs. They're also adopting tools for automated code and application validation. It was interesting to see an increasing number of tools addressing the need for dynamic security testing. Although the term seems to have been around a few years already, there were a number of people talking about \" Rugged DevOps \" and I think this is an area that will continue to grow. Security experts, especially those involved in IT audits or risk assessments are busier than ever. Some are aware of the potential risks that may exist in their development organizations but I suspect the majority are not. This is the result of two issues. Firstly, they may not fully appreciate the value of the software being developed. They know that they need to protect customer and staff personal data, but they don't necessarily realize that the software is actually their company's competitive differentiator and could be critical if leaked to a competitor. Secondly there is a lot of technology involved that they don't understand. They may be familiar with firewalls, VPNs, email, etc., but developers often bring tools into the business without their knowledge and these tools, such as Subversion or Git are inherently vulnerable. It's increasingly hard to keep track of business documents in a world full of email, cloud file sharing services and BYOD mobiles, but this technical software content is even harder to grasp. I saw a number of tools that try to address some of these problems by monitoring network traffic rather than trying to lock down each application. This generates another problem though – if you're monitoring hundreds or thousands of different file types and communications, it quickly becomes an impossible management challenge. A few tools are trying to address that problem by using analytics to analyze the basic data and infer what looks like suspicious behavior. This helps with the management issue but they still don't understand the context of the data being moved around the organization which makes them inefficient for DevOps. I didn't see anything that was close to Perforce Helix Threat Detection , which focuses on protecting this valuable IP being created by design and development teams. Because it uses the rich data available from the Helix Versioning Engine it understands the context of the files being accessed. It can not only track that a user may be accessing more files than usual (and most tools can't work out what \"normal\" means), but it also understands whether those files are in projects they \"normally\" use or whether they're using the files in ways that are unusual for the user. I'm really looking forward to the webinar Perforce are hosting on June 16th where the Forrester DevOps Analysts, Kirt Bittner, and Security Analyst, Rick Holland will talk about the issues raised above and the solutions to them."},{"title":"Next Round of Online Training: Helix DVCS","tags":"scm","url":"http://ciandcd.github.io/next-round-of-online-training-helix-dvcs.html","text":"From: http://www.perforce.com/blog/150612/next-round-online-training-helix-dvcs Back in March, we announced our new Helix platform which includes highly anticipated distributed version control (DVCS) capabilities. For some of you, hearing that we now offer DVCS may have been music to your ears. For others, it may have invoked curiosity and added another acronym to your lexicon. Given the recent proliferation of Git-style workflows, we are seeing a lot of developers finding themselves working with their own private, local versioning repositories while collaborating with teammates via the new init/clone/pull/push command set. But did you know that you can now rewrite the history of changes in your personal repository before sharing them? To help you come up to speed with Helix DVCS, we are pleased to announce the availability of a new instructor-led training course. The Helix DVCS course will take place online via Webex and will include hands-on lab exercises within our lab environment. The class is taught by our expert Professional Services consultants who have a lot of experience advising customers. Topics on this new half-day course will include: Why do you need DVCS? Overview of DVCS architecture and workflows Basic DVCS operations How to perform initial setup Working with multiple streams Rewriting history The first class is now scheduled for European customers on June 29, 9am – 1pm, British Summer Time (GMT+1). We will have a DVCS training for North American customers in the weeks that follow. So sign up here and bring along your questions about DVCS. The class does assume that you are already familiar with Perforce Helix, so if you're new to Helix, we also offer introductory courses. Check out our course schedule for details. Any questions, just email training@perforce.com."},{"title":"Note on license expiration in GitLab 7.10.5 EE","tags":"scm","url":"http://ciandcd.github.io/note-on-license-expiration-in-gitlab-7105-ee.html","text":"From: https://www.gitlab.com/2015/06/04/note-on-license-expiration-in-gitlab-7-dot-10-dot-5-ee/ Note on license expiration in GitLab 7.10.5 EE If you're upgrading to GitLab Enterprise Edition 7.11, which introduces licenses keys, you're probably planning to upgrade to 7.10.5 first. This way you are able to upload your license key in advance . One of our customers notified us of a faulty description in the license uploader in GitLab 7.10.5. Upon uploading, the license is checked properly, however the text in the license view in the admin page in GitLab will show: While it should look like this: This only occurs in GitLab 7.10.5 and does not affect functionality. The license information is correctly shown in GitLab 7.11 and up. If you have any questions or comments do not hesitate to comment below or contact support. Install GitLab on your own server in 2 minutes Let's do it! Install GitLab on your own server in 2 minutes → Browse all posts @gitlab on Twitter. For the latest and most detailed news followon Twitter. Get every GitLab blog post and stay up to date. Please enable JavaScript to view the <a href=\"http://disqus.com/?ref_noscript\">comments powered by Disqus.</a>"},{"title":"Octicon Buttons Are Here! · GitHub","tags":"scm","url":"http://ciandcd.github.io/octicon-buttons-are-here-github.html","text":"From: https://github.com/blog/2030-octicon-buttons-are-here Graphs, and pencils, and locks...Oh my! Now you can collect themed Octicon buttons with the four new button packs offered in the GitHub Shop ."},{"title":"P4Python goes pip","tags":"scm","url":"http://ciandcd.github.io/p4python-goes-pip.html","text":"From: http://www.perforce.com/blog/150514/p4python-goes-pip-0 In recent years Python has changed its package manager strategy, and the result is pip . Pip is a powerful package manager that simplifies the creation and consumption of Python packages, turning the Python Package Index into a hub of an ever-growing number of useful packages. P4Python always had to stay away from the package index because it requires binary builds for some platforms. With the advent of the wheel format, this has changed. Wheels are Python packages that can contain binary builds on Windows and OSX, allowing package creators to precompile their packages. P4Python 2015.1 has been uploaded to the Python Package Index. For you this means installing the latest release of P4Python becomes a simple: pip install p4python Easier, isn't it? However, there are a few preparations you need to make before you can run this command the first time successfully: You need to have the right version of Python installed: 2.7, 3.3 or 3.4. Python 2.6 is supported but we have not uploaded binaries for it. You need to install pip. It comes with Python 3.4 and 2.7.9 automatically; everyone else will need to install it, for example from here , and make sure the pip executable is in your PATH. There is currently no binary wheel format available for Linux, so pip will download the source code of P4Python and attempt to build it. This requires ‘python-dev' and ‘build-essential' installed on Debian-based distributions (using apt) and the equivalent on RPM-based (using yum). In order to build P4Python automatically from Pip, setup.py has also gained some new tricks. First of all, it now uses setuptools and not distutils, so you need to have setuptools installed if you want to build it on, say, Python 2.6. If you run setup.py without the –apidir option, setup will now go off and attempt to download the correct P4API binaries from the Perforce FTP site for your platform. The API will be downloaded and unpacked into the temp directory and used automatically by the build process. You can still download the source or the packages, of course, and install P4Python the traditional way if you prefer. I found pip amazingly simple to use, and I hope it will make your life a lot easier when installing P4Python. As usual, if you have any issues or requests, let us know in Perforce Support or ping me on my Twitter handle @p4sven . Happy hacking."},{"title":"Perforce Takes to the Road in 2015","tags":"scm","url":"http://ciandcd.github.io/perforce-takes-to-the-road-in-2015.html","text":"From: http://www.perforce.com/blog/150622/perforce-takes-road-2015 We're hitting the road again this year with a series of one-day events focused on better ways to build and secure complex products. These events will prove interesting to professionals at every phase of the product lifecyle and from companies of any size. Register today for in-depth discussions of the challenges companies face in getting complex products to market quickly without sacrificing quality or security. These events are also great opportunities to network with your peers and to hear about the latest innovations from Perforce. No matter if you're a Perforce user or not, you'll find it a valuable use of your time. Hear from the Experts Hear from thought leaders who are well practiced in modern development practices like Continuous Delivery and DevOps. You'll get practical advice that you can put to good use immediately. Keep Good Company Spend a day with like-minded individuals and innovative product developers. The majority of sessions will be by Perforce customers, who will share practical advice from their real-world experiences. Of course, our trainers and consultants will also be on hand to answer any questions you have about new products and capabilities from Perforce. Stay for the Party! These events are free of charge and full of informational sessions. They also promise to be very fun. No Perforce event is complete without an elegant setting, great food and lavish drinks to end the day in style. You won't be disappointed! So far, we've scheduled tour stops in the following cities (with more to come): Milan – June 25 Sydney – July 21 Berlin – Sept 16 London – Sept 22 Space is limited, so register today and secure your place!"},{"title":"Read-only deploy keys · GitHub","tags":"scm","url":"http://ciandcd.github.io/read-only-deploy-keys-github.html","text":"From: https://github.com/blog/2024-read-only-deploy-keys You can now create deploy keys with read-only access. A deploy key is an SSH key that is stored on your server and grants access to a single GitHub repository. They are often used to clone repositories during deploys or continuous integration runs. Deploys sometimes involve merging branches and pushing code, so deploy keys have always allowed both read and write access. Because write access is undesirable in many cases, you now have the ability to create deploy keys with read-only access. New deploy keys created through GitHub.com will be read-only by default and can be given write access by selecting \"Allow write access\" during creation. Access level can be specified when creating deploy keys from the API as well."},{"title":"Release Manager - The invisible hero","tags":"scm","url":"http://ciandcd.github.io/release-manager-the-invisible-hero.html","text":"From: https://www.gitlab.com/2015/06/25/release-manager-the-invisible-hero/ Real heroes are sometimes unknown and we can only see their accomplishments. In GitLab we have one invisible hero every month, when we have our monthly release. As you may know, we've never failed to release a new GitLab version on the 22nd of every month. As GitLab grows, the release process becomes more complex and becoming a release manager is a more difficult, but a necessary job. Eight working days before the next release, and we start the countdown. A new volunteer \"hero\" is elected by the team. But, why is it such a challenging job? A release manager is the person who makes sure that everything is ready for the monthly release. They follow up on every single detail and make sure that the new version is working perfectly, including all the improvements and features. They also need to delegate some tasks and make sure that the procedure is being followed. Consider that right now, GitLab is huge. Our community dishes out around 900 commits a month on GitLab alone. Add Enterprise Edition, GitLab CI and runners, Omnibus-GitLab packages and you get several thousand changes done by hundreds of developers across projects which need to come together (and work) in one day. This is a lot of responsibility for one person. So, how do we manage to make it all into a single release every month? In GitLab we have a release directory for the release documents. The most powerful document for the release is called monthly.md . Release manager tasks can be broken down into: Make sure that GitLab CE, EE and GitLab CI repositories have an updated installation and upgraded guides Make sure that the Omnibus-GitLab package will be ready for the release Release the RC version, do QA, deploy on GitLab.com and ci.GitLab.com Follow reported regressions and make sure that developers are aware/working on a fix Decide which fixes can go into the release Coordinate the package building Make sure that the blog post contains all the necessary information Do the final release Decide if there needs to be a patch release Coordinate patch release A release manager volunteers to work late (or early) to get the packages out or deploy the new version to one of our services. No one is forcing you to do so, but if you don't, it will complicate the following day. This is a weakness in our process, so we need to work on improving this situation. History I don't know the exact date when the release manager duty was thought off but it was around version 6.4 . At that time, we had a couple of other things that were the release manager tasks: Notify everyone of the code-freeze (nothing was merged to master during this time), enforce it and build the packages manually. Yes, manually. This meant connecting to all machines separately and doing few commands to initiate package building. GitLab.com had a separate repository with some custom code, so the deploy needed to be done manually too. I still have nightmares as a result of these 2 things. As you can imagine, this made the release manager tasks very undesirable and limited to a few people. Even with all the improvements that followed, this job is still not popular. Improvements Since the painful beginings of the release manager tasks, we've done number of improvements. We did a massive change to the process and made it even more continuous integration oriented than it was before. There are risks to it, but also massive gains: Code freeze was removed so there is no need to watch over anyone's shoulders Keeping X git repos in sync. Syncing repositories is now a one-line script where the argument is the version that is being released Automatizing our release process. Omnibus-gitlab packages infrastructure got built, so only supplying the shas of the release version is enough to kick off the automatic builds on all platforms and machines Infrastructure for deploying GitLab.com and ci.gitlab.com got created and they are being updated by using a few lines of commands and packages The release documentation has been updated so many times that room for error is minimal (if you follow the steps closely) You would expect that all these improvements would make the Release manager job more appealing since you get to: Boss around over all of your colleagues. This includes the project lead and the CEO. It is especially sweet when you can say NO to an unreasonable request. After all, all requests are unreasonable but your own and now you get to push that through You decide at your leasure when something will be included and pushed You are the boss of everything (for a period of time) because everyone says: \"Hey, you are the release manager, your call\" With all the hard work, how do we choose a volunteer release manager? Choosing the release manager is probably one of the hardest tasks. During our team call, the release manager for the previous release mentions the subject of selecting a new release manager. At that exact moment, there's silence, cameras and mics start breaking down, people forget the whole English language, there is always someone at the door so you need to open it and lots of faces are just looking around the room. After a few minutes of silence, decision is made, but mostly because we are all friends and we don't want to see a colleague suffer for another month. We've tried improving the desirablity of this task by making procedures easier, but that is still a challenge. At some point I've asked what kind of reward we could put forward to make people happy to volunteer, but there are no good ideas yet. My ideas where limited to: Material reward: a gift might be OK for some people, but others have no need for things. In this case we could publicly thank them and acknowledge their work. \"Spiritual\" reward: We do say \"thank you\" to the RM a lot, but this gets spent. Tweeting the name of the release manager might work as a recongnition for some, but I am afraid that it won't work for introverts in our team. Being more public might also yield more work for them. Buying a beer or cocktail: This feels like something that would be appreciated, but it would only work for a few employees, since we are a very remote company. Maybe a beer voucher could be sent. With that I was out of ideas. This blog post is an attempt to say a thank you to all the release managers. You know who you are and you are a true invisible hero for accomplishing the tasks to make everything go out on schedule. Do you have any ideas? Release manager - my hero. Install GitLab on your own server in 2 minutes Let's do it! Install GitLab on your own server in 2 minutes → Browse all posts @gitlab on Twitter. For the latest and most detailed news followon Twitter. Get every GitLab blog post and stay up to date. Please enable JavaScript to view the <a href=\"http://disqus.com/?ref_noscript\">comments powered by Disqus.</a>"},{"title":"Security advisory for Logjam vulnerability","tags":"scm","url":"http://ciandcd.github.io/security-advisory-for-logjam-vulnerability.html","text":"From: https://www.gitlab.com/2015/05/21/security-advisory-for-logjam-vulnerability/ Security advisory for Logjam vulnerability A recently announced Logjam vulnerability allows an attacker to do a man-in-the-middle attack, allowing them to downgrade a TLS connection to 512-bit DH parameters. More details on what that is and means can be found on openssl blog . Impact on GitLab GitLab is using, by default, up-to-date SSL ciphers: Export Cipher Suites are not used. Elliptic-Curve Diffie-Hellman ciphers are used By default, 1024-bit DH groups are used This means that GitLab is safe in principle. When using 1028-bit DH groups there is a small chance that an attacker with nation-state resources could be eavesdropping. If you find this insufficient for your GitLab installation, you can generate 2048-bit DH groups and enable the ssl_dhparam option in NGINX config. Params can be generated with: 1 openssl dhparam -out dhparams.pem 2048 After the dhparams.pem file has been generated you will need to tell Nginx where the file is located: GitLab installations using omnibus-gitlab packages For packages version 7.11.0 and up. Place the dhparams.pem file in /etc/gitlab/ssl/ directory. In /etc/gitlab/gitlab.rb , enable the following setting: 1 nginx [ 'ssl_dhparam' ] = \"/etc/gitlab/ssl/dhparams.pem\" and do sudo gitlab-ctl reconfigure . More information can be found in the omnibus-gitlab nginx documentation . Workaround for packages prior to version 7.11.0 Place the dhparams.pem file in /etc/gitlab/ssl/ directory. In /etc/gitlab/gitlab.rb , enable the following setting: 1 nginx [ 'custom_gitlab_server_config' ] = \"ssl_dhparam /etc/gitlab/ssl/dhparams.pem; \\n \" and run sudo gitlab-ctl reconfigure . GitLab installations from source Place the generated dhparams.pem in a suitable location, for example /etc/nginx/ssl/dhparams.pem . In GitLab nginx config find ssl_dhparam config and set it to ssl_dhparam /etc/nginx/ssl/dhparams.pem; . Reload your nginx config. Impact on GitLab.com GitLab.com is using 1028-bit DH groups. Due to incompatibilities with older Java-based clients we haven't enabled 2048-bit DH params yet as this would prevent some people from using GitLab.com. We are looking into ways to keep a good SSLlabs score and allowing users with older Java-base clients to use GitLab.com. We are examining the impact of this and we will update this blog post once we have more information. Install GitLab on your own server in 2 minutes Let's do it! Install GitLab on your own server in 2 minutes → Browse all posts @gitlab on Twitter. For the latest and most detailed news followon Twitter. Get every GitLab blog post and stay up to date. Please enable JavaScript to view the <a href=\"http://disqus.com/?ref_noscript\">comments powered by Disqus.</a>"},{"title":"Simple words for a GitLab Newbie","tags":"scm","url":"http://ciandcd.github.io/simple-words-for-a-gitlab-newbie.html","text":"From: https://www.gitlab.com/2015/05/18/simple-words-for-a-gitlab-newbie/ For most of us, when we work with a new tool, there's a process of learning the right vocabulary and the best steps to make things happen; this while we try to keep the best attitude. Not very long ago, I learned how to use Git and GitLab and it was a little bit painful. I read a lot about it, but it was mostly vocabulary that didn't make any sense to me. If you've been there or if you are there now, you'll know what I'm talking about (some people may have it naturally). So, to make this learning process easier for others, I took many of the basic Git vocabulary and wrote easy definitions for each word. I hope they are useful for you and please share them with your Git and Gitlab newbie friends! Cloud Based Services What is a cloud based service? It's a service or resource that is opposed to services that are hosted on the servers inside a company, which is the traditional way of doing it. It helps people and companies lower their costs and be more efficient while helping with different functions such as trannings, storage, etc. GitLab.com is a cloud based service because it can be hosted both in house and in the cloud. Source control or revision control software What is source control? It's a system that records and manages changes to projects, files and documents. It helps you recall specific versions later. It also makes it easier to collaborate, because it shows who has changed what and helps you combine contributions. Continuous Integration What is continuous integration? It's the system of continuously incorporating the work advances with a shared mainline in a project. Git and GitLab together make continuous integration happen. Continuous deployment What is continuous deployment? It means that whenever there is a change to the code, it is deployed or made live immediately. This is in contrast to continous integration, where code is continuously being merged in the mainline and is always ready to be deployed, rather than actually deployed. When people talk about CI and CD what they usually mean to say is that they are constantly and automatically testing their code against their tests using a tool such as GitLab CI and upon passing to a certain action. That action could be merging the code into a branch (master, production, etc), deploying it to a server or building a package / piece of software out of it. Non-continuous integration would be everyone working on something and only integrating all the work as the very last step. Obviously, that results in many conflicts and issues, which is why CI is adopted widely nowadays. Git What is Git? Git is a system where you can create projects of different sizes with speed and efficiency. It helps you manage code, communicate and collaborate on different software projects. Git will allow you to go back to a previous status on a project or to see its entire evolution since the project was created. You could think of it as a time machine which will allow you to go back in time to whenever you'd like in your project. With Git, 3 basic issues were solved when working on projects: 1. It became easier to manage large projects. 2. It helps you avoid overwriting the team's advances and work. 3. With git, you just pull the entire code and history to your machine, so you can calmly work in your own little space without interference or boundaries. It's much simpler and much more light-weight. Repository What is a repository? The place where the history of your work is stored. Remote repository What is a remote repository? It's a repository that is not-on-your-machine, so it's anything that is not your computer. Usually, it is online, GitLab.com for instance. The main remote repository is usually called \"Origin\". Commit What is a commit? It's the way you call the latest changes of source code that you made on a repository. When changes are tracked, commits mark the changes on a document. Master What is a master? It's how you call the main and definitive branch (the independent line of development of a project). Branch What is a branch? It's an independent line of development. They are a brand new working directory, staging area, and project history. New commits are recorded in the history for the current branch, which results in taking the source from someone's repository (the place where the history of your work is stored) at certain point in time, and apply your own changes to it in the history of the project. Fork What is a fork? It's a copy of an original repository (the place where the history of your work is stored) that you can put somewhere else or where you can experiment and apply changes that you can later decide if publishing or not, without affecting your original project. Git Clone What is a clone? It's to get a copy of a git project to look at or to use the code. Git Merge What is to merge? It's integrating separate changes that you made to a project, on different branches. md: markdown What is markdown? It's a plain text format that will make any document easy-to-write and easy-to-read. Push a repository What is to push a repository? It's to incorporate a local branch (the independent line of development of a project) to a remote repository (online version of your project). README.md What is a README.md? I't a file in a simple format which summarizes a repository. If there's also a README (without the .md), the README.md will have priority. SSH (secure shell protocol) What is SSH? It's how you call the commands that help communicate through a network and that are encrypted and secure. It's used for remote logins and it helps users connect to a server in a secure way. Stage Files What is to stage a file? It's how you call the act of preparing a file for a commit (the latest changes of source code in a repository). GitLab What is GitLab? GitLab is an online Git repository manager with a wiki, issue tracking, CI and CD. It is a great way to manage git repositories on a centralized server. GitLab gives you complete control over your repositories or projects and allows you to decide whether they are public or private for free. GitLab.com GitLab.com hosts your (private) software projects for free. It offers free public and private repositories, issue-tracking and wikis. It runs GitLab Enterprise Edition and GitLab CI. No installation required, you can just sign up for a free account. Support Package: Free subscribers can use the GitLab.com Support Forum if they have questions. GitLab.com Bronze Support will let you email support directly for timely, personal and private answers. This costs $9.99 per user per year for next-business-day response time and is available in packs of 20 users. GitLab Community Edition (CE) Free, self hosted application where you can get support from the Community Feature rich: Git repository management, code reviews, issue tracking, activity feeds and wikis. It comes with GitLab CI for continuous integration and delivery. Open Source: MIT licensed, community driven, 700+ contributors, inspect and modify the source, easy to integrate into your infrastructure. Scalable: support 25,000 users on one server or a highly available active/active cluster. Merge requests with line-by-line comments, CI and issue tracker integrations. GitLab Enterprise Edition (EE) Self hosted application that comes with additional support. Builds on top of the Community Edition and includes extra features mainly aimed at organizations with more than 100 users. It has LDAP group sync, audit logs and multiple roles. It includes deeper authentication and authorization integration, has fine-grained workflow management, has extra server management options and it integrates with your tool stack. GitLab EE runs on your servers. GitLab Continuous Integration (CI) Free, self hosted application that integrates with GitLab CE/EE. Also availble as SaaS at ci.gitlab.com. Easy to set up since it is included in Omnibus packages of GitLab or use it for free on ci.gitlab.com. Beautiful interface with a clear menu structure. Performant and stable, as tests run distributed on separate machines. Will help you receive test results faster with each commit running in parallel on multiple jobs. Free to use and completely open source. All CI code is MIT licensed. Install GitLab on your own server in 2 minutes Let's do it! Install GitLab on your own server in 2 minutes → Browse all posts @gitlab on Twitter. For the latest and most detailed news followon Twitter. Get every GitLab blog post and stay up to date. Please enable JavaScript to view the <a href=\"http://disqus.com/?ref_noscript\">comments powered by Disqus.</a>"},{"title":"Student hackathon organizers, join us for Hackcon at GitHub HQ · GitHub","tags":"scm","url":"http://ciandcd.github.io/student-hackathon-organizers-join-us-for-hackcon-at-github-hq-github.html","text":"From: https://github.com/blog/2026-student-hackathon-organizers-join-us-for-hackcon-at-github-hq We're hosting Hackcon III at our San Francisco office on July 18th and 19th. Hackcon is the place to be for student hackathon organizers. The event is run by our friends at Major League Hacking and will bring together 150 student leaders for two days of talks and workshops. Participants will share experience and best practices in everything from starting a campus group to producing large scale campus events. If you lead a student hacker community at your university, we'd love to see you at Hackcon. You can find more information about the event and pre-register at hackcon.io . You can also check out the videos from Hackcon I and Hackcon II on YouTube."},{"title":"Support LGBTQ tech organizations with the Pridetocat Shirt · GitHub","tags":"scm","url":"http://ciandcd.github.io/support-lgbtq-tech-organizations-with-the-pridetocat-shirt-github.html","text":"From: https://github.com/blog/2016-support-lgbtq-tech-organizations-with-the-pridetocat-shirt With the purchase of the Pridetocat Shirt you will be assisting Lesbians Who Tech , Maven , and Trans*H4CK to further their work. All proceeds from sales will be donated to these organizations that are helping educate, connect and empower LGBTQ people in tech. This limited edition shirt is available in the GitHub Shop until August 31st. More info about the LGBTQ tech organizations that benefit from the purchase of this shirt: Lesbians Who Tech Lesbians Who Tech is a global community of 9,000 queer women in tech. It exists to provide value to queer women in tech, a demographic that is rarely represented in both the tech community and the LGBTQ community. Trans*H4CK Trans*H4CK is a hackathon and speaker series that tackles social problems by developing new and useful open source tech products that benefit the trans and gender non-conforming communities, while bringing visibility to transgender tech innovators and entrepreneurs. Maven Maven partner with local LGBTQA youth serving organizations and LGBTQA tech professionals to provide free tech camps, workshops, Game Jams/hackathons for the queer youth community."},{"title":"Test early, test often. Continuous testing as part of the DevOps lifecycle","tags":"devops","url":"http://ciandcd.github.io/test-early-test-often-continuous-testing-as-part-of-the-devops-lifecycle.html","text":"From: http://devops.linuxjournal.com/develop-deploy/test-early-test-often-continuous-testing-part-devops-lifecycle Continuous testing is an effective quality management method that can help testing teams keep up with agile development. This paper explains the methods and approaches that can enable continuous testing as part of the DevOps lifecycle."},{"title":"There is only one DevOps Connect","tags":"devops","url":"http://ciandcd.github.io/there-is-only-one-devops-connect.html","text":"From: http://devops.com/2015/06/25/there-is-only-one-devops-connect/ They say that imitation is the sincerest form of flattery. If that is true I guess the folks over Informa Telecoms and Media Ltd are very, very sincere. Why else would they change the name of their DevOps Conference from \"DevOps Summit\" (which name they \"borrowed\" from the Sys-Con folks) to DevOps Connect Europe. No matter that we here at DevOps.com have already held several events in Europe and London specifically with our DevOps Connect name. No matter that we have a website and own the domain http://www.devopsconnect.com . Why shouldn't they just use our name and employ the \"colorful\" technique of registering a hyphenated play on the domain name? I guess that alone should tell you all you need to know about this company and the events they produce. Now of course we can go hire some International copyright attorneys and litigate first usage and ask for injunctions and damages. But why bother? I am a big believer in letting the market be the judge, jury and executioner. I am perfectly fine with using DevOps.com to get out the word about these kinds of business practices and letting the market act. The event these people want to put on will probably be no better than the several DevOps Summit events they put on previously. I have heard from exhibitors, speakers and attendees of those events that they were less than successful. Mind you I haven't attended any, just reporting on what I was told. Ultimately though if they were successful they wouldn't be trying to rebrand using someone else's name. Hard to argue with that logic. I am asking our readers (and there are many of you in London, as it is our single biggest city for viewers) to let this company know that copying others business names will not stand. The best way to do this is to not attend, don't participate and get the word out that this is not the way business is done. Don't be fooled by the attempt to piggyback our brand and success, we have nothing to do with that event. In fact we will be holding several DevOps Connect events in London and elsewhere in Europe in the fall. Details will be announced shortly but there will be lots of opportunities to participate. Some of you may ask, \"you haven't given us the link to this event?\" You're right, I have not and for good reason. I have already given this company and their event more ink than they are probably due. So be aware of imitators, remember there is only one DevOps Connect and those are the events put on by us at DevOps.com. We hope to see you at them and will be sure to let everyone know when we do. Until then don't reward imitators."},{"title":"Visualizing and defining requirements comes to DevOps","tags":"devops","url":"http://ciandcd.github.io/visualizing-and-defining-requirements-comes-to-devops.html","text":"From: http://devops.com/2015/06/26/visualizing-and-defining-requirements-comes-to-devops/ We have all seen the nice circular diagrams of a DevOps methodology for application lifecycle management (ALM) like the one above. We interject some automation, make the two halves work closer together, get some feedback loops going and go as fast as we can. But lets look at those first two steps in the diagram, the ones labeled consult and design. What and how do we accomplish this? There is a company called iRise that has built a great business by allowing for rapidly visualizing and defining requirements for building applications. It is high time we recognize this as the important piece of the puzzle that it is. I recently had a chance to meet with some of the executive team of iRise while I was in London for the CD Summit. The premise of iRise is so simple I was thinking to myself, \"why hasn't anyone else thought of this?\" If the old adage of crap in is crap out is true, why wouldn't we take the time to define and plan our software better early on? I know everyone thinks they do, but do they really? Do we quickly visualize to help define the requirements and to help the team buy in and sign off? If it is set up right to begin with, it flows a lot faster later. Now the folks at Tasktop creators of the Eclipse Mylyn open source tool have teamed with iRise to OEM iRise into the Tasktop Software Lifecycle Integration system. In speaking with Stephen Brickley, EVP of iRise, he thinks this partnership will be a key factor in bringing iRise into the mainstream DevOps tools market. iRise's customers, like software departments everywhere are adopting new ways of developing and operating software. New times and new ways don't mean that you throw out what works though. To me it is a no brainer. If you have a tool that lets you quickly visualize, prototype and communicate to the rest of the team before investing a lot of effort into development, it really has to help. It is the ultimate shift left. I guess the issue is how quickly can you do this without adding extra steps to the development process. From what I saw of iRise it makes it really easy and fast. Of course I would be interested to hear from anyone who has used iRise or even a similar tool on your own experience. But if it all is as it seems, I would expect iRise to be a welcome addition to the DevOps toolbox. What do you think? What tool do you use for this now or do you just skip this in planning and initial requirements? We are always on the lookout for new tools to report on at DevOps.com, but ultimately we depend on our readers to tell us what is really useful or not. So let us know."},{"title":"What Different Security Testing Methodologies Are Out There?","tags":"devops","url":"http://ciandcd.github.io/what-different-security-testing-methodologies-are-out-there.html","text":"From: http://java.dzone.com/articles/what-different-security Every business has unique characteristics that set it apart from other organizations, even within the same industry. For this reason, it shouldn't be surprising that there's not a one-size-fits-all approach to app security testing. Each company has certain protection expectations and regulations to adhere to, making it essential to find the best way to achieve these goals. Here are a few examples of security testing methodologies available for quality assurance teams to leverage: Black box With black box testing, QA professionals put themselves in the shoes of the hacker and attempt to break the app through various attack vectors. This methodology can yield a lot of information and help better secure the program from actual threats. A white paper by Security Innovation noted that software testers first analyze the system's architecture and business model to identify any security vulnerabilities. Looking over the software logic in this way can uncover subtle security and privacy issues that may not have been noticed otherwise, such as defects in design, input, system dependency, authentication, cryptography and information disclosure. \"Although white box code inspection is good for analyzing static behavior, only black box exploratory testing can determine the dynamic behavior of how a system is implemented and used, the coupling between systems and the interactions of the distributed systems,\" Security Innovation wrote. Dynamic For QA teams that like to execute code, dynamic testing is the approach for them. This methodology checks the running application for how it behaves and responds to a variety of inputs. This is done to ensure that the product meets up with established regulations and is giving the expected outcomes. IBM noted that dynamic analysis is especially useful to identify code coverage, as it can discover bugs in paths that have gone untested. While dynamic testing can be manual work for testers, it can also yield significant information that will help mitigate defects and produce quality products. Static In contrast to dynamic testing, static approaches directly review the source code, often through an automated test management solution . TechTarget contributor Michael Cobb noted that this methodology occurs at the implementation phase, rather than when the app is running, and often helps mitigate vulnerabilities involved with industry compliance standards. Automation in this area can reduce the amount of time it takes to complete these tasks. However, it may not be able to detect sophisticated threats, which can be supplemented by dynamic security testing . \"A thorough source code review has an advantage over dynamic testing,\" Cobb wrote. \"Nothing is hidden from analysts during a source code review, so they can examine exactly how data flows through a program. By solving the problem at the code level, static testing reduces the number of security-related design and coding defects, and the severity of any defects that make it through to the release version, thus dramatically improving the overall security of the application.\" There are a number of security testing methodologies that organizations can pursue, and designing a combination of approaches may result with a solution that's best for their needs. Using these strategies, companies can better ensure the protection of sensitive information while providing users with the software testing tools needed to succeed."},{"title":"Why is Git better than Mercurial?","tags":"devops","url":"http://ciandcd.github.io/why-is-git-better-than-mercurial.html","text":"From: http://java.dzone.com/articles/why-git-better-mercurial Which of the two main distributed version control systems (DVCS), Mecurial and GIT, is better and why? This is an old argument, with forum posts galore about which is better; with users of both arguing their case, however this argument never seems to be resolved. This is because the feature sets are so similar, and after researching the various strengths and weaknesses it became clear that they both have clear advantages for certain situations, – different use cases. Below we have listed both pros and cons for both Git and Mercurial in order for our readers to be able to make their own decision as to what is best for them, after all user requirements and team requirements differ; there is no one size fits all. Commonly found comments about Git GIT is overly complex in every sense of the word, in particular the information model and the command line syntax as well as the help documentation, none of it is easy to understand. The statement \"if you don't understand the functionality don't use it, just use it like subversion\" just doesn't fit for the simple reason that most commands lead to further commands, where simple actions can require complex actions to undo or refine. The decision as to whether to use Git on a development project or not, is primarily a question of how knowledgeable of Git the individuals are within the team, a single weak link can bring down the entire team. The obvious example of this is seen in the version control of Git. It is simply put, – unsafe, – prone to accidental alteration by users (potentially affecting the entire team), therefore a high minimum knowledge requirement is necessary. Consider a Git test, what should the pass rate be? 70%, 80% or 90%? Continuing on the overly complex nature of Git's design, the obvious point is that it puts control firmly in the project managers hands or whoever is maintaining the codebase, you might wonder why that is bad? The problem is that the majority of Git users are coders (contributors) and what they need most of all is a clean interface which is where Git fails to deliver, putting functionality over usability, favoring the maintainer over the contributor. Git is different but not necessarily better than Mercurial It is often said that working with Git is much better then Mercurial. The two clearly have pros and cons but one is not necessarily better than the other is. It is all depends on your knowledge and expectations. The method by which both Git and Mercurial handle history, is essentially just a directed acyclic graph. However, Mercurial provides you with a simple linear history that can cause confusion due to the lack of information, whereas Git enables you to follow the history backwards but this is complicated and hard to follow, therefore this can cause confusion. It is often an argument that Git handles branches better than Mercurial without any particular reason. As a Mercurial user, you might see two branches (named branches and repository clones) and all your changes will belong to a named branch. The confusion starts if you have many with the same name. Branches structure in Git enables you to avoid putting code where you did not want. Git enforces technical excellence; – if you do not know everything about Git then you should not be using Git else you are a danger to others work. Simply put, a Git team is an expert team. Git is more powerful for larger projects, one good example is the functionality provided by \"partial checkouts\". Last, but not least, the biggest non-technical different between Mercurial and Git is the marketing. Git seems to be better marketed these days and that clearly makes many people use Git."},{"title":"7 Things I Didn't Expect to Hear at Gartner's IT Ops Summit","tags":"devops","url":"http://ciandcd.github.io/7-things-i-didnt-expect-to-hear-at-gartners-it-ops-summit.html","text":"From: http://java.dzone.com/articles/7-things-i-didn't-expect-hear Last week's Gartner IT Operations Strategies & Solutions Summit in Orlando, Fla., was exactly what you'd expect—a place to talk about the IT operations issues impacting some of the largest companies in the world. Even so, there were a few interesting surprises. Among them: 1. Bi-modal is big. Not everyone will succeed. Gartner continued to tell its customers to employ two modes of IT —a traditional, slower moving capability for older, typically internal systems of record; and a high-speed, experimental one for new, typically customer-facing Web and mobile apps. \"This is a time of experimentation and innovation,\" said Gartner VP and distinguished analyst Chris Howard in his opening keynote. Organizations can't ignore that there are multiple speeds and they should participate in all. Gartner managing VP Ronni Colville added that by 2017, 75% of IT orgs will have this \"bi-modal\" IT capability. See also: Bi-Modal IT: Gartner Endorses Both Disruptive and Conservative Approaches to Technology However, \"50% will make a mess of it,\" Colville said. Why? Not necessarily because of technology failings, but more often because of a lack of people skills. 2. IT success is all about people. Donna Scott , also a Gartner VP and distinguished analyst, told her keynote audience that \"you will be judged on agility, speed, and innovation.\" However, the biggest problems Gartner sees for infrastructure and operations team engagement and innovation are lack of time, company culture that's not conducive to these approaches, and a lack of business skills in IT. More than half of the people responding to an in-room poll said \"people\" are the part of IT ops that must change first. Not technology. Gartner research director George Spafford underscored similar issues in large organizations trying to use DevOps at scale: people and \"human factors\" are the biggest concerns from his in-room poll. All these probably contributed to hiring best-selling author Daniel Pink as a keynote speaker on the opening day of the conference. His focus? Not IT or architecture. Instead, he pounded home the importance of influencing people and selling internally. 3. Big orgs are trying DevOps. But the issues are different at scale. In numerous sessions I saw many hands go up when analysts asked, \"Who here is trying DevOps?\" Clearly, the approach is getting traction in large companies. But there's lots of learning still to do. In fact, that was Spafford's biggest bit of advice. \"Always be learning,\" he said, \"trying to see what works and what breaks, especially at scale.\" And, even once you've had some initial success, keep learning. \"If you've done ‪DevOps, stay humble,\" he advised. 4. Looking to innovative organizations for ideas … analytics on the rise. Many sessions addressed how large organizations are taking on ideas fostered by smaller, more risk-tolerant companies, and offered advice for doing so successfully. In addition to multiple discussions of DevOps, an entire session was devoted to establishing your own \"Genius Bar®—a \"walk-up IT support center\" as explained in this CIO article . As at previous conferences, Gartner research VP Cameron Haight ran several sessions on lessons learned from firms running massive, Web-scale IT systems. \"You need lots of data … and access to it inexpensively,\" he said. Some commercial monitoring companies (New Relic included!) got a shout out for taking the lessons of Web scale IT to heart in their offerings. In addition, Haight said, \"Analytics are increasingly important for application performance monitoring given the huge amount of data now available.\" 5. Cloud: Enterprises want it, but aren't very good at it yet. Gartner research director Dennis Smith talked through the enterprise's interest in cloud computing. A huge majority of his in-room poll wanted some mix of both public and private cloud, while only 9% wanted to use only a private cloud environment and a measly 4% were looking to move entirely to the public cloud. The most popular choice (41%) was an 80/20 split between private and public cloud infrastructure. \"Enterprises don't make the dean's list,\" for cloud usage, Smith said, earning no more than a C average in his opinion. Large organizations are doing well at visibility, governance, and delivering standardized stacks, he said, but are less skilled at optimizing for these new environments. Still, Smith said the trends point toward enterprises improving on all fronts. 6. Cloud security can be better than yours. Importantly, Gartner VP and distinguished analyst Neil MacDonald gave the cloud a vote of confidence: noting that, for a variety of reasons, \"Well-managed public cloud can be more secure than your own data center.\" For example, on-premise software can pose serious security risks, he said, because of \"deployment lag\" where customers are stuck using software releases with unpatched security vulnerabilities. With a cloud-based Software-as-a-Service (SaaS), security updates can be more quickly rolled out to all customers. But cloud security can be different, requiring a shift to information-level security from OS-level security. Best practices include doing away with a huge pool of all-powerful sysadmins in favor of JEA, or \"just enough administration,\" where sysadmins have just enough privileges to do their job, and no more. An analogous security practice for compute resources is \"least privilege,\" where apps and microservices can't talk to each other unless they specifically need to do so. Audience polling supported MacDonald's optimistic view of cloud security, which suggests that large enterprises may struggle less with their cloud policies moving forward. 7. Containers: Try 'em! Ahead of this week's DockerCon in San Francisco, Gartner devoted significant airtime to educating the audience on containers and microservices. My summary of ‪Gartner VP and distinguished analyst Tom Bittman's advice on containers was simple: Try 'em. Now. Complement them with VMs. ‪And Docker (the company) is important, but not the be-all and end-all in this space. Bittman (copping to some deja vu from Gartner presentations he made on server virtualization 13 years ago) noted that while virtualization has been focused on admin and ops functions, containers are focused on value for developers. But because containers are well suited for driving up VM utilization for workloads that share the same OS, we can expect to see more combinations of containers and server virtualization. Finally, Bittman underscored that Gartner doesn't see containers having much impact on premise, but making a huge difference in the cloud. That doesn't necessarily fit with what's been shown in other research, such as this 2015 State of Containers Survey sponsored by VMblog.com and StackEngine, so we'll want to watch how this plays out. This is all a lot to digest. The Gartner IT Operations Strategies & Solutions Summit acknowledges the importance of dealing with existing IT systems and practices as well as promising new technologies and thinking, and tries to point a way forward. In fact, Haight had a very good quote about microservices that I thought also served to wrap up the entire event: \"If you want to run with the big dogs, you need to rethink application architecture,\" he said. That can be very difficult for an enterprise to fully implement … but also very appealing. Note: Al Sargent contributed to this post. All product and company names herein may be trademarks of their registered owners. Server , tortoise and hare , business team , and cloud security images courtesy of Shutterstock.com ."},{"title":"Blue-Green Deployment With a Single Database","tags":"devops","url":"http://ciandcd.github.io/blue-green-deployment-with-a-single-database.html","text":"From: http://java.dzone.com/articles/blue-green-deployment-single A blue-green deployment is a way to have incremental updates to your production stack without downtime and without any complexity for properly handling rolling updates (including the rollback functionality) I don't need to repeat this wonderful explanation or Martin Fowler's original piece . But I'll extend on them. A blue-green deployment is one where there is an \"active\" and a \"spare\" set of servers. The active running the current version, and the spare being ready to run any newly deployed version. The \"active\" and \"spare\" is slightly different than \"blue\" and \"green\", because one set is always \"blue\" and one is always \"green\", while the \"active\" and \"spare\" labels change. On AWS, for example, you can script the deployment by having two child stacks of your main stacks – active and spare (indicated by a stack label), each having one (or more) auto-scaling group for your application layer, and a script that does the following (applicable to non-AWS as well): push build to an accessible location (e.g. s3) set the spare auto-scaling group size to the desired value (the spare stays at 0 when not used) make it fetch the pushed build on startup wait for it to start run sanity tests switch DNS to point to an ELB in front of the spare ASG switch the labels to make the spare one active and vice versa set the previously active ASG size to 0 The application layer is stateless , so it's easy to do hot-replaces like that. But (as Fowler indicated) the database is the most tricky component. If you have 2 databases, where the spare one is a slave replica of the active one (and that changes every time you switch), the setup becomes more complicated. And you'll still have to do schema changes. So using a single database, if possible, is the easier approach, regardless of whether you have a \"regular\" database or a schemaless one. In fact, it boils down to having your application modify the database on startup, in a way that works with both versions. This includes schema changes – table (or the relevant term in the schemaless db) creation, field addition/removal and inserting new data (e.g. enumerations). And it can go wrong in many ways, depending on the data and datatypes. Some nulls, some datatype change that makes a few values unparseable, etc. Of course, it's harder to do it with a regular SQL database. As suggested in the post I linked earlier, you can use stored procedures (which I don't like), or you can use a database migration tool . For a schemaless database you must do stuff manually, but but fewer actions are normally needed – you don't have to alter tables or explicitly create new ones, as everything is handled automatically. And the most important thing is to not break the running version. But how to make sure everything works? test on staging – preferably with a replica of the production database (automatically) run your behaviour/acceptance/sanity test suites against the not-yet-active new deployment before switching the DNS to point to it. Stop the process if they fail. Only after these checks pass, switch the DNS and point your domain to the previously spare group, thus promoting it to \"active\". Switching can be done manually, or automatically with the deployment script. The \"switch\" can be other than a DNS one (as you need a low TTL for that). It can be a load-balancer or a subnet configuration, for example – the best option depends on your setup. And while it is good to automate everything, having a few manual steps isn't necessarily a bad thing. Overall, I'd recommend the blue-green deployment approach in order to achieve zero downtime upgrades. But always make sure your database is properly upgraded, so that it works with both the old and the new version."},{"title":"Cameron of Gartner talks DevOps – DevOps Days Austin","tags":"devops","url":"http://ciandcd.github.io/cameron-of-gartner-talks-devops-devops-days-austin.html","text":"From: http://java.dzone.com/articles/cameron-gartner-talks-devops Last month at DevOps Days Austin I did a series of interviews with a variety of speakers and attendees. One of the attendees I chatted with was Cameron Haight of Gartner. For the past five years Cameron has been writing about, and advising clients on, DevOps. I caught some time with Cameron to get his thoughts. Some of the ground Cameron covers : How Cameron came to cover the DevOps movement. What changes has he seen in the community over the past five years. How does Cameron see DevOps evolving as it moves into the mainstream and where it fits within the larger transformation enterprises are undergoing. Stay tuned for the final interview in this series starring the one and only John Willis . Extra-credit reading Pau for now…"},{"title":"Cardiff: Silicon Valley comes to Wales","tags":"devops","url":"http://ciandcd.github.io/cardiff-silicon-valley-comes-to-wales.html","text":"From: http://blog.devopsguys.com/2015/05/05/cardiff-silicon-valley-comes-to-wales/ We've been set up in Cardiff, South Wales for nearly six months now. Every week it becomes more and more apparent that this city is fast becoming an exciting IT and technical hub; an attractive area for emerging and experienced tech talent alike. The term ‘Silicwm Valley' is being bandied about as more and more tech start-ups spring up in, or near, the city centre. Companies like DevOpsGuys, Cardiff Start, Indycube, Method 4, BBC Cymru's Roath Lock studios and a huge collection of digital and design agencies are choosing Cardiff as their base. It seems to be a logical step; the community is small enough to be interconnected, influential and supportive, but large enough to allow for the freedom to develop, expand and learn from the huge range of related industries in the immediate area. With several major universities in and around the city the wealth of talent is growing and Cardiff is taking the reins and nurturing Welsh talent and ability; a very different picture from several years ago where work in Wales was hard to come by and the majority of experienced IT professionals were obliged to seek work further afield, in London or Cambridge. Founder James Smith says: \"Cardiff has historically been built on industry, from the days of exporting coal. It's also frequently voted one of the top places to live and work in the UK, so it's no wonder that this tradition is developing and changing shape with the emergence of the tech industry – Cardiff is moving with the times. \"We've set up DevOpsGuys in Cardiff in order to be a part of this development. We wanted to provide opportunities for people in Wales – there's so much skill here. Plus we are working with international companies and forming partnerships with industry giants across the world; this is a great opportunity to share some of the home-grown Welsh talent, create unique, fulfilling career opportunities and forge connections all over the world. It's a really exciting time.\" The movement has been supported by the Welsh Government, with DevOpsGuys receiving funding to grow as a business and provide career opportunities in the Welsh capital. Meet-ups, tech events, talks and conferences all taking place in the city, give related, but wildly diverse businesses a chance to meet, mix, talk, share thoughts; ideas flow freely, business connections are forged easily and some new and interesting work is emerging. We're excited about Agile Cymru – the first event of its kind in Cardiff – this summer. There seems to be something new to see, do, read, visit, look at or enjoy every week! We're excited to see where Cardiff will take the DevOpsGuys and the future of the UK tech industry."},{"title":"Create a Maven archetype","tags":"devops","url":"http://ciandcd.github.io/create-a-maven-archetype.html","text":"From: http://java.dzone.com/articles/create-maven-archetype After creating our project structures from the basic maven archetypes since starting to use Maven I thought it was time to start automating the process. It wasn't too painful but not quite as straight forward as I hoped either so I thought it was worth writing an article about my experience. Example Here's an example of the code which I've got working, so feel free to start from there. Click on the link on my website post to download (it should anchor so that the zip file link is at the top of the page) Link to zip I found it easiest to create a Java Project and copy the files into there and then convert the project to Maven project but whatever takes your fancy. Using property names The hard part of creating the archetype was getting the property names to show throughout the code. I had followed instructions on other websites that had mentioned the __my-property-name__ convention to get this to work. However after following the instructions on the Maven website I found that updating the files from the resulting project didn't work. After some more searching I found out about the archetype-metadata.xml file which appears to be quite important. Once this was added into the system I found that my property names started resolving to their expected values. It also gave me an opportunity to add additional parameters like the projectName used in the example. I found that if you want to place property names in a filename or directory name then you needed to use the __my-property-name__ convention but inside the file you need to use the ${my-property-name} convention. The archetype-metadata.xml file also meant that I could do away with the archetype.xml file that was suggested in the Maven tutorial. With this file you had to say exactly which files you wanted to include into your project, however by specifying **/* for the include tag of the archetype-metadata.xml file it included all of the files inside that folder without having to update the config each time the files changed. Velocity and If Statements The maven archetype build uses the velocity templating engine which means that you have a lot of control over what you can do. I'm not going to go through all the details of what Velocity does as there's a perfectly good website for that here . What I will have a quick chat about is the if statement functionality. This is really powerful as I had initially started creating seperate archetypes depending on the type of project I was creating. I then quickly realised that a lot of these shared the same structure and files so managed to merge most of them together into a single archetype by using a mix of a new project type required parameter (as mentioned about above) and the velocity if statement. An example of the statement is shown below, the keyword #else can also be used. #if( ${projectType} == \"cms\" || ${projectType} == \"ecommerce\" ) private int myNewVariable; #end Adding empty folders This one had me stuck for a little while as I was trying to add empty folders into the archetype-resources section but they were not appearing in the generated project. The answer is to include them as a fileset in the archetype-metadata.xml as I have done in the example archetype with the \"/src/main/webapp\" folder. <fileSet filtered=\"true\" encoding=\"UTF-8\"> <directory>src/main/webapp</directory> </fileSet> Create a project from the archetype I use Eclipse as my development environment, if you are using NetBeans or others I imagine this part will be slightly different. In order to create the archetype from the project it was simply a case of right clicking the POM file and choosing to run Maven install. Creating a new project from the archetype however was slightly more tricky as when you go to choose the archetype from the list it's not there. I had to add an archetype using the 'Add Archetype' button on the page where you would normally select the archetype from the list. For the example I just put in com.aplos for the Archetype Group Id, example-archetype for the Archetype Artifact Id and 1.0 for the Version, I left the repository url field blank and Eclipse found the archetype. Conclusion Following these steps should allow you to create a wide range of project structures and base files. I still cannot see how to populate the group id and package name for the project to the default of com.aplos. However I'm happy for now and if I learn the other parts I'll be sure to update this article, or if you know I'd be grateful to hear from you."},{"title":"Customer Feedback drives Continuous Delivery - Planning for DevOps","tags":"devops","url":"http://ciandcd.github.io/customer-feedback-drives-continuous-delivery-planning-for-devops.html","text":"From: http://devops.linuxjournal.com/urban-code/customer-feedback-drives-continuous-delivery-planning-devops Businesses need an approach that goes beyond Application Lifecycle Management (ALM) in order to achieve sustained innovation. The basis for that approach is DevOps, a contraction of \"development and operations,\" the two teams that form the core of an organization's technology department"},{"title":"Deployment Automation Basics","tags":"devops","url":"http://ciandcd.github.io/deployment-automation-basics.html","text":"From: http://devops.linuxjournal.com/urban-code/deployment-automation-basics Why should we bother automating deployments in the first place? What should the scope of the automation effort be? How do we get started? This white paper provides a solid introduction to these topics."},{"title":"DevOps for Enterprise Systems - a quick intro","tags":"devops","url":"http://ciandcd.github.io/devops-for-enterprise-systems-a-quick-intro.html","text":"From: http://devops.linuxjournal.com/collaborative-development/devops-enterprise-systems-quick-intro Established IT organizations are challenged by long software delivery cycles and the complexity surrounding connecting Systems of Record to new Systems of Engagement. Tools, processes and culture typically contribute to prolonged delivery cycles, so IT organizations are moving to DevOps. How do you, as a practitioner, address the siloes involved in modern enterprise development and leverage DevOps to make your job easier?"},{"title":"DevOps Leadership Series: Compliance, Testing, and Rugged","tags":"devops","url":"http://ciandcd.github.io/devops-leadership-series-compliance-testing-and-rugged.html","text":"From: http://java.dzone.com/articles/devops-leadership-series-1 This past week, I headed to London for the Rugged DevOps event, where I had the chance to catch up with a few more industry thought leaders. First, I caught up with Gareth Rushgrove from Puppet Labs, who also runs the DevOps Weekly newsletter. In this episode of the DevOps Leadership Series, Gareth explains why the importance of security tests within DevOps practices is going to be a big topic for years to come. Gareth anticipates that \"In five years time we will look back on this and ask ‘why were we not always doing this'?\" I then caught up with Helen Beal , Head of DevOps at Ranger4, where she discussed the importance of DevOps and security. While the two ultimately need to work hand and hand, she voiced concerns about DevOps practices sometimes circumventing controls that are essential to a business's safety. She also said that DevOps supports security in a number of ways: from making things consistent to relying on more automation. Finally, I caught up with Justin Arbuckle , Vice President, EMEA & Chief Enterprise Architect at Chef. He discusses key trends he expects to see in over the next 9-12 months regarding compliance and security. Justin explains that compliance as a core driver of DevOps is something that will start to shape our conversations over the next year, whereas the two have previously been seen as contradictory. He believes that we will see that high velocity organizations are able to improve compliance continuously, and tells us to look out for the changing role of the security officer in 2015. Next up in the series, I head to the United States Capitol, Washington DC, for DevOps Days DC . NOTE : If you have missed any of the other videos from this series, you can find them here . (We're up to 15 so far)."},{"title":"Does DevOps Reduce Technical Debt--or Make it Worse?","tags":"devops","url":"http://ciandcd.github.io/does-devops-reduce-technical-debt-or-make-it-worse.html","text":"From: http://java.dzone.com/articles/does-devops-reduce-technical-0 DevOps can help reduce technical debt in some fundamental ways. Continuous Delivery/Deployment First, building a Continuous Delivery/Deployment pipeline , automating the work of migration and deployment, will force you to clean up inconsistencies and holes in configuration and code deployment, and inconsistencies between development, test and production environments. And automated Continuous Delivery and Infrastructure as Code gets rid of dangerous one-of-a-kind snowflakes and configuration drift caused by making configuration changes and applying patches manually over time. Which makes systems easier to setup and manage, and reduces the risk of an un-patched system becoming the target of a security attack or the cause of an operational problem . A CD pipeline also makes it easier, cheaper and faster to pay down other kinds of technical debt. With Continuous Delivery/Deployment, you can test and push out patches and refactoring changes and platform upgrades faster and with more confidence. Positive Feedback The Lean feedback cycle and Just-in-Time prioritization in DevOps ensures that you're working on whatever is most important to the business. This means that bugs and usability issues and security vulnerabilities don't have to wait until after the next feature release to get fixed. Instead, problems that impact operations or the users will get fixed immediately. Teams that do Blameless Post-Mortems and Root Cause(s) Analysis when problems come up will go even further, and fix problems at the source and improve in fundamental and important ways. But there's a negative side to DevOps that can add to technical debt costs. Erosive Change Michael Feathers' research has shown that constant, iterative change is erosive : the same code gets changed over and over, the same classes and methods become bloated (because it is naturally easier to add code to an existing method or a method to an existing class), structure breaks down and the design is eventually lost. DevOps can make this even worse. DevOps and Continuous Delivery/Deployment involves pushing out lots of small changes, running experiments and iteratively tuning features and the user experience based on continuous feedback from production use. Many DevOps teams work directly on the code mainline, \" branching in code \" to \" dark launch \" code changes, while code is still being developed, using conditional logic and flags to skip over sections of code at run-time. This can make the code hard to understand, and potentially dangerous: if a feature toggle is turned on before the code is ready, bad things can happen . Feature flags are also used to run A/B experiments and control risk on release, by rolling out a change incrementally to a few users to start. But the longer that feature flags are left in the code, the harder it is to understand and change . There is a lot of housekeeping that needs to be done in DevOps: upgrading the CD pipeline and making sure that all of the tests are working; maintaining Puppet or Chef (or whatever configuration management tool you are using) recipes; disciplined, day-to-day refactoring ; keeping track of features and options and cleaning them up when they are no longer needed, getting rid of dead code and trying to keep the code as simple as possible. Microservices and Technology Choices Microservices are a popular architectural approach for DevOps teams. This is because loosely-coupled Microservices are easier for individual teams to independently deploy, change, refactor or even replace . And a Microservices-based approach provides developers with more freedom when deciding on language or technology stack: teams don't necessarily have to work the same way, they can choose the right tool for the job, as long as they support an API contract for the rest of the system. In the short term there are obvious advantages to giving teams more freedom in making technology choices. They can deliver code faster, quickly try out prototypes, and teams get a chance to experiment and learn about different technologies and languages. But Microservices \" are not a free lunch \". As you add more services, system testing costs and complexity increase. Debugging and problem solving gets harder. And as more teams choose different languages and frameworks, it's harder to track vulnerabilities, harder to operate, and harder for people to switch between teams. Code gets duplicated because teams want to minimize coupling and it is difficult or impossible to share libraries in a polyglot environment. Data is often duplicated between services for the same reason, and data inconsistencies creep in over time. Negative Feedback There is a potentially negative side to the Lean delivery feedback cycle too. Constantly responding to production feedback, always working on what's most immediately important to the organization, doesn't leave much space or time to consider bigger, longer-term technical issues, and to work on paying off deeper architectural and technical design debt that result from poor early decisions or incorrect assumptions. Smaller, more immediate problems get fixed fast in DevOps. Bugs that matter to operations and the users can get fixed right away instead of waiting until all the features are done, and patches and upgrades to the run-time can be pushed out more often. Which means that you can pay off a lot of debt before costs start to compound. But behind-the-scenes, strategic debt will continue to add up. Nothing's broke, so you don't have to fix anything right away. And you can't refactor your way out of it either, at least not easily. So you end up living with a poor design or an aging technology platform, slowly slowing down your ability to respond to changes, to come up with new solutions. Or forcing you to continue filling in security holes as they come up, or scrambling to scale as load increases. DevOps can reduce technical debt. But only if you work in a highly disciplined way. And only if you raise your head up from tactical optimization to deal with bigger, more strategic issues before they become real problems."},{"title":"How to Debug Your Maven Build with Eclipse","tags":"devops","url":"http://ciandcd.github.io/how-to-debug-your-maven-build-with-eclipse.html","text":"From: http://java.dzone.com/articles/how-debug-your-maven-build When running a Maven build with many plugins (e.g. the jOOQ or Flyway plugins ), you may want to have a closer look under the hood to see what's going on internally in those plugins, or in your extensions of those plugins. This may not appear obvious when you're running Maven from the command line, e.g. via: C:\\Users\\jOOQ\\workspace>mvn clean install Luckily, it is rather easy to debug Maven. In order to do so, just create the following batch file on Windows: @ECHO OFF IF \"%1\" == \"off\" ( SET MAVEN_OPTS= ) ELSE ( SET MAVEN_OPTS=-Xdebug -Xnoagent -Djava.compile=NONE -Xrunjdwp:transport=dt_socket,server=y,suspend=y,address=5005 ) Of course, you can do the same also on a MacOS X or Linux box, by using export intead of SET . Now, run the above batch file and proceed again with building: C:\\Users\\jOOQ\\workspace>mvn_debug C:\\Users\\jOOQ\\workspace>mvn clean install Listening for transport dt_socket at address: 5005 Your Maven build will now wait for a debugger client to connect to your JVM on port 5005 (change to any other suitable port). We'll do that now with Eclipse. Just add a new Remote Java Application that connects on a socket, and hit \"Debug\": That's it. We can now set breakpoints and debug through our Maven process like through any other similar kind of server process. Of course, things work exactly the same way with IntelliJ or NetBeans. Once you're done debugging your Maven process, simply call the batch again with parameter off : C:\\Users\\jOOQ\\workspace>mvn_debug off C:\\Users\\jOOQ\\workspace>mvn clean install And your Maven builds will no longer be debugged. Happy debugging!"},{"title":"How to get your servers out of coma – First steps in capacity planning and management","tags":"devops","url":"http://ciandcd.github.io/how-to-get-your-servers-out-of-coma-first-steps-in-capacity-planning-and-management.html","text":"From: http://java.dzone.com/articles/how-get-your-servers-out-coma 30% of servers are sitting \"comatose\" . Out of personal experience, I'm pretty sure that one of the major reasons for that is how tedious it is to have your server's load balanced accordingly. Why? Allow me to explain. The bad part about capacity management and planning in the past was that it was tedious work collecting all the required host-related metrics. But those days are behind us. Have a look at this sortable table view of monitored-host metrics provided by Ruxit : Sorting hosts by metric type can greatly simplify capacity planning Sorting hosts by metric type can greatly simplify capacity planning Having the ability to sort your hosts (both virtual and physical) by metric type helps you to easily see if your infrastructure can handle more load. Having such detailed metrics at your fingertips is a simple first step toward knowing if your hosts are chronically overloaded or under-utilized. If you discover that your hosts are running at less than full capacity you may have the opportunity to save some money by, for example, automatic scaling of VM instances. On the other hand, if your hosts suffer from overload, their performance will be negatively affected. This leads to increased response times and, ultimately, lost revenue. For more on this point, see the Kissmetrics article How Loading Time Affects Your Bottom Line . Know the differences between virtual and physical hosts Virtual hosts can max out their allocated resources while the the physical hosts they reside on sit largely idle with plenty of available capacity. Remember there are physical constraints hehind every virtual host. Remember there are physical constraints hehind every virtual host. There are other reasons besides maxed out physical-host resources that can cause a VM to sit idle. For example, a physical host may not be able to assign more resources to a VM because one or more other VMs have already claimed all the resources. In such cases, it's important to keep a watchful eye on both your physical and your virtual hosts and have integrated metrics and graphs available for comparison. It's no fun correlating these numbers manually though. Again, Ruxit can save you hours of manual number crunching here. Now, with all the tedious work behind you you're ready for … The amazing part The amazing part begins once you've mastered the challenge of identifying your potentially tunable hosts. For me, benchmarking and load testing applications and watching them utilize hardware resources is the really fun part. And with virtualization technology at hand, it's easy to find out if your applications scale better horizontally or vertically. When it comes to load testing, there are loads of tools available. Google can deliver a list of open source load-testing tools if you're interested: Try us out Give Ruxit a try! Watch your environment under normal load, place some load on your infrastructure, and then see how your resource utilization changes. I'd love to hear your findings. Does your environment scale better horizontally or vertically? I'm looking forward to hearing your stories!"},{"title":"How to Keep REST API Credentials Secure","tags":"devops","url":"http://ciandcd.github.io/how-to-keep-rest-api-credentials-secure.html","text":"From: http://java.dzone.com/articles/how-keep-rest-api-credentials If you are building mobile apps then you are connecting to some REST API. For example, if you want to resolve an address to a latitude/longitude information to display on a map, you might use the If you are building mobile apps then you are connecting to some REST API. For example, if you want to resolve an address to a latitude/longitude information to display on a map, you might use the Google Geocoding API https://maps.googleapis.com/maps/api/geocode/json?address=San Francisco,CA&key=AIzaSyDvFMYGjeR02RH Google Geocoding API: If you are invoking the API from the client, then the API key also has to be present on the client. But, this is also the problem. It's very easy to look at the app source in the browser and get access to the API key. If someone has access to your API key, they can send requests on your behalf (without you knowing), and use up your request quota. Even if you are building a hybrid app, it's still the same problem. A hybrid app is HTML/JavaScript inside a native wrapper, it's possible to download the app, un-package it and gain access to API keys or any sensitive information stored in the app. Even native apps are not immune to this. For example, an Android app is just a Java application and a Java application can be de-compiled to view the original source. The next image shows how to get access to an API key in the browser: A good solution is to never expose the API key (or any other sensitive data) on the client. How do you do that? You keep the API key and any other sensitive information on the server. Appery.io Secure Proxy (part of Backend Services) enables app developers to keep sensitive app data on the server. Your API keys or any other data is never exposed on the client. Watch this 5-minute video on how to use Secure Proxy: Before using the Secure Proxy, you need to store the data on the server. To store the data you are going to use the Appery.io Database. It's as simple as creating a collection with two columns. The first column is the value name, the second column is the actual value. This is how the database looks when storing the API key for Google Geocoding API: As this key is stored on the server, no one (but you) has access to it. You can store other data as well such as URLs, tokens or anything else that shouldn't be exposed on the client. The next step is to setup the proxy that will use the information stored in the database. This step is also very simple, this is how it looks: You give the proxy a name and then link it to a database which stores your data. The above proxy is linked to Secrets_db database, Credentials collection, and secretName , secretValue columns. The last step is to link a REST API service to the proxy. In the service editor you select the secure proxy created: then in the Request tab you reference the API key stored in the database (the name stored in secretName column): and that's it. When the API service is invoked, the call will go through the secure proxy (server) where the API key will substituted: For web apps, you can add an extra layer of security by specifying from which page URLs the proxy should accept requests: The proxy will only accept requests from page URLs listed in the table. Another option to keep API keys private is to invoke the API from the server using Server Code , I will cover this in another post. Setting up an using the Appery.io Secure Proxy is simple. It provides a very important feature by allowing to keep sensitive and private data on the server, never exposing it on the client, and adding an extra security layer to your app."},{"title":"IBM Cloudant: The Do-More NoSQL Data Layer","tags":"devops","url":"http://ciandcd.github.io/ibm-cloudant-the-do-more-nosql-data-layer.html","text":"From: http://devops.linuxjournal.com/cloud/ibm-cloudant-do-more-nosql-data-layer Cloudant represents a strategic acquisition by IBM® that extends the company's Big Data and Analytics portfolio to include a fully managed, NoSQL cloud service. Cloudant simplifies the development cycle for creators of fast-growing web and mobile applications, by alleviating the burdens of mundane database administration tasks. Developers are then able to focus on building the next generation of systems of engagement – social and mobile applications – without losing time, money, or sleep managing their database infrastructure and growth. Critically, Cloudant is an enterprise-ready service that supports this infrastructure with guaranteed performance and availability. Built atop a CouchDB-based NoSQL data layer, Cloudant's fully managed database-as-a-service (DBaaS) enables applications and their developers to be more agile. As a part of its data layer, clients have access to multi-master replication and mobile device synchronization capabilities for occasionally connected devices. Applications can take advantage of Cloudant's advanced real-time indexing for ad hoc full text search via Apache Lucene, online analytics via MapReduce, and advanced geospatial querying. Mobile applications can use a durable replication protocol for offline sync and global data distribution, as well as a geo-load balancing capability to ensure cross-data center availability and optimal performance. Cloudant's RESTful web-based API, flexible schema, and capacity to scale massively are what empower clients to deliver applications to market faster in a cost-effective, DBA-free service model. This IBM Redbooks® Solution Guide describes the IBM Cloudant features."},{"title":"IBM mobile testing point of view","tags":"devops","url":"http://ciandcd.github.io/ibm-mobile-testing-point-of-view.html","text":"From: http://devops.linuxjournal.com/develop-deploy/ibm-mobile-testing-point-view-0 Determining the quality of mobile applications requires you to look beyond the application itself and take into account all the systems and services the application makes use of. This paper describes the IBM point-of-view on critical mobile application testing scenarios and execution engines that are necessary and appropriate for a comprehensive mobile application testing solution."},{"title":"Improve quality and increase your business agility through automated testing. Test automation solutions for Financial Services Organizations","tags":"devops","url":"http://ciandcd.github.io/improve-quality-and-increase-your-business-agility-through-automated-testing-test-automation-solutions-for-financial-services-organizations.html","text":"From: http://devops.linuxjournal.com/collaborative-development/improve-quality-and-increase-your-business-agility-through-automated IBM Rational test automation capabilities and tools can help software integrators, developers and testers alike overcome the challenges of testing banking, insurance and financial market applications."},{"title":"Investing in Your Infrastructure","tags":"devops","url":"http://ciandcd.github.io/investing-in-your-infrastructure.html","text":"From: http://java.dzone.com/articles/investing-your-infrastructure Having mentored and invested in startups I have come to learn what works and doesn't websites. The reality of getting out of startup mode and scaling takes vision and the ability to anticipate how you may need to pivot. There are two main reasons why startups do not scale. The first is a lack of experience and mentorship. This is closely followed by a lack of a working capital. An effective entrepreneur understands not only how to pivot but how to utilize capital investment. This article aims to demystify the need for investing in site infrastructure. When scaling a business there are several huge issues that executives tend to forget. whether you are a startup or a fortune 500 company your website is the organizations public face. Maintaining a clean and secure site will help to avoid deep routed problems that could potentially destroy not only you site by your reputation. Hackers can be devastating and end up costing you millions. Another aspect of your site to consider is funnel optimization. By making this a priority you will be able to effective guide customers step by step into a conversion. American With Disabilities Act (ADA) The ADA can be a freighting legal area for many entrepreneurs. Many think that ADA only applies to physical boundaries such as implementing ramps for wheelchairs and accessible bathroom stalls. However this is far from the case. Few people know that the digital world also counts. Is your website written in HTML5? If not you are in violation of ADA. This newest version of the HTML coding language allows for an audible version of a web site for those facing impaired vision. However there are some quick an easy ways to gain some ADA points to allow for more leeway in other more long-term solutions. Implementing closed captioning into your promotional videos will allow for increased accessibility for those facing an auditory disability. Investing in infrastructure is not a single step but a constant to building a good company culture where employees can feel proud of where they work and remain safe. Please comment on this article if you have any additional suggestions."},{"title":"LastPass Breach, Password Security, and Reason","tags":"devops","url":"http://ciandcd.github.io/lastpass-breach-password-security-and-reason.html","text":"From: http://java.dzone.com/articles/lastpass-breach-password LastPass , the password manager that lets you manage your passwords between different devices, was recently hacked . From this there has been a fair amount of FUD circulating and not enough rational thought. With that in mind, this seems like a good time to talk about password security and LastPass with some rational ideas. Since we can't get rid of passwords just yet we need to manage them well. 1. LastPass Detected The Breach No useful system is impenetrable. Computers not connected to the Internet, that don't even have a network card, have been hacked across an air gap using their speakers and mic. The most up to date systems still suffer from zero-day exploits . Two of the elements of an organization that takes security seriously are keeping certain pieces of data separate and detecting when a breach occurs. From the LastPass announcement of the breach we can see these two things in action. While some information was obtained the actual vaults of passwords were not downloaded. And, they detected there was a problem and enough monitoring in place to distinguish what was effected. I can't overstate how nice that is to hear. Many organizations won't detect if they have been breached. Even many of those that could detect a breach wouldn't be able to tell you what was affected. That's right, many of the places you put personal information couldn't do what LastPass did. 2. LastPass Responded To The Breach Even though the password vaults were not taken LastPass is having everyone change their vault password. They detected the problem and are going the extra mile to protect their users. Now, let's consider an alternative option. Consider a 1Password or KeePass user who stores their information in Dropbox or a similar service. A malicious program on one of their systems could have taken their vault and sent it to an attacker. Those users would not have known. Or, the service could have been hacked but since it's not password specific who would have suggested changing the master password? I'm not trying to defend LastPass. It's a matter of considering the alternatives and the security measures around them. Is a 1Password or KeePass alternative setup actually more secure in practice? 3. Different Passwords For Different Sites There's a good reason to have a different password or passphrase for different sites. You can't trust that a site you submit it to will store it securely so that it won't be misused to access other sites. With all the sites we connect to we it's difficult to remember a different password for each site. Congratulations if you can do that. For the rest of us we need a system to help. This is where a password manager is useful. That is, until we can stop using passwords for something better. So, use a password manager if you can't otherwise have a different password for each site. It's more secure than using the same password everywhere. Note, I'm not recommending a particular password manager on purpose. Use a good one. 4. Security Is Not About Perfection There is no such thing as perfect security. Security needs to be practical. For example, for most people it's more security to use a password manager than to use the same password everywhere. Neither is perfect but when you weigh the differences the password manager comes out as more secure. In a distributed device world where we need passwords on more than one system it's good to go with a system that does this for you. A system that focuses on security and handling issues that come up. You could roll your own solution. But, will it be more secure? For most people the professional solution is the more secure one. When considering password security choose the one that's more secure for you rather than seeking the perfect option. 5. When Not To Use A Password Manager There are some places I would recommend not using a password manager. For example, I would recommend not using one for your financial sites. Those few places that are very important use a passphrase. 6. Encrypt Your Password Store In our multi-device world you'll likely need to share your password datastore between devices. And, any device can be hacked even if you don't need to use multiple devices. Imagine a virus on a computer looking for your password excel file and uploading that to someone bad. It happens. Use an encrypted datastore. This is why password managers are important. They are designed to store your data in an encrypted manner. This way, if someone gets your data store they will have a very hard time reading it. Before they can get to anything they'll need to break the encryption which isn't so easy. That means, even if an attacker had gotten the password vaults from LastPass, which they didn't, they would not have been able to read the data in them. 7. Limit Your Attack Vector One of the problems with LastPass is that they are a known password manager. That makes them a target if someone wants to try and get passwords. Alternatives that store their distributed information in general purpose systems pose a different attack vector. For example, if you use 1Password or KeePass and store your information in Dropbox you can still be hacked. Dropbox has been externally hacked in the past and other applications can access your Dropbox folder. Using alternatives to LastPass doesn't mean you won't be attacked. Take a few minutes and consider the attack vectors of the different solutions you're considering and how each of those will detect a breach and respond to that. For example, I could self host my encrypted file on the Internet somewhere. This would be managed by me and wouldn't be a known system for someone to target. But, the IPv4 addresses are regularly checked for known vulnerabilities so attackers and get onto a system a poke around. That is the entire IPv4 space, which is still the only space routable for all things, is regular checked. Will I keep everything on that system up to date? Will I detect if someone broke into the system? Will I respond appropriately? All of this needs to be taken into account. Final Thoughts On LastPass I'm not trying to defend LastPass. I'm trying to give a little more of a holistic picture of security. It's complicated and any alternatives to a LastPass or LastPass-like solutions need to have their security considered. Viewing the options with security and attacks in mind keeps everything in perspective."},{"title":"Managing change in SAP: reducing cost and risk with IBM DevOps","tags":"devops","url":"http://ciandcd.github.io/managing-change-in-sap-reducing-cost-and-risk-with-ibm-devops.html","text":"From: http://devops.linuxjournal.com/collaborative-development/managing-change-sap-reducing-cost-and-risk-ibm-devops IBM DevOps provides a flexible, collaborative approach to SAP delivery that supports continuous innovation and improvement across the SAP landscape. It provides time-to-value from change request to delivery is accelerated, helping achieve your business and IT objectives more quickly. The visibility is enhanced and change management is unified across both SAP and non-SAP landscapes. The solution Co-development by IBM and SAP ensures tight integration and adoption of joint best practices."},{"title":"Managing Outsourced Quality Assurance Teams","tags":"devops","url":"http://ciandcd.github.io/managing-outsourced-quality-assurance-teams.html","text":"From: http://java.dzone.com/articles/managing-outsourced-quality Business leaders like to be in control of every aspect of their operations, but if any element is outsourced, that sense of governance becomes much more difficult to maintain. Outsourcing can be appealing to organizations looking for talent at reasonable costs - it just takes significant planning to pull off successfully. When a team is in a different location from the company, there are a number of challenges that must be overcome. We will look at what some of the biggest issues are and how you can appropriately manage an outsourced quality assurance team. Obstacles of outsourcing While outsourcing can have a lot of advantages for businesses, the number of roadblocks can be intimidating for many enterprises. Here are the biggest challenges to prepare for: Information sharing: When you don't see individuals every day, it can often be easy to forget to relay important messages. Communication in this situation is imperative, as it could affect the overall operation of the application. As soon as a request is given, there should be seamless transfer of knowledge to ensure that everyone is on the same page and that the project proceeds as expected. This will eliminate any redundancies and lower the overall development cost. Engagement: Many outsourced teams have a difficult time becoming personally involved with their projects. This will also affect their ability to collaborate effectively with other teams in the business, ultimately hurting program quality. Organizations must have a strategy to keep these individuals motivated and provide them with the tools to succeed. Technology/skills: An outsourced team may use different pieces of technology or have skills separate from what the organization was looking for. For example, if the company really wants to move to agile software development , but the outsourced group still uses waterfall methods, that could create problems down the road for their software development initiatives. Similarly, the business must ensure that the outsourced individuals have the skills necessary to meet corporate goals and spur innovation through their app testing. How to regain control Although total governance of outsourced assets won't be possible, there are still some things that organizations can do to take control of these teams and ensure that they're fulfilling business objectives. Australian outsourcer Beepo suggested a consistent schedule for gathering feedback and using technology like the cloud and test management tools. Let's dissect each of these ideas. Many outsourced teams are often left by the wayside when it comes to communication. This can reasonably lead to mistakes being made and leave the members feeling apathetic toward their work. However, by setting up regular video conferences and requesting feedback, outsourced individuals can feel empowered to express their opinions and become a larger part of their projects. This will also help build trust and motivate teams to collaborate more. Using tools like test management and the cloud can also be helpful when working with an outsourced team due to the fact that they provide a singular platform for all users. This means that the outsourced and in-house teams can be working on the same project at the same time, with any changes being made in real time. This will not only reduce redundancies, but it creates accountability and ensures that tasks are being addressed according to their priority. Considerations to make when outsourcing Whether you're looking to outsource, or simply make your outsourced team better, there are some key items to address. Ashok Mani from AppLabs noted that organizations must look into a provider's engagement models, mobilization efforts, communication plans, security and scalability. These elements will be essential to clear up before trying to manage a team. \"While organizations are deriving value from outsourcing software development , outsourced software testing will maximize returns from their investments and provide the right level of objectivity and rigor required to create a high-quality product,\" Mani stated. \"If an independent QA and testing service provider is chosen whose focus is on ensuring quality products/systems are implemented, benefits will be fully maximized.\" Outsourcing a quality assurance team is going to have a few challenges for businesses. But by preparing for these obstacles, they will be able to manage the outsourced group more effectively. Having a communication plan and technology available will be essential to working well with the team and improving development operations."},{"title":"Mobile Application Development Primer","tags":"devops","url":"http://ciandcd.github.io/mobile-application-development-primer.html","text":"From: http://devops.linuxjournal.com/collaborative-development/mobile-application-development-primer-0 Industries of all varieties have begun to realize that the target audiences for their business applications have shifted in massive numbers from the use of traditional personal computers, such as desktops and laptops, to using mobile devices such as smart phones and tablets for accessing the internet and for obtaining the information they seek. This applies if the intended audience for the application is a direct customer of the enterprise (Business-to-Consumer apps, or \"B2C\"), or if the targeted user is an employee or business partner (\"B2E\" and \"B2B\", or Business-to-Employee and Business-to-Business apps). Across the globe, more people are now using mobile devices that they can carry with them wherever they go, and which are more user friendly and intuitive to use, as their primary means of obtaining information and requesting services over the internet."},{"title":"Notes from Troy Hunt's Hack Yourself First Workshop","tags":"devops","url":"http://ciandcd.github.io/notes-from-troy-hunts-hack-yourself-first-workshop.html","text":"From: http://java.dzone.com/articles/notes-troy-hunts-hack-yourself Troy Hunt ( @troyhunt , blog ) had a great, very hands-on 2-day workshop about webapp security at NDC Oslo. Here are my notes. Highlights – resources Personal security and privacy https://www.entropay.com/ – a Prepaid Virtual Visa Card mailinator.com – tmp email f-secure VPN https://www.netsparker.com/ – scan a site for issues (insecure cookies, framework disclosure, SQL injection, …) (lot of $k) Site security https://report-uri.io/ – get reports when CSP rules violated; also displays CSP headers for a site in a human-friendly way https://securityheaders.io/ check quality of headers wrt security free SSL – http://www.startssl.com/ , https://www.cloudflare.com/ (also provides web app firewall and other protections) ; SSL quality check: https://www.ssllabs.com/ssltest/ https://letsencrypt.org/ – free, automated, open Certificate Authority (Linux Found., Mozilla) Breaches etc. http://arstechnica.com/security/2015/06/hack-of-cloud-based-lastpass-exposes-encrypted-master-passwords/ https://twitter.com/jmgosney – one of ppl behind http://passwordscon.org . http://password-hashing.net experts panel. Team Hashcat. http://arstechnica.com/security/2012/12/25-gpu-cluster-cracks-every-standard-windows-password-in-6-hours/ To follow ! http://krebsonsecurity.com/ ! http://www.troyhunt.com/ ! https://www.schneier.com/ ! https://twitter.com/mikko (of F-Secure) also great [TED] talks kevin mitnick (jailed for hacking; twitter, books) Books http://www.amazon.com/We-Are-Anonymous-LulzSec-Insurgency/dp/0316213527 – easy read, hard to put down http://www.amazon.com/Ghost-Wires-Adventures-Worlds-Wanted/dp/1441793755 – about Mitnick's hacking, social engineering, living on the run ? http://www.amazon.com/Art-Intrusion-Exploits-Intruders-Deceivers/dp/0471782661/ Mitnick: http://www.amazon.com/Art-Deception-Controlling-Element-Security/dp/076454280X/ – social engineering Other https://www.xssposed.org/ See https://www.drupal.org/SA-CORE-2014-005 https://www.youtube.com/watch?v=Qvhdz8yE_po – Havij example http://www.troyhunt.com/2013/07/everything-you-wanted-to-know-about-sql.html , http://www.troyhunt.com/2010/05/owasp-top-10-for-net-developers-part-1.html , http://www.troyhunt.com/2012/12/stored-procedures-and-orms-wont-save.html , Googlee: find config files with SA access info: `inurl:ftp inurl:web.config filetype:config sa` https://scotthelme.co.uk/hardening-your-http-response-headers/ and https://securityheaders.io/ https://developer.mozilla.org/en-US/docs/Web/Security/Public_Key_Pinning – prevent MITM wappalyzer chrome plugin displaying info about the server and client that can be detected (jQuery, NewRelic, IIS, win OS, …) http://www.troyhunt.com/2015/05/do-you-really-want-bank-grade-security.html http://www.troyhunt.com/2012/05/everything-you-ever-wanted-to-know.html tool: https://github.com/gentilkiwi/mimikatz extract plaintexts passwords, hash, PIN code and kerberos tickets from memory on Windows Notes HackYourselfFirst.troyhunt.com – an example app with many vulnerabilities Note: maximizing your browser window will share info about your screen size, which might help to identify you haveibeenpwned.com – Troy's online DB of hacked accounts Tips check robots.txt to know what to access Example Issues no https on login page insecure psw requirements cookies not secure flag => sent over http incl. AuthCookie) psw sent in clear text in confirm email user enumeration, f.eks. an issue with AdultFriendFinder – entry someone's email to login to find out whether they've an account post illegal chars, get them displayed => injection no anti-automation (captcha) login confirm. email & autom. creating 1m accounts => sending 1m emails => pisses ppl off, likely increase one's spam reputation (=> harder to send emails) brute-force protection? ### XSS Reflected XSS: display unescaped user input Encoding context: HTML, JS, CSS … have diff. escape sequences for the same char (e.g. <) – look at where they're mixed Check the encoding consistency – manual encoding, omitting some chars JS => load ext resources, access cookies, manipulate the DOM Task: stal authCookie via search ### SQL injection Error-based injection: when the DB helps us by telling us what is wrong -> use ti learn more and even show some data Ex.: http://hackyourselffirst.troyhunt.com/Make/10?orderby=supercarid <—— supercarid is a column name orderby=(select * from userprofile) … learn about DB sructure, force an exception that shows the valueex.: (select top 1 cast(password) as int from userprofile) => \"Conversion failed for the nvar value ‘passw0rd …'\" Tips think of SQL commands that disclose structure: sys.(tables,columns), system commands enumerate records: nest queries: select top X ows asc then top 1 rows from that desc write out how you think the query works / is being constructed internally cast things to invalid types to disclose values in err msgs (or implicit cast due to -1 ..) #### Defenses whitelist input data types (id=123 => onlyallow ints) enumerable values – check against an appropr. whitelist if the value is stored – who uses it, how? making query/insertion safe permissions: give read-only permissions as much as possible; don't use admin user from your webapp ### Mobile apps Look at HTTP req for sensitive data – creds, account, … Apps may ignore certificate validations In your app: param tampering, auth bypass, direct object refs Weak often: airlines, small scale shops, fast foods, … Tips certificate pining – the app has the fingerprint of the server cert. hardcoded and doesn't trust even \"valid\" MITM certificate (banks, dropbox, …)x ### CSRF Cross-Site Request Forgery = make the user send a request => their auth cookie included async Ajax req to another site forbidden but that doesn't apply to normal post Protection anti-forgery tags ### Understanding fwrk disclosure http://www.shodanhq.com/ -> search for \"drupal 7\" -> pwn How disclosed: headers familiar signs – jsessionid cookie for java, … The default error and 404 responses may help to recognize the fwr HTML code (reactid), \".do\" for Sttruts implicit: order of headers (Apache x IIS), paths (capitalized?), response to improper HTTP version/protocol, => likely still possible to figure out the stack but not possible to simple search for fwrk+version ### Session hijacking Steal authentication cookie => use for illegal requests. Persistence over HTTP of auth., session: cookie, URL (but URL insecure – can be shared) Session/auth ID retrieval: insecure transport, referrer, stored in exceptions, XSS Factors limiting hijacking: short duration expiry, keyed to client device / IP (but IPs may rotate, esp, on mobile devices => be very cautious) DAY 2 ——– ### Cracking passwords Password hashing: salt: so that 2 ppl choosing the same psw will have a different hash => cracking is # salts * # passwords inst. of just N has cracking tips: character space Dictionary: passw0rd, … Mutations: manipulation and subst. of characters Tips: 1Password , LastPass, …. GPU ~ 100* faster than CPU #### Ex: Crack with hashcat common psw dict + md5-hashed passwords => crack ./hashcat-cli64.bin –hash-type=0 StratforHashes.txt hashkiller.com.dic # 23M psw dict -> Recovered.: 44 326/860 160 hashes [obs duplications] in 4 min (speed 135.35k plains) Q: What dictionary we use? Do we apply any mutations to it? ### Account enumeration = Does XY have an account? Multiple vectors (psw reset, register a new user with the same e-mail, …) Anti-automation: is there any? It may be inconsistent across vectors Does it matter? (<> privacy needs) How to \"ask\" the site and how to identify + and – responses? Timing attacks: distinguish positive x negative response based on the latency differing between the two ### HTTPS Confidentiality, Integrity, Authenticity Traffic hijacking: [a href=\"https://www.wifipineapple.com/\"]https://www.wifipineapple.com/ – wifi hotspot with evil capabilities monitor probe requests (the phone looks for networks it knows), present yourself as one of those, the phone connects autom. (if no encryption) Consider everything sent over HTTP to be compromised Look at HTTPS content embedded in untrusted pages (iframes, links) – e.g. payment page embedded in http Links HSTS Preload – tell Chrome, FF that your site should only be ever loaded over HTTPS – https://hstspreload.appspot.com/ https://www.owasp.org/index.php/HTTP_Strict_Transport_Security header ### Content Scurity Policy header https://developer.chrome.com/extensions/contentSecurityPolicy See e.g. https://haveibeenpwned.com/ headers w/o CSP anything can be added to the page via a reflected XSS risk Anyth, can be added to the DOM downstream (on a proxy) With CSP the browser will only load resources you white-list; any violations can be reported Use e.g. https://report-uri.io/home/generate to create it and the report to watch for violations to fine tune it. ### SQL injection cont'd (Yesterday: Error-Based) #### Union Based SQLi Modify the query to union whatever other data and show them. More data faster than error-based inj. Ex.: http://hackyourselffirst.troyhunt.com/CarsByCylinders?Cylinders=V12 : V12 -> `V12′ union select voteid, comments collate SQL_Latin1_General_CP1_CI_AS from vote– ` #### Blind Boolean (laborious) Blind inj.: We can't always rely on data being explicitly returned to the UI => ask a question, draw a conclusion about the data. Ex: http://hackyourselffirst.troyhunt.com/Supercar/Leaderboard?orderBy=PowerKw&asc=false -> ordedby => case when (select count(*) from userprofile) > 1 then powerkw else topspeedkm end Extract email: Is ascii of the lowercase char #1 < ascii of m ? Automation: SqlMap #### Time based blind injection When no useful output returned but yes/no responses differ significantly in how much time they take. F.ex. ask the db to delay the OK response. MS SQL: IF ‘b' > ‘a' WAITFOR DELAY '00:00:05′ ### Brute force attacks Are there any defences? Often not How are defences impl? block the req resources block the src IP rate limit (by src IP) ### Automation penetration testing apps and services such as Netsparker, WhiteHatSec targets identification: shodan, googledorks, randowm crawling think aout the actions that adhere to a pattern – sql injection, fuzzing (repeat a req. trying diff. values for fields – SQLi, …), directory enumeration automation can be used for good – test your site tip: have autom. penetration testing (and perhaps static code analysis) as a part fo your build pipeline Task: Get DB schema using sqlmap (see python2.7 sqlmap.py –help) ### Protection Intrusion Detection System (IDS) – e.g. Snort Web Application Firewall (WAF) – e.g. CloudFare ($20/m)"},{"title":"Open source and Samsung take center stage at Red Hat Summit","tags":"devops","url":"http://ciandcd.github.io/open-source-and-samsung-take-center-stage-at-red-hat-summit.html","text":"From: http://devops.com/2015/06/25/open-source-samsung-take-center-stage-red-hat-summit/ While much of the DevOps world was focused on DockerCon in San Francisco this week, there was also big news happening on the east coast. While DockerCon was shaking things up with the announcement of the Open Container Project the Red Hat Summit in Boston held its own share of big news and much of it spotlighted the important role of open source. \"Red Hat is really the independent voice of Linux focused entirely on business,\" said Rob Enderle, principal analyst with the Enderle Group . \"It is also a showcase of how to make money on the platform without taking advantage of anyone in the process and largely remaining true to the core tenets that created Linux in the first place.\" Focus on Open Source One of the primary core tenets—if not the defining core tenet—of Linux is open source. The shared development and collaborative nature of the operating system as an open source project is its main strength. At the Red Hat Summit open source played an even larger role than normal. \"The company continues to show strong momentum for OpenShift Commons as the re-write of the technology is moving forward showing that the platform-as-a-service (PaaS) battles are alive and well,\" explained Al Hilwa, program director of software development research for IDC . \"Red Hat also rebranded and updated its FeedHenry platform as it continues to turn it fully into open source and integrate it with OpenShift.\" Samsung Partnership Aside from the focus on open source, the other big news out of the Red Hat Summit was a new strategic alliance with Samsung. Samsung will push Red Hat's mobile platform and potentially other software into the enterprise in a more integrated fashion as a function of the new partnership. The relationship has potential, but according to Enderle there are both positive and negative outcomes possible. \"Samsung needs Red Hat far more than Red Hat needs Samsung because Red Hat provided the enterprise brand that Samsung needs to penetrate the space. Samsung is a firm that has never found a platform they didn't like and have historically been unable to focus on any one of them for extended periods of time,\" cautions Enderle. \"Red Hat alone won't be able to fix that and Samsung's behavior could actually tarnish Red Hat if they aren't careful.\" Strong Momentum Hilwa also made another observation about the strength of Red Hat's business model. \"That the company will cruise through the two billion dollar annual revenue barrier sometime this year is all but certain with its posted rates of growth. This is an enormous milestone for the open source world.\" We'll have to wait and see how the Red Hat / Samsung partnership pans out for either company, but the Red Hat Summit highlighted the strength of Red Hat's momentum and the unstoppable juggernaut of open source development."},{"title":"Rancher Labs hopes to help DevOps teams rope in container management","tags":"devops","url":"http://ciandcd.github.io/rancher-labs-hopes-to-help-devops-teams-rope-in-container-management.html","text":"From: http://devops.com/2015/06/25/rancher-labs-hopes-help-devops-teams-rope-container-management/ DevOps teams have turned to containerization in a big way in order to help improve their efficiency. But many enterprises have run into problems scaling containers across their infrastructure to account for issues like networking and load balancing. Rancher Labs, a Cupertino-based cloud software startup, is hoping to help organizations address the scalability with a new open-source Docker platform. Eponymously dubbed Rancher, the beta version for the platform went live this week. It's designed to be a full set of tools to help DevOps teams manage Docker containers in production. The platform folds in infrastructure services built specifically for containers, including networking, storage management, load balancing, service discovery, monitoring, and resource management. The goal of containers is efficiency and full portability, but Sheng Liang, co-founder and CEO at Rancher Labs, notes that the implementation can be hard to get perfect. \"As users deploy containers across different infrastructures, they quickly realize that different clouds, virtualization platforms and bare metal servers have dramatically different infrastructure capabilities,\" said Liang. It's these variables that Rancher seeks to limit. \"By building a common infrastructure backplane across any resource, Rancher implements an entirely new approach to hybrid cloud computing,\" said Liang. The company is going long on containers—an assessment that backers agree with. The company also recently announced that it has received $10 million in series A funding from Mayfield and Nexus Venture Partners. In addition to its Rancher suite, the company also produces RancherOS, a minimalist Linux distribution tightly focused on running Docker containers. The management team behind Rancher is one that is familiar to many cloud practitioners. Co-founder and CEO Sheng Liang was co-founder and CEO of Cloud.com, and continued on at Citrix as CTO of Cloud Platforms Group after the company's 2011 acquisition. Also on board are Cloud.com and Citrix veterans Shannon Williams as vice president of sales and marketing, Darren Shepherd as chief architect and Will Chan as vice president of engineering. According to industry analyst Paul Burns, Rancher Labs' niche is ripe for new innovation. \"In its current state, running any containerization solution at scale and in production requires a significant amount of effort from those adopting the solution,\" says Burns, who is president of analyst firm Neovise. \"Key functions such as networking, service discovery, storage and load balancing can cause integration issues, keeping enterprises from truly reaping the benefits desired. By providing a unique solution that eases and simplifies the management of container software, Rancher is allowing companies to better utilize and benefit from the technology.\""},{"title":"Seven ways to reduce waste and accelerate software delivery","tags":"devops","url":"http://ciandcd.github.io/seven-ways-to-reduce-waste-and-accelerate-software-delivery.html","text":"From: http://devops.linuxjournal.com/collaborative-development/seven-ways-reduce-waste-and-accelerate-software-delivery This paper will explore best practices for identifying and eliminating seven types of waste: Waiting, handoffs and task switching, motion, extra processes, extra features, partially completed work and defects."},{"title":"Smart Planning for Smarter Infrastructure","tags":"devops","url":"http://ciandcd.github.io/smart-planning-for-smarter-infrastructure.html","text":"From: http://devops.linuxjournal.com/collaborative-development/smart-planning-smarter-infrastructure A smart Infrastructure project is a large system of systems. This paper looks at how a systems engineering approach can benefit an organization planning smart infrastructure projects."},{"title":"Smarter quality management: The fast track to competitive advantage","tags":"devops","url":"http://ciandcd.github.io/smarter-quality-management-the-fast-track-to-competitive-advantage.html","text":"From: http://devops.linuxjournal.com/continuous-testing/smarter-quality-management-fast-track-competitive-advantage This paper introduces quality management (QM), a practical, multi-disciplined approach to software delivery that helps reduce time to market without sacrificing quality in the outcome."},{"title":"Ten Answers Regarding Mobile App Testing","tags":"devops","url":"http://ciandcd.github.io/ten-answers-regarding-mobile-app-testing.html","text":"From: http://devops.linuxjournal.com/develop-deploy/ten-answers-regarding-mobile-app-testing This white paper digs deep into the reasons testing mobile apps is fundamentally harder than traditional web or desktop applications. A collaboration by Tina Zhuo and Dennis Schultz from IBM along with Yoram Mizrachi from Perfecto Mobile and John Montgomery from uTest, these experts explore the complexities of mobile test environments, the value of the mobile device cloud, the unique role crowd sourcing can play, and how teams can leverage automation to help deliver quality apps."},{"title":"Building GitHub Pull Requests with Continua CI","tags":"ciandcd","url":"http://ciandcd.github.io/building-github-pull-requests-with-continua-ci.html","text":"From: https://www.finalbuilder.com/resources/blogs/postid/700/building-github-pull-requests-with-continua-ci GitHub makes it relatively simple to contribute to open source projects, just fork the repository, make your changes, submit a pull request. Couldn't be simpler. Accepting those Pull requests, is dead simple too, most of the time. But what if you want to build and test the pull request first, before accepting the request. Fortunately the nature of GitHub Pull requests (or more to the point, Git itself) makes this possible. Git References Git References are a complex topic all on it's own, but lets take a quick look at a typical cloned repository. In the .git folder, open config file in notepad and take a look at the [remote \"origin\"] section, here's what mine looks like : [remote \"origin\"] url = https://github.com/VSoftTechnologies/playground.git fetch = +refs/heads/*:refs/remotes/origin/* The key entry here is the fetch. Quoting from the git documentation : \"The format of the refspec is an optional + , followed by <src>:<dst> , where <src> is the pattern for references on the remote side and <dst> is where those references will be written locally. The + tells Git to update the reference even if it isn't a fast-forward.\" The default fetch refspec will pull any branches from the original repository to our clone. But where are our pull requests? Anatomy of a pull request When a pull request is submitted, GitHub make use of Git References to essentially \"attach\" your pull request to the original repository. But in my local clone, I won't see them because the default fetch refspec doesn't include them. You can see the pull requests by using the git ls-remote command on the origin : $ git ls-remote origin $ git ls-remote origin 27dfaaf83f60ac26a6fe465042f2ddb515667ff1 HEAD 654b98d6eb862e247e5c043460e9f9a64b2f0972 refs/heads/Test 27dfaaf83f60ac26a6fe465042f2ddb515667ff1 refs/heads/master b333438310a56823f1938071af8c697b202bf855 refs/pull/1/head 95cb80af1330e73188ea32659d7744dcfe37ab43 refs/pull/2/head 90ba13b8edaab04505396dbcb1853f6f9bdaed64 refs/pull/2/merge Notice something odd there. There are two pull requests, but pull request 2 has two entries in the list, whilst pull request 1 has only 1 entry. refs/pull/2/head is a reference to the head commit of your pull request, whilst refs/pull/2/merge is a reference to the result of the automatic merge that GitHub does. On pull request 1, there was a merge conflict, so the the /merge reference was not created, on pull request 2, the merge succeeded. On the pull request page, you would typically see something like this if the merge succeeded : Getting Continua CI to see the Pull Requests The main reason for building pull requests on your CI server is to see if they build, and to run your unit tests against that build. You can chose to build the original pull request, or the result of the automatic merge, or both. In reality, if the automatic merge failed, then the person who submitted the pull request has some more work to do, so there's really no point building/testing the original pull request. What you really want to know, is \"if I accept this request, will it build and the tests pass\", so it's generally best to only build the automatic merge version of the pull request. Continua CI makes this quite simple. On the Git Repository settings, check the \"Fetch other Remote Refs\" option. This will show the Other Refs text area, which already has a default RefSpec that will fetch the pull requests (the merged versions), and create local (to Continua CI) branches with the name pr/#number - so pull request 1 becomes branch pr/1. You can modify this to taste, for example if you are fetching both the merge and the head versions of the pull requests, you might use a refespec like this : +refs/pull/*/merge:refs/remotes/origin/pr-merge/* +refs/pull/*/merge:refs/remotes/origin/pr-head/* Building the pull Requests Now we have gotten this far (which is to say, you enabled one option and clicked on save!) we can build the pull requests (it may take a few minutes to fetch the pull requests). If you manually start a build, you can select the pull request from the branch field for the github repository using the intellsense, just start typing pr/ and you will see a list : Now we can add a trigger to build pull requests (we are talking continuous integration after all). Using the Pattern Matched Branch feature on Continua CI Triggers you can make your trigger start builds when a pull request changeset is fetched from Github. The pattern is a regular expression, so &#94;pr/.* would match our pull request branches (assuming you we use the default refspec) Adding a trigger specific to the pull requests allows you to set variables differently from other branches, and you will then be able to tailor your stages according to whether you are building a pull request or not. For example, you probably don't want to run your deploy stage when building a pull request). Updating GitHup Pull Request Status One last thing you might like to add, is to update the Pull Request Status . This can be done using the Update GitHub Status Action in Continua CI (In a future update this will done via build event handlers, a new feature currently in development). This is what the pull request might look like after the status is updated by Continua CI :"},{"title":"Continua 1.5 released","tags":"ciandcd","url":"http://ciandcd.github.io/continua-15-released.html","text":"From: https://www.finalbuilder.com/resources/blogs/postid/721/continua-15-released Continua Version 1.5 is now available for download Today marks a milestone in Continua CI as we release version 1.5 of the product. Its been many months in the making, we hope you enjoy the update as much as we enjoyed making it. There are many features that we think you'll benefit from by updating to v1.5, some of these include: Reworked UI: Now using bootstrap framework for styling Redesigned dashboards that show more information including graphs. Added stages information to the Project tile and list views Disabled configurations are now displayed as faded Cloning: Can now clone whole projects and clone configurations between projects. Stage Conditions: Auto Promote conditions - stages can now use conditions to control whether to auto-promote to the next stage. Skip conditions - you can now provide conditions for skipping stages or disable a stage completely. New Actions: Update GitHub Status Action is now deprecated (use event handler instead - see below). NuGet restore action . Fake (F#) build runner. Repository Tags: (Git, Mercurial and Subversion repositories only) Continua CI can now detect and list repository tags. Tags are now displayed in changeset tabs on configuration and build views. Repository trigger can now be set to trigger on tag changes (new tags, edits and deletions) changes). You can now run a build on a tagged changeset . Octopus Deploy: Create/Deploy/Promote Octopus Deploy releases. Tag Repository Changesets: Apply tags to a repository changeset (Git, Mercurial and Subversion repositories only) Update GitHub Status: replaces the Update GitHub Status action . and many more changes including: Styling changes for improved handling on small screen sizes Report ordering: you can choose which one is displayed first on the build view. New expression functions: Utils.NewGuid() and Utils.RandomNumber() can be used for generation of temporary paths for example Additional LatestChangeset object within the repository object with Branch, BranchName, Comment, CommitterUserName, CommitterFullName, Created, FileCount, Id and RepositoryUsername properties to use in expressions Continua now supports DUnitX enhanced Command Line Options Updating Updating to this new release is the same regardless if you are using v1.0.X or a recent build from the beta or release candidate. Simply download the installer and run it, the setup will guide you through the install process. As usual we are available on support@finalbuilder.com if you run into any troubles. For this release you will need to update both the server and agents. A word of thanks The team wishes to thank everyone who has participated in the beta and release candidate stages for this release. Your positive feedback has been invaluable in shaping the features and functionality of the product. Thank you for your continued support."},{"title":"Continua CI and the OpenSSL Heartbleed Vulnerability","tags":"ciandcd","url":"http://ciandcd.github.io/continua-ci-and-the-openssl-heartbleed-vulnerability.html","text":"From: https://www.finalbuilder.com/resources/blogs/postid/706/continua-ci-and-the-openssl-heartbleed-vulnerability The short answer is No . Continua CI itself does not use Open SSL directly, but the default database engine, PostgreSQL, does. The version of PostgreSQL we ship with Continua CI is 9.1.3 .1254 and it does include a version of OpenSSL with the vulnerability, however ssl support is turned off by default and is not used by Continua CI. We also update the pg_hba.conf during install to only allow connections from localhost, however it turns out that if ssl is enabled, the ssl negotiation happens before the rules in pb_hba.conf are matched and this alone does not protect the server. If you are using your own install of PostgreSQL (or you want to be sure that what I say is correct) then I suggest you check your PostgreSQL server. You and easily check if ssl is enabled by running the following query in PGAdmin: show ssl Another option is to try the testing tool here : https://github.com/titanous/heartbleeder heartbleeder -pg yourciserver:9001 here's the output from testing one of our CI servers : heartbleeder.exe -pg pilatus:9001 Error connecting to pilatus:9001: dial tcp 10.0.0.80:9001: ConnectEx tcp: No connection could be made because the target machine actively refused it. If you are using SQLServer, then you (for once) are ok, SQL Server doesn's use OpenSSL. We will issue an update in the next few days with an updated PostgreSQL version once we have completed testing By now you have probably heard or read about the OpenSSL Heartbleed Vulnerability (unless you have been living under a rock for the last week)! We have had a few customers ask us whether Continua CI is vulnerable to this exploit.The short answer isContinua CI itself does not use Open SSL directly, but the default database engine, PostgreSQL, does. The version of PostgreSQL we ship with Continua CI is 9.1.3 .1254 and it does include a version of OpenSSL with the vulnerability, however ssl support is turned off by default and is not used by Continua CI.We also update the pg_hba.conf during install to only allow connections from localhost, however it turns out that if ssl is enabled, the ssl negotiation happens before the rules in pb_hba.conf are matched and this alone does not protect the server.If you are using your own install of PostgreSQL (or you want to be sure that what I say is correct) then I suggest you check your PostgreSQL server. You and easily check if ssl is enabled by running the following query in PGAdmin:Another option is to try the testing tool here :here's the output from testing one of our CI servers :If you are using SQLServer, then you (for once) are ok, SQL Server doesn's use OpenSSL.We will issue an update in the next few days with an updated PostgreSQL version once we have completed testing"},{"title":"ContinuaCI Version 1.5","tags":"ciandcd","url":"http://ciandcd.github.io/continuaci-version-15.html","text":"From: https://www.finalbuilder.com/resources/blogs/postid/711/continua-15-new-dashboards With the upcoming 1.5 release of ContinuaCI we have made a number of enhancements to the dashboards, we think you'll love them! Here is a peek at what's coming soon. The New List View Improvements: - Stage indicator blocks provide quick drill-down into the build log. - Improved viability and responsiveness of build actions buttons. - Build action buttons moved to the left for easier access. - Viability enhancements around projects. - Improved Responsiveness and performance tweeks. List View: With Builds Completed List View: With Builds Running The New Details View Improvements: - Build and Queue times now have graphs! - Build status card on the left hand side displays the status of the build as it progresses. - Build action buttons are more obvious and responsive. - Stage indicator blocks (present on the build status cards) provide quick drill-down into the build log. - Improved Responsiveness and performance tweeks. Details View: with Builds Queued Details View: with Builds Executing Details View: with Builds Finished Stay tuned for more exciting details regarding the version 1.5 release! With the upcoming 1.5 release of ContinuaCI we have made a number of enhancements to the dashboards, we think you'll love them! Here is a peek at what's coming soon.Stay tuned for more exciting details regarding the version 1.5 release!"},{"title":"Continuous Delivery with Go","tags":"ciandcd","url":"http://ciandcd.github.io/continuous-delivery-with-go.html","text":"From: http://www.go.cd/2014/08/07/go-webinar-recording.html Every couple weeks ThoughtWorks hosts learning sessions for people who want more information about continuous delivery with Go. This is a recording of the session from 7 August, 2014"},{"title":"Continuous integration and deployment solution!","tags":"ciandcd","url":"http://ciandcd.github.io/continuous-integration-and-deployment-solution.html","text":"From: http://www.pmease.com/hotnews?id=1 QuickBuild 6.0 is now available Feature highlights in this release: Find repository/step/variable overrides and usages for configuration refactoring. Optionally trust authenticated user in specified http header to support single sign-on. Permission set definition to facilitate assigning same set of permissions repeatedly. Administrator can select to run as arbitrary user to facilitate checking user profile. Aggregate SCM changes to display change summary and statistics in high level configuration. Gerrit integration to verify open changes and score specified Gerrit label accordingly. JFrog Artifactory integration to publish and use artifacts during build. Persist unprocessed build requests after server shutdown and resume processing after startup. Accurev proof build to test active changes on QuickBuild before getting them promoted. Optionally run scripts after deletion of configuration and build. Able to view live log by step, and view log of finished steps before build finishes. Step to record SCM changes without checking out the repository. Able to display custom banner in QuickBuild page. For detailed explanation of all features added in this release, please visit We are proud to annouce QuickBuild 6.0.Feature highlights in this release:For detailed explanation of all features added in this release, please visit http://www.pmease.com/features/whats-new QuickBuild 5.1 is now available Feature highlights in this release: Verify GitHub pull requests and update pull request status based on build result. GitHub issue tracker integration to parse issues in commit messages. Leverage perforce shelve/unshelve feature to run pre-commit builds without using user agent. Retrieve changes of Subversion externals for source view and diff. Custom columns to display custom build and request info. Display reasons for waiting builds and steps. Define environment variables in composite steps for inheritance and overriding. Detect broken communication links to agents to fail build fast. Drag&drop to organize favorite dashboards. Dashboard list to display all dasbhoards in system. Resource access information to know about resource usage status. Coverity report rendering For detailed explanation of all features added in this release, please visit We are proud to annouce the formal release of QuickBuild 5.1Feature highlights in this release:For detailed explanation of all features added in this release, please visit http://www.pmease.com/features/whats-new QuickBuild 5.0 is now available Feature highlights in this release: Launch build agent on demand in cloud environment including Amazon EC2. Build pipeline to visualize commits life cycle across different build and deployment stages. Optionally store artifacts of configuration sub tree to specified build agents to reduce server load. Grid and server metrics collecting and trending. Alert definition and notification for key performance indicators. Enhanced tray monitor and refined message window. Toggle node and step information in build log. Share dashboards with specified users besides groups. Headless plugin build. New dashboard gadgets to display build pipeline, grid performance measurements and system alerts. For detailed explanation of all features added in this release, please visit We are proud to annouce the formal release of QuickBuild 5.Feature highlights in this release:For detailed explanation of all features added in this release, please visit http://www.pmease.com/features/whats-new QuickBuild 4.0 formal release is now available Feature highlights in this release: Customizable dashboard for users and groups to organize build information via gadgets. Report aggregation to provide build metrics summary of descendant configurations. Resource management for better control of build distribution and agent load. Grid partition to divide grid nodes between different configuration trees. User activity audit to track and review every modification to the system. CollabNet TeamForge integration for user authentication, file uploading, release creation, issue linking. and issue updating. Redmine integration to link QuickBuild builds with Redmine issues. Google Repo integration to detect changes, check out source, and create tags against Repo. Boost test integration to render test reports and display test trends. Redesigned report system for improved user experience and performance. RESTful API for changes, issues, and various reports. Plugin API for third party issue tracker and unit test framework integration. Searchable users and groups. For detailed explanation of all features added in this release, please visit We are proud to annouce the formal release of QuickBuild 4.Feature highlights in this release:For detailed explanation of all features added in this release, please visit http://www.pmease.com/features/whats-new QuickBuild 3.1 - distributed version control system integration and enhanced .NET support QuickBuild 3.1 is released to integrate with Git, Mercurial, Bazaar and Team Foundation Server. This integration makes possible below actions in a continuous integration or release management environment when dealing with these SCMs: Retrieve source code for build and test from tip or specified revision. Create tags for retrieved source code if necessary. Detect source changes between builds and notify committers under specified condition. Promote SCM revisions to higher stage, for example from qa to release. Git, Mercurial and Bazaar integration also includes the gated push feature, with which you can submit ready-for-push commits to QuickBuild for build/test, and have QuickBuild to push them to the official repository automatically after building/testing successfully. This release also supports to build .NET projects through MSBuild and Visual Studio solution builder. Refer to http://www.pmease.com/features/whats-new/ for details. QuickBuild 3.1 beta1 released to support Git, Mercurial, TFS and Bazaar You may visit Git, Mercurial, Team Foundation Server and Bazaar support is now in beta. In this beta, QuickBuild can checkout code, create tags, detect changes, view/diff source files from these version control systems. Proof build support is not yet included but will be delivered in future betas.You may visit this link to download the beta. Any feedbacks or suggestions are very welcomed! The formal release of QuickBuild 3 is now available This release works tightly with issue tracking systems to provide an integrated view of issues, builds and SCM changes. No longer worry about which issues are fixed in a particular build, or which build a particular issue is fixed in. QuickBuild tracks these information for you automatically! The release management functionality is improved considerably with the ability to use next unreleased version in issue tracker as next build version, and push built versions into issue tracker as released versions. Currently JIRA, Trac and Bugzilla are supported. Other feature highlights in this release: Step can be repeated for different set of parameters, either parallelly, or sequentially. For example, you may create a singe test step to have it execute for each combination of possible databases and OS platforms, or have it run on all applicable build agents. This can greatly reduce number of steps needed in a complex build workflow. QuickBuild can now terminate spawned build processes immediately and reliably when a build is canceled or timed out. You no longer need to manually kill relevant processes to release workspace mutexes. This works on Windows, Linux and Unix platforms. A non-admin account can now be authorized to administer a configuration subtree. Multiple promote actions can be defined with the ability to customize the condition of each action. For example, you may define a release action and have it appear only when build is recommended and current user belongs to release manager group. Inherited settings such as steps, repositories and variables will be displayed directly in descendant configurations. This makes examination and modification of inherited settings much easier. Build workflow can now be created/rearranged by dragging and dropping steps. Trends of duration and success rate of each executed steps are now available in statistics tab of a configuration. You can even compare these trends between different steps to find out which step fails the most and which step costs the most time. SCM changes screen is reworked to support text search in changes between two arbitrary builds. The same step can now be reused in different composition steps. Add the option of auto-detecting user time zone from browser to display local date/time. For detailed explanation of all features added in this release, please visit We are pround to annouce the formal release of QuickBuild 3.This release works tightly with issue tracking systems to provide an integrated view of issues, builds and SCM changes. No longer worry about which issues are fixed in a particular build, or which build a particular issue is fixed in. QuickBuild tracks these information for you automatically! The release management functionality is improved considerably with the ability to use next unreleased version in issue tracker as next build version, and push built versions into issue tracker as released versions. Currently JIRA, Trac and Bugzilla are supported.Other feature highlights in this release:For detailed explanation of all features added in this release, please visit http://www.pmease.com/features/whats-new QuickBuild 3.0 beta1 is now available The first beta of QuickBuild 3.0 is now available. This release integrates tightly with JIRA, Trac and Bugzilla to streamline the development process. Other improvements include reusable and repeatable steps, inheritance visibility, process tree killing, build engine optimization, UI polishments. Refer to release notes for details. QuickBuild 2.1 is available now QuickBuild 2.1 is just released with plugin and RESTful API, a cross-platform tray monitor, FxCop, NCover and CPD support, custom statistics, Oracle and SSL support, and much more. Refer to what's new for a complete list of new features added to this release. The brand new QuickBuild 2.0 is released Please refer to After years of development and test, QuickBuild 2.0 is finally released to embrace latest innovations in continuous integration and build management area. Most important features introduced in this version are pre-commit test, advanced build grid, versatile build reports, graphical build workflow design, visual build promotion, source code view/diff, and build comparison. QuickBuild 2.0 also includes enormous improvments such as intuitive user interface, fine-grained permission control, real time build progress and log monitoring, variable prompting.Please refer to the feature page for the complete list of achievements in this version."},{"title":"Deployment with Continua CI 1.5 and Octopus Deploy","tags":"ciandcd","url":"http://ciandcd.github.io/deployment-with-continua-ci-15-and-octopus-deploy.html","text":"From: https://www.finalbuilder.com/resources/blogs/postid/712/deployment-with-continua-ci-and-octopus-deploy So you've got your Continua CI server set up to automatically build, run unit tests and produce reports for your awesome new web application. Now you're ready to try out your project in its natural environment and then eventually release it to the wild for well-deserved public applause. Up until now, your options were either to use a Copy action to push the files up to test server and a PowerShell action to set up web services, or preferably run a FinalBuilder script utilising the plethora of actions available for transferring files and interacting with web servers. As of version 1.5, Continua CI can also work together with This post will walk through the steps required to push a .Net web application built in Continua to Octopus Deploy and trigger a deployment process to effortlessly get your application running on your test and production servers. Preparing your solution Octopus Deploy requires that you provide your applications as Lets go with the recommended OctoPack option. First prepare your Visual Studio solution - use the NuGet package manager to install the OctoPack package into the projects you want to deploy. This will include web application projects, console application projects and Windows service projects but not class libraries and unit test projects. You can now optionally add a Setting up the deployment process Next head over to your Octopus server and set up a deployment project. This should include a \"Deploy a NuGet package\" process step as below. We will set this to retrieve the application package from the built-in NuGet feed. Note that the NuGet package id should match the id element in your .nuspec file - this will default to the name of your assembly. We added a few more steps: And some variables: Setting up the build process You can now get back to Continua and set up a configuration for building your project. Once you have entered the configuration details and linked up the repository containing your project, move on over to the Stages page: For this simple example you'll need two actions: a NuGet Restore action to ensure that the OctoPack package is available for the build and an MSBuild action to build and push the application to your Octopus Deploy server. Just enter the path to your solution for the NuGet Restore action (the other fields can be left as is) and complete the main tab of the MSBuild action as required for your project. You then need to enter some additional properties to tell MSBuild to run OctoPack and tell it where to send your package. Set the RunOctoPack property to true and the OctoPackPublishPackageToHttp property to the URL for the NuGet feed on the Octopus Deploy server e.g. You will also need to provide an OctoPackPublishAPIKey property – you can generate an API key on your profile page on the Octopus Deploy server. Optionally, y ou can use the OctoPackPackageVersion to specify up the package version. Here we use Continua expressions to set this based on the build version. If you leave this out then OctoPack will get this value from the AssemblyVersion property in your AssemblyInfo.cs file. Once the actions are set up and saved, run a build and check that your package gets uploaded to the Octopus Deploy server. Then create a release for your deployment project and test that it deploys ok. Now we are ready to look into how to run this process automatically from Continua CI. Build event handler On the Continua CI configuration wizard after Stages, we have a new area titled Events. Here you can add Create a new build event handler, give it a name and select the Octopus Deploy as the Type. You can now provide general project details under the Octopus Deploy tab. The Octopus Deploy URL should end with '/api' e.g. http://octopusserver/api. Enter the API key generated under your Octopus Deploy profile and the name of your deployment project. You can then choose one or more actions to run. The available options are Create , Deploy and Promote and are used to create a new deployment release, deploy a release to an environment and promote a release from one environment to another. As you select each action, new tabs open so you can provide further details and hook the action to a build event. Creating a release Before you can deploy an application you need to create a Octopus Deploy release When creating a release you can specify the Release Version or leave this blank to automatically create a number based on the highest package version. You must provide either a Default Package Version or Step Package Versions for each step which requires one e.g. Flip over to the Create Options tab to tell Continua when to create the release. There are six On Before Stage Start On Sending Stage To Agent On Stage Completed On Build Pending Promotion On After Build Continued On Build Completed Generally we want to create the release at the start of the build before the first stage starts. Deploying to an environment Now on to the crux of this whole process - deploying your application. We generally deploy to a Test environment first and then, once we are happy with the outcome, promote to a User Acceptance environment or directly to Production. Continua CI allows you to deploy a release previously created by a Create action in the same build event handler, the highest release version in the project or a specific release version. It's up to you to ensure that the release version exists before the deploy action is run. An environment can consist of multiple machines - you can specify which machines you want to deploy to. If no machines are specified then the release will be deployed to all machines in the environment. When selecting the Build Event for deployment, ensure that it is triggerred after the package has been built and pushed to the Octopus Deploy server. Here we have set this to be run once the Build stage has completed successfully. Promoting a release You can promote the latest release from one environment to another. Ideally this would be linked to the promotion of a stage e.g. a testing stage, so that the application can be promoted from a test environment to production environment. We have set our test stage to require manual promotion; and set our promote action to run when a build is continued after waiting for promotion. Variables You can also pass variables from Continua CI to your deployment, these will be sent to the Octopus Deploy server before each action is run, updating the variables for the deployment project. We have used expressions is this example to send the build versions number and branch name. These variables can then be used to update project files with details for display or configure services differently depending on the source of the project. Running the configuration Once your build event handler dialog has been completed and saved, its time to start running the configuration. As the build processes Continua CI will display status information mirroring the process running on Octopus Deploy. You can also see full details of the deployment process in the build log. And all going well you will now see a successful deployment on your Octopus Deploy server! So you've got your Continua CI server set up to automatically build, run unit tests and produce reports for your awesome new web application. Now you're ready to try out your project in its natural environment and then eventually release it to the wild for well-deserved public applause.Up until now, your options were either to use a Copy action to push the files up to test server and a PowerShell action to set up web services, or preferably run a FinalBuilder script utilising the plethora of actions available for transferring files and interacting with web servers.As of version 1.5, Continua CI can also work together with Octopus Deploy server to provide an end-to-end continuous delivery mechanism. Using the new build event handlers feature, Continua CI builds can now be set up to create Octopus Deploy releases and initiate deployment to test and production environments, at key points in the build process.This post will walk through the steps required to push a .Net web application built in Continua to Octopus Deploy and trigger a deployment process to effortlessly get your application running on your test and production servers.Octopus Deploy requires that you provide your applications as NuGet packages . You can create and push the package to the Octopus Deploy server using Nuget Pack and Push actions, or create and push an OctoPack from MSBuild or VisualStudio build runner actions.Lets go with the recommended OctoPack option. First prepare your Visual Studio solution - use the NuGet package manager to install the OctoPack package into the projects you want to deploy. This will include web application projects, console application projects and Windows service projects but not class libraries and unit test projects.You can now optionally add a .nuspec file to the root folder of your project to describe the contents of your package. If you don't provide a .nuspec file, OctoPack will automatically create one based on your project settings.Next head over to your Octopus server and set up a deployment project. This should include a \"Deploy a NuGet package\" process step as below.We will set this to retrieve the application package from the built-in NuGet feed. Note that the NuGet package id should match the id element in your .nuspec file - this will default to the name of your assembly.We added a few more steps:And some variables:You can now get back to Continua and set up a configuration for building your project. Once you have entered the configuration details and linked up the repository containing your project, move on over to the Stages page:For this simple example you'll need two actions: a NuGet Restore action to ensure that the OctoPack package is available for the build and an MSBuild action to build and push the application to your Octopus Deploy server.Just enter the path to your solution for the NuGet Restore action (the other fields can be left as is) and complete the main tab of the MSBuild action as required for your project.You then need to enter some additional properties to tell MSBuild to run OctoPack and tell it where to send your package.Set the RunOctoPack property to true and the OctoPackPublishPackageToHttp property to the URL for the NuGet feed on the Octopus Deploy server e.g. http://octopusserver/nuget/packages You will also need to provide an OctoPackPublishAPIKey property – you can generate an API key on your profile page on the Octopus Deploy server.ou can use the OctoPackPackageVersion to specify up the package version. Here we use Continua expressions to set this based on the build version. If you leave this out then OctoPack will get this value from the AssemblyVersion property in your AssemblyInfo.cs file.Once the actions are set up and saved, run a build and check that your package gets uploaded to the Octopus Deploy server. Then create a release for your deployment project and test that it deploys ok. Now we are ready to look into how to run this process automatically from Continua CI.On the Continua CI configuration wizard after Stages, we have a new area titled Events. Here you can add Build Event Handlers for tagging repository changesets, updating the GitHub status and interacting with Octopus Deploy.Create a new build event handler, give it a name and select the Octopus Deploy as the Type.You can now provide general project details under the Octopus Deploy tab.The Octopus Deploy URL should end with '/api' e.g. http://octopusserver/api. Enter the API key generated under your Octopus Deploy profile and the name of your deployment project.You can then choose one or more actions to run. The available options areandand are used to create a new deployment release, deploy a release to an environment and promote a release from one environment to another. As you select each action, new tabs open so you can provide further details and hook the action to a build event.Before you can deploy an application you need to create a Octopus Deploy releaseWhen creating a release you can specify the Release Version or leave this blank to automatically create a number based on the highest package version. You must provide either a Default Package Version or Step Package Versions for each step which requires one e.g.Flip over to the Create Options tab to tell Continua when to create the release.There are six Build Events available to choose from. Some allow you to select a Stage and some allow you to select a successful or failed Build StatusGenerally we want to create the release at the start of the build before the first stage starts.Now on to the crux of this whole process - deploying your application. We generally deploy to a Test environment first and then, once we are happy with the outcome, promote to a User Acceptance environment or directly to Production. Continua CI allows you to deploy a release previously created by a Create action in the same build event handler, the highest release version in the project or a specific release version. It's up to you to ensure that the release version exists before the deploy action is run. An environment can consist of multiple machines - you can specify which machines you want to deploy to. If no machines are specified then the release will be deployed to all machines in the environment.When selecting the Build Event for deployment, ensure that it is triggerred after the package has been built and pushed to the Octopus Deploy server. Here we have set this to be run once the Build stage has completed successfully.You can promote the latest release from one environment to another. Ideally this would be linked to the promotion of a stage e.g. a testing stage, so that the application can be promoted from a test environment to production environment.We have set our test stage to require manual promotion;and set our promote action to run when a build is continued after waiting for promotion.You can also pass variables from Continua CI to your deployment, these will be sent to the Octopus Deploy server before each action is run, updating the variables for the deployment project. We have used expressions is this example to send the build versions number and branch name. These variables can then be used to update project files with details for display or configure services differently depending on the source of the project.Once your build event handler dialog has been completed and saved, its time to start running the configuration. As the build processes Continua CI will display status information mirroring the process running on Octopus Deploy.You can also see full details of the deployment process in the build log.And all going well you will now see a successful deployment on your Octopus Deploy server!"},{"title":"Distributed Test Execution with Go + TLB","tags":"ciandcd","url":"http://ciandcd.github.io/distributed-test-execution-with-go-tlb.html","text":"From: http://www.go.cd/2014/10/09/Distrubuted-Test-Execution.html Writing tests has finally become the norm. Consequently, running tests for every commit is central to & the most time consuming activity in any CI/CD setup. In a decent-sized production quality project you tend to have thousands of tests. That means the cycle time, i.e. the time it takes for a commit to reach deployable state (after running all unit, integration & functional tests), keeps growing. It gets harder when teams follow XP related practices like \"small commits, frequent commits\" since it causes parallel builds & resource starvation. One such example is Go's codebase. Just the \"Common\" & \"Server\" components of Go which comprises of unit & integration tests, together has ~6000 tests which will take about ~5 hours if run serially! The functional test suite is about 260+ tests with combined runtime of ~15 hours. That's close to a day & we haven't even run everything for a single commit! Note that the number of tests is so huge that just putting in a powerful box & running test in parallel will not bring it down to acceptable limits. Also, a large number of other problems surface if you start running tests in parallel on same box (without sandboxed environment) like concurrency issues etc. Solution [Go + TLB] Go improves the cycle time of its own build by making test execution faster, distributing it across many agents (machines). After this \"Common\" + \"Server\" takes 20 minutes. All functional tests run in 45 minutes. Thats close to an hour! Still not ideal (a few minutes - constrained by resource availability), but better. :) Test Load Balancer (TLB) TLB is an open-source library which provides the ability to break up a test suite into pieces and run a part. It guarantees 'Mutual Exclusion' & 'Collective Exhaustion' properties that are essential to reliably running tests in distributed fashion. TLB's strength lies in intelligent test distribution which is based on time, i.e. the tests will be distributed based on time they take to execute, making the jobs close to equal runs which leads to better resource utilization. It falls back on count based splitting if test times are not available. It also runs tests in 'Failed First' order, so if a test has failed in previous run it will be run before other tests which means faster feedback. Note: As of this writing, TLB integrates with JUnit (through Ant, Maven & Buildr), RSpec (through Rake), Cucumber (through Rake), Twist (through Ant & Buildr). Quick Setup Download TLB Unzip tlb-complete-0.3.2.tar.gz to tlb-complete-0.3.2 $ cd tlb-complete-0.3.2/server $ chmod +x server.sh $ ./server.sh start This should start server at http://host-ip-address:7019 Resources: Go Go is an open-source CI/CD tool. Its well known for its powerful modelling, tracing & visualization capabilities. While TLB is doing all the distribution, Go does what it does best - orchestrate the parallel execution. Run 'X' instances Starting release 14.3 you can spawn 'x' instances of a job. So if you want to distribute your tests across 10 machines you just need to set run instance count to 10 & Go will spawn 10 instances of the job when scheduling. Sample Configuration Setup a pipeline with material (SCM) that contains your tests. Setup Job to spawn required number of instances (run instance count). Setup TLB related environment variables at Environment / Pipeline / Stage / Job level. Setup the task to consume GO_PIPELINE_NAME , GO_STAGE_NAME , GO_PIPELINE_COUNTER , GO_STAGE_COUNTER , GO_JOB_RUN_INDEX & GO_JOB_RUN_COUNT environment variables that Go exposes. Upload junit xmls as test artifacts. Sample Pipeline Configuration <pipeline name= \"maven-project\" > <materials> <git url= \"https://github.com/test-load-balancer/sample_projects.git\" dest= \"sample_projects\" /> </materials> <stage name= \"unit-tests\" > <jobs> <job name= \"test-split\" runInstanceCount= \"3\" > <environmentvariables> <variable name= \"TLB_BASE_URL\" > <value> http://localhost:7019 </value> </variable> <variable name= \"TLB_TMP_DIR\" > <value> /tmp </value> </variable> <variable name= \"TLB_JOB_NAME\" > <value> ${GO_PIPELINE_NAME}-${GO_STAGE_NAME}-test-split </value> </variable> <variable name= \"TLB_JOB_VERSION\" > <value> ${GO_PIPELINE_COUNTER}-${GO_STAGE_COUNTER} </value> </variable> <variable name= \"TLB_PARTITION_NUMBER\" > <value> ${GO_JOB_RUN_INDEX} </value> </variable> <variable name= \"TLB_TOTAL_PARTITIONS\" > <value> ${GO_JOB_RUN_COUNT} </value> </variable> </environmentvariables> <tasks> <exec command= \"mvn\" workingdir= \"sample_projects/maven_junit\" > <arg> clean </arg> <arg> install </arg> <arg> -DskipTests </arg> <runif status= \"passed\" /> </exec> <exec command= \"mvn\" workingdir= \"sample_projects/maven_junit\" > <arg> clean </arg> <arg> test </arg> <arg> -DskipTests </arg> <arg> -Drun.tests.using.tlb=true </arg> <runif status= \"passed\" /> </exec> </tasks> <artifacts> <test src= \"sample_projects/maven_junit/target/reports/*.xml\" dest= \"test-reports\" /> </artifacts> </job> </jobs> </stage> </pipeline> Other features that helps with Test Parallelization Wait for all jobs to finish Go's modelling capability gives it the ability to run jobs in parallel but wait for all of them to finish before the next Stage / downstream Pipelines are triggered. Stop the downstream flow If any of the tests (and as a result the Job running the test) fails, the Stage is considered as failed. This causes the flow to stop as expected. Consolidated Test Report Once all the Jobs are done running, Go consolidates test reports & shows the result at stage level for easy consumption. Drill down You can drill down at job level to know more information like 'test count', 'console output' for the Job (test) etc. Partition re-run Go also provides ability to re-run a Job of a stage. This provides ability to run the partition that could have failed due to flaky test etc. The best part is, TLB runs the exact tests that it ran the last time making sure no test is missed out! TLB Correctness Check TLB provides an ability to check correctness, i.e. it will make sure all tests were run. You can configure to run this correctness check once all partitions are done executing, may be in next stage / pipeline. Power of dynamic splitting Go's one knob control to amount of parallelization means that when the number of tests increase/decrease all you will need to do is change the run instance count based on number of tests & resource availability & you are done! -- As always, Go questions can be asked at go-cd ."},{"title":"DUnitX has a Wizard!","tags":"ciandcd","url":"http://ciandcd.github.io/dunitx-has-a-wizard.html","text":"From: https://www.finalbuilder.com/resources/blogs/postid/702/dunitx-has-a-wizard Thanks to a contribution from Robert Love, DUnitX now sports a shiny new IDE Wizard for creating Test projects and Test Units. Before you install and use the wizard, there is one thing I recommend you do. In your Delphi IDE, add and Environment variable DUNITX and point it at your copy of the DUnitX source. The reason for doing this, is that when the wizard creates a project, it adds $(DUNITX) to the project search path. This avoids hard coding the DUnitX folder in yhe project search path, and also avoids installing it in your global library path (I have nothing other than the defaults installed there, I always use the project search path, makes it easier to share projects with other devs). Once you have that done (I'm assuming you have pulled down the latest source from GitHub), open the project group (.grouproj) for your IDE version and build the project group. Then right click on the wizard project and click on install : If the package installs successfully then we are ready to use the wizard. Close the project group, and invoke the File\\New\\Other dialog, you will see the DUnitX Project listed You might like to Customize your File\\New menu, I made DUnitX prominent on mine (in part to remind myself to create unit tests first!) : Invoking the wizard will show a simple dialog : The options are pretty self explainatory, so I won't go into them here. The wizard generates a console application, once we have a gui runner (being worked on) we'll update the wizard to add options for that. DUnitX is open source, get it from GitHub - contributions are welcome. We also have a Google Plus Community for DUnitX."},{"title":"Feature Branch Support","tags":"ciandcd","url":"http://ciandcd.github.io/feature-branch-support.html","text":"From: http://www.go.cd/2015/04/27/Feature-Branch-Support.html Go 15.1 introduced support for writing material repository plugins, to extend the kind of source code material repositories that Go works with. This resulted in community-driven plugins developed for Go, to implement support for feature branches, with help from members of Go's core contributors. This blog posts has information specifically about GitHub Pull Request support. Note: In this post, the terms Branch and Pull Request are used interchangeably, since a Pull Request is essentially just a branch. As codebases grow and teams start writing more tests, they often hit upon a challenging problem. If they have setup their build, test and deploy pipelines as a normal team or teams working with trunk-based development would have, then increasing the number of tests they have results in a longer time to certify a build and deploy to production. Here is an example of a Value Stream Map from Go CD (Username: view, Password: password) itself, where running all the tests and generating installers can take hours: Figure 1: GoCD - Value Stream Map (Click to enlarge) Due to this, it becomes critical to keep the main Value Stream green all the time. A failed build would mean all other commits ready to go in have to wait until the failed build is fixed: Figure 2: Failed build stops everything (Click to enlarge) The root of this problem is a slow build, and sometimes that can be tackled directly. However, with the advent of short-lived feature branches (aka, Pull Requests in GitHub land), this problem can become worse. Since feature branches are not regularly verified before merging, merging them could itself be a little risky, and could cause the build to fail un-necessarily. In general, development workflows in organizations has moved to something which looks like: Pull Request (GitHub, Gerrit etc.) / Feature Branch => Code Review => Merge => Build Now, whether a feature branch based workflow is the best approach or not is hotly debated (see Martin Fowler's article on this). Organizations who follow a feature branch based workflow have been wanting support for it in Go. Historically, Go has advocated against feature branches and support for it has been limited. Go users have come up with some innovative work arounds, like this one from Vision Critical . Though the Go core contribution team continues to be wary of long-lived feature branches, short-lived feature branches create a window for validating changes before they are merged into the main branch. Since the majority of time spent in a CI/CD setup tends to be in running tests, and failed builds are typically due to test failures, you could run tests on a proposed change in a feature branch, get feedback about it and fix tests if needed, before merging it into the trunk. Though this does not always catch integration issues (that depends on what else was merged before this one was), it allows you to increase the chances of your main Value Stream staying green and in a deployable state for longer. A problem with this approach though, is that every change will be tested twice (once on the feature branch and once on the main branch after the merge) which means the effective time for a commit to reach production could be more, unless you have more hardware (agents) to run branch builds. The way forward Assuming you have chosen the approach mentioned above, you can now use Go 15.1, with its two new extension points - SCM end-point and the Notification end-point , to test feature branches before they are merged. To use this with GitHub requires the use of two community-driven and community-supported plugins: Git Branch Poller Plugin and the Build Status Notification Plugin . The first one is an SCM Material plugin, and is responsible for polling a configured repository for changes, while the second one is a Notification plugin, which is responsible for notifying GitHub about the suitability of a Pull Request for merging. Note : Even though this post specifically mentions GitHub only, plugins have been written to work with plain Git, Atlassian Stash, Gerrit and more! See the Go community plugins page for more information. Quick Setup Download the Git Branch Poller Plugin and the Build Status Notification Plugin . Place them under <go-server>/plugins/external . Restart the Go Server. Verify that the plugins are loaded correctly. Figure 3: Verify Plugins (Click to enlarge) Decide which parts of the value stream you want the Pull Requests to run till, and extract a template for those pipelines, so that you can have a parallel set of pipelines to run against Pull Requests. The need to create a separate set of pipelines is to make sure that the main build and the branch build never get interleaved, and a branch build never gets deployed into production, by mistake. Your decision should be based on how much of your tests can reasonably be run for every Pull Request, and how far down the Value Stream can a build containing those changes Go. For some, every test in the system needs to run before it is deemed merge-able and for some, only unit and integration tests might be enough. It depends. Suppose you have a setup of three pipelines like this: Figure 4: Example setup (Click to enlarge) and you decide that you want the first two pipelines to run for every Pull Request, you need to change your pipelines to look like this: Figure 5: Extract templates, create pipelines for PR (Click to enlarge) Based on your decision, extract templates and create the new pipelines: Figure 6: Extract template (Click to enlarge) In the new pipeline or pipelines that have been setup to run for every Pull Request, change the Git material to use the GitHub material (this material is provided by the GitHub poller plugin installed earlier): Figure 7: Add GitHub material (Click to enlarge) Figure 8: Add GitHub material - Details (Click to enlarge) Once you have setup the GitHub material for the pipeline, you can remove the Git material from that pipeline. That's it. Results Go will trigger builds for every new Pull Request and for new commits to existing Pull Requests: Figure 9: PR triggers build (Click to enlarge) Go will update Pull Request in GitHub with the build status: Figure 10: GitHub PR page gets updated (Click to enlarge) Figure 11: GitHub PR listing page gets updated (Click to enlarge) Fan-in and Value Stream Map work as expected: Figure 12: Fan-in and VSM work (Click to enlarge) Shortcomings and known issues: If multiple branches are updated at once, the plugin provides all of them as changes and Go will not run the pipeline for every change separately. Go currently combines multiple changes into a single pipeline run (to save time). A feature allowing \"force trigger pipeline for each change\" should be able to overcome this. This has not yet been accepted into the main GoCD codebase. If there are multiple commits in a branch, the plugin only returns the top commit as a change. Hence only one change shows up in the dashboard, value stream, etc. Also, since Go does not know about the other changes you will not be able to manually trigger a pipeline with the other commits. The UI is lacking in certain areas: For instance, it is not possible to add an SCM plugin material during pipeline creation, to associate an existing SCM to a pipeline you will need to edit Config XML etc. These will be fixed in upcoming releases. References Some discussions on the GoCD mailing lists and on GitHub about this: Sample Configuration Here is a part of the configuration used to create the images shown above: <scms> <scm id= \"b7386c23-71d5-4581-8129-bba5b67638e4\" name= \"sample-repo\" > <pluginConfiguration id= \"github.pr\" version= \"1\" /> <configuration> <property> <key> url </key> <value> https://github.com/srinivasupadhya/sample-repo.git </value> </property> </configuration> </scm> </scms> <pipelines group= \"sample-group-master\" > <pipeline name= \"sample-pipeline-master\" template= \"sample-pipeline\" > <materials> <git url= \"https://github.com/srinivasupadhya/sample-repo.git\" dest= \"sample-repo\" materialName= \"sample-repo\" /> </materials> </pipeline> <pipeline name= \"sample-downstream-pipeline-master\" template= \"sample-downstream-pipeline\" > <materials> <pipeline pipelineName= \"sample-pipeline-master\" stageName= \"sample-stage-2\" /> </materials> </pipeline> </pipelines> <pipelines group= \"sample-group-PR\" > <pipeline name= \"sample-pipeline-PR\" template= \"sample-pipeline\" > <materials> <scm ref= \"b7386c23-71d5-4581-8129-bba5b67638e4\" dest= \"sample-repo\" /> </materials> </pipeline> <pipeline name= \"sample-downstream-pipeline-PR\" template= \"sample-downstream-pipeline\" > <materials> <pipeline pipelineName= \"sample-pipeline-PR\" stageName= \"sample-stage-2\" /> </materials> </pipeline> </pipelines> <templates> <pipeline name= \"sample-pipeline\" > <stage name= \"sample-stage-1\" > <jobs> <job name= \"sample-job-1\" > <tasks> <exec command= \"ls\" /> </tasks> </job> </jobs> </stage> <stage name= \"sample-stage-2\" > <jobs> <job name= \"sample-job-2\" > <tasks> <exec command= \"ls\" /> </tasks> </job> </jobs> </stage> </pipeline> <pipeline name= \"sample-downstream-pipeline\" > <stage name= \"sample-stage-3\" > <jobs> <job name= \"sample-job-3\" > <tasks> <exec command= \"ls\" /> </tasks> </job> </jobs> </stage> </pipeline> </templates> As always, Go questions can be asked on the mailing list ."},{"title":"Filtering Tests","tags":"ciandcd","url":"http://ciandcd.github.io/filtering-tests.html","text":"From: https://www.finalbuilder.com/resources/blogs/postid/717/dunitx-updated-filtering-tests Still evolving DUnitX is still quite young, and still evolving. One of the features most often requested (other than the gui runner, which is still planned) is the ability to select which tests to run. I found myself wishing for that feature recently. I never missed it while the number of my tests were relatively small and fast, but as time went by, it was taking longer and longer to debug tests. So, time to add filtering of fixtures and tests. The command options support in DUnitX was to be honest, quite useless and poorly though out. So my first task was to tackle how options were set/used in DUnitX, and find an extensible way of handling command line options. The result turned out better than I exepected, so I have published a separate project for that. VSoft.CommandLine is a very simple library for defining and parsing command line options, which decouples the definition and parsing from where the parsed values are stored. I'll blog about this library separately. I did try to avoid breaking any existing test projects out there. To invoke the command line option parsing, you will need to add a call to TDUnitX.CheckCommandLine; at the start of you project code, eg: begin try TDUnitX.CheckCommandLine; //Create the runner runner := TDUnitX.CreateRunner; The call should be inside the try/except because it will throw exceptions if any errors are found with the command line options. I modified the IDE Expert to include the needed changes in any new projects it creates, I recommend running the expert to generate a project and then compare it to your existing dpr. Filtering The next thing to look at was how to apply filtering. After much experimentation, I eventually settled on pretty much copying how NUnit does it. I ported the filter and CategoryExpression classes from NUnit, with a few minor mods needed to adapt them to our needs. The cool thing here is I was able to port the associated unit tests over with ease! There are two types of filters, namespace/fixture/test filters, and category filters. Namespace/Fixture/Test filtering The new command line options are : --run - specify which Fixtures or Tests to run, separate values with a comma, or specify the option multiple times eg: --run:DUnitX.Tests.TestFixture,DUnitX.Tests.DUnitCompatibility.TMyDUnitTest eg: If you specify a namespace (ie unit name or part of a unit name) then all fixtures and tests matching the namespace will run. Category Filters A new CategoryAttribute allows you to a apply categories to fixtures and/or tests. Tests inherit their fixture's categories, except when they have their own CategoryAttribute. You can specify multiple categories, separated by commas, eg: [TestFixture] [Category('longrunning,suspect')] TMyFixture = class public [Test] procedure Test1; [Test] [Category('fast')] procedure Test2; In the above example, Test1 would have \"longrunning\" and \"suspect\" categories, whilst Test2 would have just \"fast\". You can filter tests using these categories, using the --include and/or --exclude command line options. When both options are specifies, all the tests with the included categories are run, except for those with the excluded categories. The following info is copied from the NUnit doco (on which these options are based) : Expression Action A|B|C Selects tests having any of the categories A, B or C. A,B,C Selects tests having any of the categories A, B or C. A+B+C Selects only tests having all three of the categories assigned A+B|C Selects tests with both A and B OR with category C. A+B-C Selects tests with both A and B but not C. -A Selects tests not having category A assigned A+(B|C) Selects tests having both category A and either of B or C A+B,C Selects tests having both category A and either of B or C As shown by the last two examples, the comma operator is equivalent to | but has a higher precendence. Order of evaluation is as follows: Unary exclusion operator (-) High-precendence union operator (,) Intersection and set subtraction operators (+ and binary -) Low-precedence union operator (|) Note : Because the operator characters have special meaning, you should avoid creating a category that uses any of them in it's name. For example, the category \"db-tests\" could not be used on the command line, since it appears to means \"run category db, except for category tests.\" The same limitation applies to characters that have special meaning for the shell you are using. I have also fixed some other minor issues with the naming of repeated tests and test cases to allow them to work with the filter. Other options Once you have added the command line check, run yourexe /? to see the other command line options available. None of the options are required so running the exe without any options will behave as it did before. Delphi 2010 Resolved - Thanks to Stefan Glienke for figuring this out - D2010 now support again . This fix was to remove any use of of STRONGLINKTYPES. One thing of note: at the moment these changes break our D2010 support. I get a linker error when I build : [DCC Fatal Error] F2084 Internal Error: L1737 Interestingly, the resulting executable is produced and does seem to run ok, however it makes debugging tests impossible, and of course it would fail in automated build. I did spend several hours trying to resolve this error but got nowhere. Since my usage of DUnitX is currently focused on XE2, I'm willing to live with this and just use an older version of DUnitX for D2010. I have tested with XE2, XE5 and XE6."},{"title":"FinalBuilder 8 Beta","tags":"ciandcd","url":"http://ciandcd.github.io/finalbuilder-8-beta.html","text":"From: https://www.finalbuilder.com/resources/blogs/postid/729/finalbuilder-8-beta What's new in FinalBuilder 8 IDE Themes It's almost 5 years since FinalBuilder 7 was released. Since it's release we have shipped 44 official updates , nearly every update including new features or improvements. This program of continuous improvement has worked well, with customers not having to wait for major new versions to arrive to get support for new versions of Visual Studio or Delphi etc, but it has limited our ability to make major changes. So it's time for a new major version of FinalBuilder. The IDE has two new themes, Dark and Light (yes, imaginatively named!). The IDE defaults to Dark on first run, however you can change the theme in the options quite easily. The themes are still a work in progress, we are waiting on an update from a third party control vendor to resolve some issues. Debugger One of the most asked for features now available in FinalBuilder 8, stepping into included projects . In FinalBuilder 7 and earlier, you could only step over included projects, and wait for them to return. In FinalBuilder 8, you can step into the included project, if it is not already opened the IDE will open the project and switch to it automatically. To make this possible, there are now \"Step Into\" and \"Step Over\" functions. The Step into/over now also applies to targets (see below). Debugger breakpoints now have conditions : Actionlists renamed to Targets ActionLists have been renamed to Targets. Targets can now also define dependencies, so you can for example define Clean, Build, Test, and have Test depend on Build. If you execute the Test target, and Build has not already been executed, it will be executed first before Test. Targets can be specified on the command line. In FinalBuilder 7 and earlier, projects had a Main and an OnFailure (global error handler) actionlist. In FinalBuilder 8, projects just have a Default Target. Older projects will be imported such that the Main and OnFailure Targets are called from the Default Target inside a try/catch block. Run Target Action You can now return values from Targets (ie out parameters) . New Help System The help has moved online in the form of a wiki. This enables us to do inline help updates without needing to ship new builds. The new help is still being worked on, lots of screenshots are missing etc.. Non Visible Changes Stepping Engine The stepping engine was rewritten to enable stepping into included projects, and to enable target dependencies. This, work, together with the new variables architecture is where the bulk of effort/time was spent in the FinalBuilder 8 development cycle. Variables Architecture The variables architecture and the expression evaluator were rewritten to resolve several corner case issues that we were not able to resolve in FinalBuilder 7. The expression evaulator has a new parser that will allow us to more easily extend the syntax in the future. The User variable namespace was removed, it caused too many problems with projects not running under other users, not running on the build server etc. Use Project variables instead. Core Messaging Changes to the messaging has allowed us to improve the performance of the stepping engine and logging, with much less thread switching. This also improved the IDE performance. CLR Hosting The minimum CLR version is now .NET 4.0 (ie FinalBuilder requires .net 4.0 to be installed). Code Changes In addition to the architectural changes, we also spent a lot of time refactoring the code, running static analysis tools over the source, looking for memory leaks, potential bugs etc. One of the results of this is reduced memory usage during a build compared to FB7. The FB8 IDE does use slightly more memory than the FB7 IDE at startup (mostly due to the heavy use of delphi generics), however the runtime memory usage is much lower.A large part of the refactoring involved unit testing (we created a new unit test framework to suite our needs!) and creating a suite of integration tests. FBCmd The command line parameters have changed to be more consistent and easier to specify. You can also specify one or more targets to execute (when not specified, the default target is executed). New Project File Formats FinalBuilder has used an xml file format since version 1, however a common complaint over the years, has been that it is difficult to diff file versions. FinalBuilder 8 has tackled this in two ways. A new DSL style project file format (.fbp8) is now the default format, it is very easy to diff. project begin projectid = {04710B72-066E-46E7-84C7-C04A0D8BFE18} target begin name = Default targetid = {E6DE94D6-5484-45E9-965A-DB69885AA5E2} rootaction begin action.group begin id = {D860420B-DE46-4806-959F-8A92A0C86429} end end end end A new xml format (.fbx8), much less verbose than the old format. <?xml version=\"1.0\" encoding=\"UTF-8\"?> <finalbuilder> <project> <projectid>{6A717C24-D00F-4983-9FD0-148B2C609634}</projectid> <target> <name>Default</name> <targetid>{E6DE94D6-5484-45E9-965A-DB69885AA5E2}</targetid> <rootaction> <action.group> <id>{D860420B-DE46-4806-959F-8A92A0C86429}</id> </action.group> </rootaction> </target> </project> </finalbuilder> Compressed project files (.fbz8) use the dsl format internally (compressed projects are just a zip file with a project.fbp8 inside it). The default project file encoding is now UTF-8, which is more version control friendly (some version control systems treat utf-16 as binaries). New Actions There are no new actions at the moment, although several are in development, they will be added to the beta builds as they are completed. How do I get the Beta? Links to the beta downloads will be published to the What if I find a bug? We have created a We are particularly keen for people to load up their existing projects from older (ie 7 or earlier) versions of FinalBuilder, save them in FB8 format, and load them again and confirm that everything loaded ok. When will it be released? When it's ready ;) Links to the beta downloads will be published to the FinalBuilder Downloads page.We have created a Beta forum on our forums, or you can email support (please added Beta to the subject). When reporting an issue, be sure to include the beta build number and details about your environment. Please test with the latest beta build before reporting bugs.We are particularly keen for people to load up their existing projects from older (ie 7 or earlier) versions of FinalBuilder, save them in FB8 format, and load them again and confirm that everything loaded ok.When it's ready ;)"},{"title":"FinalBuilder and Team Foundation Server 2013","tags":"ciandcd","url":"http://ciandcd.github.io/finalbuilder-and-team-foundation-server-2013.html","text":"From: https://www.finalbuilder.com/resources/blogs/postid/705/finalbuilder-and-team-foundation-server-2013 FinalBuilder offers you tight integration into TFS with an easy to understand IDE. In this post I will go into how to integrate FinalBuilder into your TFS build process. The post will cover; To make sure this post doesn't rival \"war and peace\" in size, I will assume a few things during the post. Namely that you have used TFS in some fashion, and/or have gotten a solution building under TFS with the default TFS template. For more information on getting up and running with TFS and common issues with TFS I suggest taking a look at the TFS ranger books and blog . Prerequisites To follow along with this post you will need the following installed: Team Foundation Server 2013, with build agent (12.0.21005.1 or later) FinalBuilder 7 (7.0.0.2745 or later) Required Visual Studio or MSBuild version available on agent Agents for Microsoft Visual Studio 2013 Team Explorer 2013 The Team Foundation Server can be of any configuration as long as it has at least one agent and a reporting service. FinalBuilder 7 is required on the agent machine, and could also be installed on the developer's machine for editing of scripts. Agents for Microsoft Visual Studio 2013 will provide access to VSTest.Console which we require for testing on the build agent. Lastly, Team Explorer is required for interaction with source control from FinalBuilder's IDE. Without this installed source control will need to be done manually. To start I have a solution under source control on a TFS 2013 collection. The solution I will be using is under a team project called VSoftSFTPLibrary under the collection \"TFS2013\\DefaultCollection\". The layouts of the team project looks like the following: I have a directory for the SFTP projects source, library files and its tests. The layout of your source control may vary from mine, however the main point to note here is that separating out directories isn't an issue. With the solution, all related files, and binaries under source control I now add a build definition to the team project. Next the folders to be used in the FinalBuilder script need to be mapped from the server to an agent location. It's important to map all folders which contain files required for the build process. At this stage we only really know of the Source directory we are going to be build and the Library folder which the source relies on. Note that if your solution relies on a certain structure of folders this structure should be the same when it arrives on the agent. In the example the source and library paths are at the same folder depth, therefore this is reflected on the agent side. When the build is finished usually we want them \"dropped\" somewhere on the network. A FinalBuilder script can use this drop location, which we will go into later on in the artcile. For now I set this to a server location accessible by the build agent service user. By default the build process is set to the default build process which comes with TFS2013. The template is called \"TfvcTemplate.12.xaml\". When used the default template will enable the building, testing, impact testing, and deployment of a solution and its projects. Our aim is to perform all the same build activities with the additon of a FinalBuilder project. To this end, the build process template needs to be changed over to one of the two supplied with FinalBuilder. In the [%ProgramFiles(x86)%\\FinalBuilder 7\\TFS Templates\\2013] folder there are two TFS 2013 build workflow templates. \"FinalBuilderTFS2013Build.xaml\" performs all the same steps as the default build workflow and adds a FinalBuilder step just after the MSBuild activity and just before the optional \"after MSBuild script\". This script is typically used by those looking to convert current default script builds to FinalBuilder in a simple and piece-meal fashion. The \"FinalBuilderOnlyTFS2013Build.xaml\" template is used when the FinalBuilder script is to take over the entire build process. For the moment I will use the \"FinalBuilderTFS2013Build.xaml\" script. Add these two scripts to the team project, typically under a BuildTemplates folder to separate them from the source and library code. Copy these templates into the local mapped location for this team project, and check them into source control. Now the \"FinalBuilderTFS2013Build.xaml\" template can be selected as the build process template. To do this add a new new build in the process section of the build definition. Navigate to the source control location where the template is stored and select it. The next step is to fill in the parameters for the build process. The majority of these are the exact same as the default TFS 2013 template with the addition of some FinalBuilder specific settings. The FinalBuilder section of settings allow for the specification of the project file, and custom arguments for the FinalBuilder project. Note that the \"2. Build | Projects\", and the \"6. FinalBuilder | Project File\" are both required by the build template. These are used to determine what should be built and what FinalBuilder project should be run. To run a FinalBuilder project we need to create one. Therefore create a FinalBuilder project with just an [Action Group] action for the moment. Save this project to the team projects FinalBuilderScripts folder and check it into source control. Once the FinalBuilder project is in source control select it in the build process using the file selector provided through the ellipse selector. You should end up with something reading like this \"$/VSoftSFTPLibrary/FinalBuilderScripts/BuildVSoftSFTPLibrary.fbp7\" Now that we have included the FinalBuilder project into our build process we need to make sure its folder is mapped to the agent. Open the source settings section, and add a mapping for the folder in which the FinalBuilder project resides. Something like the following should be what results. Queue the build and the log should read like the following excerpt. The [Action Group] line is the action group you added in the FinalBuilder project. So now we have a TFS build process which is able to call a FinalBuilder script. In the next section we will delve more into how to extract information from TFS about the build inside our FinalBuilder script. Retrieving information from TFS in a FinalBuilder Script For the FinalBuilder script to be of use in the TFS build process, it requires information about the TFS build currently running. To provide this we offer a number of actions that can extract this information during the TFS build run. To get you started, there is an example project in <FinalBuilderInstallDir>\\TFS Templates called TFSExample.fbp7. This sample project contains examples of the actions to use during a TFS build process. The [Get Team Foundataion Build Parameters] action is a special action that is only useful when a project is launched from TFS. It assigns TFS data to the specified project variables. Each of the variables in the [Get Team Foundation Build Parameters] action are as follows: Team Server URL: This is the URL of the team foundation server that queued the build. Team Project: Is the name of the team project the build belongs to. Build Id: Is the unique number allocated to this build. Platform/Flavor: These are the build parameters for the compilation of the solution to be built. Default Solution File: The solution file listed as the primary for the team project. Solution File List: The list of solutions to be built by the build process. Deployment Folder: The drop folder configured for the build process. It is blank if it is not set. Source Root: The first listed solutions root folder. Working Directory: A working directory on the agent for the current build definition. Typically space which is shared between builds made on the same agent. On the Custom Arguments tab is a list of ten variables which can be passed from the TFS build process to FinalBuilder. These are passed as plain text and converted to the variable types used to read them in the FinalBuilder script. For my script I only wanted to get the Team Foundation Build Parameters, and the variables that it used. So first I checked out the \"BuildVSoftSFTPLibrary.fbp7\" project using the built in source control features of FinalBuilder. First I need to make sure that it is indeed added to source control. If it hasn't detected that it is part of the TFS source control at this stage I add it to source control. I make sure to select the \"Microsoft Team Foundation Server MSSCCI provider\" which will use the Team Explorer 2013 installed on the machine. I select the TFS 2013 server and collection I am working with, also making sure the team project is correct. Once this is done I am then able to use the file menu to check the project out ready for editing. Next I copy all the variables I want from the TFSExample project, and paste them into my BuildVSoftSFTPLibrary.fbp7 project. To paste the variables I open the Variables Editor, right click and select paste. Last I copy over the [Get Team Foundation Build Parameters] and [Trigger Files Iterator] actions. These use the variables we just copied over and will hook themselves up as they appeared in the example project. Now we can check in these changes and queue the build. In the log for the build you will see the following: By default the [Get Team Foundation Build Parameters] action will write what it has retrieved from TFS to the build log. Something to keep in mind when debugging a FinalBuilder script in TFS. So now we want to make the FinalBuilder process take over the build completely. The first thing to do here is to stop the TFS build process from building, testing, and publishing results of the build. This requires changing the build workflow to remove the activities which do this (otherwise we would be doing things twice), and updating the FinalBuilder script to perform these tasks. Instead of working out which activities to remove from the build template we provide a build template with all the build and testing activities removed. The \"FinalBuilderOnlyTFS2013Build.xaml\" template which was copied into the BuildTemplates folder is this template. In the process section of the build definition, create a new build process using the \"FinalBuilderOnlyTFS2013Build.xaml\" file. You will notice that nearly everything in the build parameters is the same except now the before and after script events have been removed. Also the testing section is no longer present. All of this will be handled by the FinalBuilder script. Once again open your FinalBuilder project for this build and check it out. Also open up the TFSExample.fbp7 project and take a look at the [Build VS.Net Solution] action. Copy this action to the project used to build your solution. The [Build VS.NET Solution] action builds a Visual Studio.NET solution. On the [Solution] tab you will see that the Solution File is set to \"%SourceRoot%\\<YourSolution>.sln\". Replace <YourSolution> with the name of the solution that you wish to build. Note that \"%SourceRoot%\" will be the directory of only the first solution in the list of projects to build. In my project I ended up with a Solution File value of \"%SourceRoot%\\VSoftSFTPLibrary.sln\" for the [Build VS.NET Solution] action. On the [Paths] tab you will see that the Output Directory is set to %SourceRoot%\\Binaries. You may change this if you wish, but it's it not necessary. Note because the drop location may be on a different server to the build agent, it is important that VSTest runs on files located on the build agent machine. Unless you explicitly set up the trust relationship, .NET will not allow executing of assemblies on remote machines. This is why we build and test in a directory under %SourcesRoot% and then move the files to the drop location after testing. This will be covered more in the next section. On the agent the TFS agent the FinalBuilder options for Visual Studio will need to be set. If not and DEVENV.COM is required, the build will fail about the build tool location being unknown. Now are right to run the build with using just FinalBuilder. Queue the build again and now the project will build, not from a template activity but from the FinalBuilder script it is running. To perform testing we need to add a [Run VSTest.Console] action to the FinalBuilder project. The [Run VSTest.Console] uses VSTest.Console to run your test assemblies. On the [Settings] tab add the name of your test assembly to the list. You should end up with something along the lines of \"%SourceRoot%\\Binaries\\<YourTestAssembly>.dll\". On the [Publish Results] tab the action should be set to automatically publish the results to the TFS server. This means that after the tests are run they will automatically be stored with the build on the TFS Server. Note that the [Ignore Failure] option, located on the [Options] tab, is important. If any unit tests fail, the [Run VSTest.Console] action will fail, and setting [Ignore Failure] allows the FinalBuilder and TFS builds to continue. Un-check [Ignore Failure] if you would prefer the build to stop on failed tests. In either case, test failures will appear in the TFS build log and in TFS reports. On the agent the TFS agent the FinalBuilder options for VSTest.Console will need to be set. If not, the build will fail with an error about the VSTest.Console location being unknown. The last step to complete the process is to move all the files from the agent to your drop location. The simplest way to achieve this is by a [Move File(s)] action. We already have the drop folder location stored in the %DropShare% variable. It is this value which we then use in the [Move File(s)] action. Once this is run we will have a built solution, with tests run, and the binaries copied into the specified drop folder."},{"title":"For Go 15.1 upgrade your Java","tags":"ciandcd","url":"http://ciandcd.github.io/for-go-151-upgrade-your-java.html","text":"From: http://www.go.cd/2015/04/23/Go_15_1_jdk7_announcement.html GoCD has stopped support JDK 6 for some time now. But we understand that some users were using Java 6, so we continued to support it as long as we could while helping users migrate their Go servers and agents to Java 7. Java 6 was declared end-of-life in February 2013, and Java 7 is scheduled to be declared end-of-life soon. Starting with the 15.1 release of GoCD, it will only run with Java 7. Users are encouraged to upgrade to the latest release of GoCD with Java 8. Starting with the next release, we plan on providing support for Java 8."},{"title":"Get Started Using Go","tags":"ciandcd","url":"http://ciandcd.github.io/get-started-using-go.html","text":"From: http://www.go.cd/2015/05/06/Getting-Started-Resources.html Some resources to help get started using Go. Go User Documentation There are a couple sections of the user documentation that can be especially helpful to people new to Go. Concepts in Go - This covers some of the basic concepts used in Go. A good understanding of what Pipelines, Stages, Jobs and Tasks are will be very helpful. Managing Agents - The Go server produces the user interface for Go, but it doesn't actually run your jobs. Learn how to use Go Agents to \"do the work\". Setting up a new Pipeline See how to set up your first pipeline Of course there's a lot more information available as well. Other information online The Go mailing list - A great place to search for answers to questions you may have, or of course ask them if they aren't already covered. IRC - Connect to freenode with your own IRC client or use this web client. Don't forget to uncheck \"auth to services\" if you're not planning to login with your preset freenode information. Live demonstrations Webinars - ThoughtWorks presents live webinars every couple weeks so that you can see Go in action. There are also recordings of previous webinars on this blog. Professional Support ThoughtWorks - The first 30 days of professional support provided by ThoughtWorks is free. You'll get access to a global support team, and tough issues can be escalated directly to the Go development team. Alternative Trial Installation Docker Container - This is an easy way to see what Go does. As it says in the description, this is not a production container. You'll also need at least one instance of the agent container"},{"title":"Go 14.3 Released","tags":"ciandcd","url":"http://ciandcd.github.io/go-143-released.html","text":"From: http://www.go.cd/2014/11/11/Go_14_3_announced.html Today we released Go 14.3 You can download it from here . Take a look at release notes to see details. This release saw lot of contributions from the community. A huge callout to the following contributors (not in any particular order) for their outstanding contributions : @lcs777 , @ciotlosm , @tusharm , @juniorz , @RikTyer , @mmb , @afoster , @sahilm , @gregoriomelo , @greenmoss , @dvarchev and Temmert (We have tried to be as accurate as possible. Sincere apologies if we missed mentioning anyone above) We would also like to thank people who reported issues/feature requests and participated in various discussions. That list is too big to be mentioned here, but please know that all the time and energy spent by everyone in improvising Go is very much appreciated. Thanks once again!"},{"title":"Go 14.4 Released","tags":"ciandcd","url":"http://ciandcd.github.io/go-144-released.html","text":"From: http://www.go.cd/2014/12/17/Go_14_4_announced.html Today we released Go 14.4 You can download it from here . Take a look at release notes to see details. Sincere thanks to everyone who contributed to Go in form of features, ideas, issues / feature requests and much more! A special mention goes to @mythgarr , @hammerdr and to the Pivotal team: @mmb , @gajwani , @fkotsian , @bsnchan for their active contributions and support. Thanks once again!"},{"title":"Go 15.1.0 Released","tags":"ciandcd","url":"http://ciandcd.github.io/go-1510-released.html","text":"From: http://www.go.cd/2015/04/29/Go_15_1_announced.html We would like to announce a new release of gocd. Head over to our downloads page to get your hands on the latest and greatest. Read more about what's new in this release from our release notes . Sincere thanks to everyone who contributed to Go in form of features, ideas, issues / feature requests and much more! A special mention goes to @ashwanthkumar , @alexschwartz , @sachinsudheendra , @pwen , @pamo , @bernardn , @danielsomerfield , @iliasbartolini for their active contributions and support. Thanks once again!"},{"title":"Go Plugin Competition","tags":"ciandcd","url":"http://ciandcd.github.io/go-plugin-competition.html","text":"From: http://www.go.cd/2015/01/20/Go_plugin_competition.html Are you up for the challenge? Do you have what it takes to build an awesome Go plugin? Here's your chance to put your development skills to the test. ThoughtWorks invites you to the first ever Go plugin challenge. We want you to build a plugin that showcases the best of Go. Have you been playing with an idea on the side or has your organisation developed something really cool that others would love? This is your chance to showcase it and win a prize! Read more... Important Dates Submission Deadline: February 20, 2015 at 11:59 PM CST (February 21, 2015 at 5:59 AM GMT) Notification of Acceptance: February 27, 2015 Results: March 10, 2015"},{"title":"Hardly Anyone Knows Continuous Delivery","tags":"ciandcd","url":"http://ciandcd.github.io/hardly-anyone-knows-continuous-delivery.html","text":"From: http://www.go.cd/2015/06/23/hardly-anyone-knows-cd.html Those of us who work in or around teams doing continuous delivery often think it's a mainstream thing. This couldn't be further from the truth. I work for ThoughtWorks, a company that implements processes and technologies we think are good long before most. We built the first CI server with Cruise Control, and Go was the first purpose built Continuous Delivery server. I go to a lot of conferences and events, read a lot of blogs, talk to a lot of peers, work with a lot of partners, etc. I thought most people involved with the creation of software had a pretty good idea what CD is. I was wrong. I just got back from a pretty big software conference that was a bit off my normal track. They had a few DevOps sessions this year, but historically this particular conference has been more about agile methodologies. As one of the sponsors, I spent a lot of time at the booth talking to people. The conversations in a trade show booth generally start with the visitor asking what we do (gotta work more on that so they don't have to) and me telling them that Go is a continuous delivery server. From there we go on to talk about what makes Go unique and why they should use it. At this show, when I told people Go is a continuous delivery server I was met with mostly blank stares. This was a conference attended by 100% people who create software for a living. The people attending care enough about their craft to spend (or get their company to spend) a couple thousand US dollars to come. But they had no idea what Continuous Delivery really is. I probably should note, this isn't meant as a knock on that conference at all. The lack of knowledge is a really bad thing. Not just for the Go CD project, but for software in general. The world runs on software. Too much of that software is bad. The practices around Continuous Delivery could make some of it better, or kill it before it gets out. So what can we do? Buy or borrow a copy of Continuous Delivery by Jez Humble and Dave Farley for your office. Make everyone read at least the chapters that apply to them. Yes, Jez and Dave both worked for ThoughtWorks when they were writing the book. Yes, Jez was the product owner of Go before the book came out. No, we won't make any money off the link if you buy it. I promise this isn't bias, it's the definitive work on the subject. Get The Phoenix Project by Gene Kim. It's a fictional novel and a bit corny at times, but people will learn a bit even if they don't mean to. Send people that don't know about Continuous Delivery to conferences that are specific to CD and DevOps. My favorite is DevOpsDays . You don't need huge, expensive conferences where you'll have to get finance approval to attend. The next one I'm going to is 200 bucks. If they don't have one in your area create one or find someone that will. (FYI, if anyone in Seattle is interested in doing that let me know) Take a friend who's CD impaired to a DevOps Meetup . As I'm writing this there are groups in 404 cities worldwide at that link alone. Trying to get your meetup going and struggling for content and/or speakers? Tell me, I know a few people and might be able to help. Stop assuming everyone knows what we're talking about when we talk about CD. Many of them are smiling and nodding the same way I do when my mother talks about her flowers. Feel free to comment with your own resource, this isn't even close to a definite list. One last thing... Stop telling people that the phrases DevOps and Continuous Delivery are overused. They aren't. Hardly anyone knows what Continuous Delivery is."},{"title":"Help Us Improve JetBrains.com and Win a License","tags":"ciandcd","url":"http://ciandcd.github.io/help-us-improve-jetbrainscom-and-win-a-license.html","text":"From: http://blog.jetbrains.com/blog/2015/06/01/help-us-improve-jetbrains-com-and-win-a-license/ It has been a while since the last time that we updated our web site design, nearly three years ago when we switched to the current design from the one below. We're thinking about making another update some time soon, but as a team of very technical geeks we love numbers. We do have lots of data already from the different analytics systems we're using but we want to do a special survey right now dedicated specifically for JetBrains.com. It is people like you who are visiting the web site and using it to find the information that you need, so we are asking for your help. As the survey is about a web site, it might feel odd that we're asking some questions which might not seem relevant. However, often decisions we make are somewhat related to other aspects in our lives. We're catering the site to so many diverse individuals and some of the questions play an role in this. Having said that, some questions are optional. If you are willing to help us, please complete our survey . And yes, we have some prizes for those who complete the survey: a chance to win one of 10 personal licenses for a JetBrains product of your choice, or one of 20 Amazon vouchers worth $25 . Thank you!"},{"title":"How to Fix your system path after installing Delphi","tags":"ciandcd","url":"http://ciandcd.github.io/how-to-fix-your-system-path-after-installing-delphi.html","text":"From: https://www.finalbuilder.com/resources/blogs/postid/722/how-to-fix-your-system-path-after-installing-delphi Update : Still the same situation with XE8. The Windows Path environment variable has a limit of 1023 * 2,048 characters, a stupidly short limit in this day and age, and when this limit is exceeded the path is truncated. Why this limit still exists on windows I have no idea.. for that matter why it ever existed... anyway, we're stuck with it (along with it's best buddy, MAX_PATH). Each version of Delphi adds over 200 characters to your system path. Worst still, they add those 200+ characters to the front of the path, not the end. What happens, is that eventually, important entries get truncated off end of the path, and strange things happen . You will find programs will not run, the task bar displays the wrong icons for programs, even getting to the control panel can be problematic. If you look at the entries that XE7 added to the start of your path you will see something like this : C:\\Program Files (x86)\\Embarcadero\\Studio\\15.0\\bin; C:\\Program Files (x86)\\Embarcadero\\Studio\\15.0\\bin64; C:\\Users\\Public\\Documents\\Embarcadero\\Studio\\15.0\\Bpl; C:\\Users\\Public\\Documents\\Embarcadero\\Studio\\15.0\\Bpl\\Win64; Fortunately this can be shortened by the use of junction points. Sadly, this will polute your C: drive with new folders but it's better than the alternative. The trick is to create links for the most common paths, so on mine I created these For XE5 and earlier : mklink /j RS C:\\Program Files (x86)\\Embarcadero\\RAD Studio mklink /j rspub C:\\Users\\Public\\Documents\\RAD Studio XE6 and above mklink /j Studio C:\\Program Files (x86)\\Embarcadero\\Studio mklink /j spub C:\\Users\\Public\\Documents\\Embarcadero\\Studio Once you have those junction points, you can then edit your path and replace the long paths, for example (for XE7) : C:\\Program Files (x86)\\Embarcadero\\Studio\\15.0\\bin -> C:\\Studio\\15.0\\bin C:\\Program Files (x86)\\Embarcadero\\Studio\\15.0\\bin64 -> C:\\Studio\\15.0\\bin64 C:\\Users\\Public\\Documents\\Embarcadero\\Studio\\15.0\\Bpl -> C:\\spub\\15.0\\Bpl C:\\Users\\Public\\Documents\\Embarcadero\\Studio\\15.0\\Bpl\\Win64 - > c:\\spub\\15.0\\Bpl\\Win64 So for XE7, that cuts it down from 218 to 80 characters, if like me you also have multiple versions of Rad Studio installed, this can be a big saving. As for Rad Studio, it's extremely rude to add things to the start of the path, the truncation it causes can ruin a machine.. I wasted several hours again today after installing XE7. Embarcadero were told about this issue many times over several releases.. according to the PLEASE ADD IT TO THE END!! and save us all a bunch of time. * Correction, max path length is 2048 - very difficult to find a difinitive source of this information on the microsoft site - the max size of an env variable is 2048, however I have seen the path variable truncated at 1024 many times. Each version of Delphi adds over 200 characters to your system path. Worst still, they add those 200+ characters to the front of the path, not the end. What happens, is that eventually, important entries get truncated off end of the path, and. You will find programs will not run, the task bar displays the wrong icons for programs, even getting to the control panel can be problematic.If you look at the entries that XE7 added to the start of your path you will see something like this :C:\\Program Files (x86)\\Embarcadero\\Studio\\15.0\\bin;C:\\Users\\Public\\Documents\\Embarcadero\\Studio\\15.0\\Bpl;C:\\Users\\Public\\Documents\\Embarcadero\\Studio\\15.0\\Bpl\\Win64;Fortunately this can be shortened by the use of junction points. Sadly, this will polute your C: drive with new folders but it's better than the alternative.The trick is to create links for the most common paths, so on mine I created theseFor XE5 and earlier :mklink /j RS C:\\Program Files (x86)\\Embarcadero\\RAD Studiomklink /j rspub C:\\Users\\Public\\Documents\\RAD StudioXE6 and abovemklink /j Studio C:\\Program Files (x86)\\Embarcadero\\StudioOnce you have those junction points, you can then edit your path and replace the long paths, for example (for XE7) :So for XE7, that cuts it down from 218 to 80 characters, if like me you also have multiple versions of Rad Studio installed, this can be a big saving.As for Rad Studio, it's extremely rude to add things to the start of the path, the truncation it causes can ruin a machine.. I wasted several hours again today after installing XE7. Embarcadero were told about this issue many times over several releases.. according to the doco , XE7 will popup a message about this.. I didn't see it so not sure when that is supposed to appear, but in any event, if your installer or progam needs to add something to the path environment variable,and save us all a bunch of time.Correction, max path length is 2048 - very difficult to find a difinitive source of this information on the microsoft site - the max size of an env variable is 2048, however I have seen the path variable truncated at 1024 many times."},{"title":"Integrating DUnitX Unit Testing with Continua CI","tags":"ciandcd","url":"http://ciandcd.github.io/integrating-dunitx-unit-testing-with-continua-ci.html","text":"From: https://www.finalbuilder.com/resources/blogs/postid/699/integrating-dunitx-unit-testing-with-continua-ci Continua CI includes support for running and reporting on Unit Tests, in this post we will take a look at running DUnitX Unit Tests. If you not familiar with DUnitX , it's a newish Delphi Unit Test framework, I blogged about it recently . I'm not going to cover how to get up and running in Continua CI, but rather I'll focus on the Unit Testing support. If you are not familiar with Continua CI, take a look at this recent post which describes how to build Delphi projects with Continua CI. Assuming you have you Continua CI Configuration all set up, lets take a quick look at how to integrate our unit tests into the build process. You need to build a console application, and make use of the xml logger. This is how the dpr of our typical DUnitX console test application might look : var runner : ITestRunner; results : IRunResults; logger : ITestLogger; xmlLogger : ITestLogger; begin try //Create the runner runner := TDUnitX.CreateRunner; //add a console logger, pass in true to specify quiet mode //as we don't need detailed console output. logger := TDUnitXConsoleLogger.Create(true); runner.AddLogger(logger); //add an nunit xml loggeer xmlLogger := TDUnitXXMLNUnitFileLogger.Create; runner.AddLogger(nunitLogger); //Run tests results := runner.Execute; {$IFDEF CI} //Let the CI Server know that something failed. if not results.AllPassed then System.ExitCode := 1; {$ELSE} //We don;t want this happening when running under CI. System.Write('Done.. press key to quit.'); System.Readln; {$ENDIF} except on E: Exception do begin System.Writeln(E.ClassName, ': ', E.Message); System.ExitCode := 2; end; end; end. The key thing to remember, is that this application is going to be running unattended, so never use ReadLn or any sort of interaction/prompting for input etc. If I had a dollar for every \"Finalbuilder/Continua CI hangs during my build\" bug report in the last 13 years, I'd be a... well not rich, but a few hundred dollars better off! Notice I used $IFDEF CI above to set the exit code to 1 if not all tests pass. So the next thing we need to is actually get the console application building in Continua CI. I covered building delphi applications with Continua CI in earlier post, so I'll just highlight a few things specific items that we need. Firsly, if you don't have the DUnitX source code in your repository, and have configured a repositoroy for it in Continua CI, then you need to update the Search Path for your console application. If you are using MSBuild to build the console app, then it's done on the Properties tab of the MSBuild Action : I have DUnitX in a Continau CI repository named DUnitX, and I've used the default path to the repository in the workspace ( $Source.DUnitX$ translates to \"/Source/DUnitX\" in the build workspace). If you are using FinalBuilder, you need to pass that to a FinalBuilder variable in the FinalBuilder Action, I'll cover that in more detail in a future post. Notice I also set the CI define I used in my code. The other important setting, is the ExeOutput path, which much be somewhere inside the build's workspace, so I set it to $Workspace$\\Output - Continua CI will translate $Workspace$ at build time to be the workspace folder for the build (each Continua CI build gets a clean unique workspace folder). Now it's time to add our DUnitX action, somewhere in the stage workflow after we have built the test application. Setting up the DUnitX action is quite simple : We set the Test Executable to our test console app, which in the MSBuild action we configured to be output to $Workspace$\\Output - and we specify where to put the xml file that DUnitX will generate (because we added the NUnit logger). The other two options control whether the to fail the Action/Build if any tests fail or error. If you have more than one Unit Test action to run in your build process, then it's bests to leave these unchecked and use the Stage Gate feature (on the Stage Options dialog) to fail the build (more on this later). After running the build, the Unit Test results appear in a two places, firstly the Build Details page, which shows the totals for the build (for all unit tests run) : You can drill into the tests by clickong on the numbers, or by clicking on the Unit Tests tab : This page allows you to filter by status (click on the numbers), and filter by Fixture, Namespacve etc. The first time a test fails, it will show up inder the New Failures category and under Failures. Clicking on a Failed or Errored test expands the row to show the error message logged by the test framework : Notice the Shelve button next to each test. If you have a test that always fails, and you don't want it to cause the build to fail, shelving the tests tells Continua CI to ignore those failures in the future, until you unshelve them. One last feature which I touched on, is the use of Stage Gates to fails the build. Continua CI collects a bunch of metrics during the build, and we can use those metrics in Stage Gate Conditions to fail the build if for example, there are any failing tests. Each Stage has it's own Gate Conditions, which are evaluated once the stage completes.. When the build fails at the Stage Gate, you will see the output of the condition expressions (note, the expressions below are the defaults on all stages) :"},{"title":"IntelliJ IDEA and WebStorm: InfoWorld's 2015 Technology of the Year Award Winners","tags":"ciandcd","url":"http://ciandcd.github.io/intellij-idea-and-webstorm-infoworlds-2015-technology-of-the-year-award-winners.html","text":"From: http://blog.jetbrains.com/blog/2015/01/30/intellij-idea-and-webstorm-infoworlds-2015-technology-of-the-year-award-winners/ On January 26th, 2015, InfoWorld announced their 2015 Technology of the Year award recipients . In total there were 32 winners representing the best of cloud, data, hardware and software applications. For the second year in a row WebStorm is a winner and IntelliJ IDEA returns to the list in 2015 ! WebStorm The WebStorm review by Martin Heller, InfoWorld Test Center Editor, highlights the core features that makes WebStorm \"more than an editor\" such as: built-in code inspections and code quality tools, Node.js and JavaScript debugger and tracer, Live edit, and integration with the testing tools. IntelliJ IDEA Just last month IntelliJ IDEA 14 picked up the 2015 Jolt Productivity Award for Coding Tools and now InfoWorld's 2015 Technology of the Year. What a great ending to 2014 and start to the new year! Here is part of what Rick Grehan had to say in his review. \"Granted, we wish the Community edition were equipped with the sorts of J2EE development tools found only in the Ultimate edition: database tools, support for frameworks such as JPA and Hibernate, deployment tools for application servers like JBoss AS, WildFly, and Tomcat. Nevertheless, the Community edition makes a fine Java application development platform that also gives you Android tools, as well as support for other JVM languages like Groovy, Clojure, and Scala (the last two via free plug-ins). Whichever version of IntelliJ IDEA you use, you'll find a rich array of tools designed to simplify otherwise tedious development chores.\" Read more about WebStorm (slide 15), IntelliJ IDEA (slide 16) and the other winners in InfoWorld's 2015 Technology of the Year Award slide show ."},{"title":"Introducing VSoft.CommandLineParser for Delphi","tags":"ciandcd","url":"http://ciandcd.github.io/introducing-vsoftcommandlineparser-for-delphi.html","text":"From: https://www.finalbuilder.com/resources/blogs/postid/719/introducing-vsoftcommandline-for-delphi Command line parsing Pretty much every delphi console application I have ever written or worked on had command line options, and every one of the projects tried different ways for defining and parsing the supplied options. Whilst working on DUnitX recently, I needed to add some command line options, and wanted to find a nice way to add them and make it easy to add more in the future. The result is VSoft.CommandLineParser (copies of which are included with the latest DUnitX). Defining Options One of the things I really wanted, was to have the parsing totally decoupled from definition and the storage of the options values. Options are defined by registering them with the TOptionsRegistry, via TOptionsRegistry.RegisterOption<T> - whilst it makes use of generics, only certain types can be used, the types are checked at runtime, as generic constraints are not flexible enough to specify which types we allow at compile time. Valid types are string, integer, boolean, enums & sets and floating point numbers. Calling RegisterOption will return a definition object which implements IOptionDefinition. This definition object allows you to set various settings (such as Required). When registering the option, you specify the long option name, the short option name, help text (will be used when showing the usage) and a TProc<T> anonymous method that will take the parsed value as a parameter. procedure ConfigureOptions; var option : IOptionDefintion; begin option := TOptionsRegistry.RegisterOption<string>('inputfile','i','The file to be processed', procedure(value : string) begin TSampleOptions.InputFile := value; end); option.Required := true; option := TOptionsRegistry.RegisterOption<string>('outputfile','o','The processed output file', procedure(value : string) begin TSampleOptions.OutputFile := value; end); option.Required := true; option := TOptionsRegistry.RegisterOption<boolean>('mangle','m','Mangle the file!', procedure(value : boolean) begin TSampleOptions.MangleFile := value; end); option.HasValue := False; option := TOptionsRegistry.RegisterOption<boolean>('options','','Options file',nil); option.IsOptionFile := true; end; For options that are boolean in nature, ie they have do not value part, the value passed to the anonymous method will be true if the option was specified, otherwise the anonymous method will not be called. The 'mangle' option in the above example shows this scenario. You can also specify that an option is a File, by setting the IsOptionFile property on the option definition. This tells the parser the value will be a file, which contains other options to be parsed (in the same format as the command line). This is useful for working around windows command line length limitations. Currently the parser will accept -option:value --option:value /option:value Note the : delimiter between the option and the value. Unnamed parameters are registered via the TOptionsRegistry.RegisterUnNamedOption<T> method. Unlike named options, unnamed options are positional, but only when more than one is registered, as they will be passed to the anonymous methods in the order they are registered. Parsing the options. Parsing the options is as simple as calling TOptionsRegistry.Parse, which returns a ICommandLineParseResult object. Check the HasErrors property to see if the options were valid, the ErrorText property has the parser error messages. Printing Usage If the parser reports errors, then typically you would show the user what the valid options are and exit the application, e.g: parseresult := TOptionsRegistry.Parse; if parseresult.HasErrors then begin Writeln(parseresult.ErrorText); Writeln('Usage :'); TOptionsRegistry.PrintUsage( procedure(value : string) begin Writeln(value); end); end else ..start normal execution here The TOptionsRegistry.PrintUsage makes it easy to print the usage to the command line. When I started working on this library, I found some really complex libraries (mostly .net) out there with a lot of options, but I decided to keep mine as simple as possible and only cover off the scenarios I need right now. So it's entirely possible this doesn't do everything people might need, but it's pretty easy to extend. The VSoft.CommandLineParser library (just three units) is open source and available on Github, with a sample application and unit tests (DUnitX) included."},{"title":"Issue with uploading compressed artifacts in Go 14.3.0","tags":"ciandcd","url":"http://ciandcd.github.io/issue-with-uploading-compressed-artifacts-in-go-1430.html","text":"From: http://www.go.cd/2014/11/14/Go_14_3_issue_with_uploading_compressed_artifacts.html There was an issue reported with respect to artifact uploads in the 14.3.0 release of Go. Issue On Go server running with OpenJDK7, uploading compressed artifacts fails. The operation is reported as success in console-log on job details page but actual artifact does not get uploaded. Please note this does not affect uploading console-log itself or even a simple file as an artifact. This is an issue only with Go v14.3.0. Read further to know if you are affected by this defect. Who does this affect? This defect would affect you only if your Go Server v14.3.0 is run using OpenJDK(jdk or jre) and the agent responsible for artifact upload is using a version of java other than the one used by Go server. You are unaffected by this defect if: you use Sun/Oracle java to run Go Server you have SunEC extension installed for your OpenJDK [EDIT: on your Go server] Both your server and agent processes run using OpenJDK (not necessarily the same version) [EDIT: without ECC cipher suites . As explained in \"What caused this?\" section, this issue occurs when java on agent supports ECC cipher suites but java on server doesnot. You should definitely run the litmus test to be sure.] Litmus test You should run a pipeline which uploads a compressed file as an artifact to ensure you are not affected by this defect. If the artifact shows up on the artifacts tab of the job and can be successfully downloaded, you are safe from the defect. Workaround We are aware that you have been waiting to try your hands on the new APIs and several bug fixes that came out in 14.3.0. Fortunately we have a few workarounds available to help you move ahead with the upgrade, meanwhile we would be working on finding the best possible fix for this issue. You could go for one of the workarounds listed below: Options: Install SunEC extension on your OpenJDK setup. Get sunec.jar and place it under jre/lib/ext/ folder. You must also place libsunec.so in jre/lib/amd64/ folder. OR Install Oracle JRE on your Go server and switch to that. For Go server running on Windows, this means updating the system level environment variable GO_SERVER_JAVA_HOME to oracle jre home For Linux based installations, update /etc/default/go-server to set JAVA_HOME to oracle jre home. For Others, set the value of system level environment variable JAVA_HOME to oracle jre home. OR Install the same version of java as you have on your agent. Update java used by Go server to appropriate value as suggested in the previous option. At this point restart Go Server. Trigger your affected pipeline to ensure the artifact got uploaded successfully. This should resolve the issue. If the issue persists even after applying the suggested workaround, do write to the go-cd mailing list reporting the same. What caused this? I cannot speak about this without getting into some technical details. As a part of upgrading to Rails v4, we also had to upgrade bouncycastle-bcprov library. Earlier versions of Go used bouncycastle-bcprov v1.40. With that the cipher suites accepted by Go server would always be one of SSL_RSA_WITH_RC4_128_SHA , SSL_RSA_EXPORT_WITH_RC4_40_MD5 or SSL_RSA_WITH_RC4_128_MD5 . Go 14.3.0 has moved on to use org.bouncycastle-bcprov v1.47. Bouncycastle v1.46 brought in support for ECC cipher suites . ECC cipher suites are made available by SunEC extension which is not packaged along with OpenJDK7 by default. Go server is run using Jetty which uses bouncycastle crypto package (i.e. bcprov) for the handshake. At this point, instead of agreeing on the expected SSL & RSA based ciphers, one of the ECC ciphers get picked during the agent/server handshake. Jetty6 (yes, we are still using this!) does not work well with the modern cipher suites and causes issues like the one mentioned above. To tackle such scenarios, all supported ciphers suites apart from the three mentioned above are excluded from Jetty. Since, ECC cipher suites were not available on the server, they did not get excluded. However if your JVM has ECC ciphers available (by default or after applying one of the workarounds), Go would ensure they get excluded and make Jetty happy. Jetty upgrade has been on the cards for a while, and that would possibly help resolve this. But that is a much involved task and hence works better as a long term solution. We need to evaluate other options as well which could fix the issue in the short term. Many thanks to Vladimir Bormotov for reporting this issue and allowing us to use his setup to gather the required debug information. As always, you could write to go-cd and go-cd-dev mailing lists if you have any ideas/feedback/questions."},{"title":"Jazz Team Blog  Announcing Rational Publishing Engine 2.0 GA","tags":"ciandcd","url":"http://ciandcd.github.io/jazz-team-blog-announcing-rational-publishing-engine-20-ga.html","text":"From: https://jazz.net/blog/index.php/2015/06/17/announcing-rational-publishing-engine-2-0-ga/ The Rational Publishing Engine 2.0 is now available as a GA download. This major release includes a simplified web interface that will help clients focus on generating documents with minimal steps and also apply effective template reuse within their organization. To learn more about the capabilities of this release, read the \" What's new \" section in our Infocenter documentation for RPE 2.0 . You can try the document generation capabilities by accessing our cloud sandbox on https://rpe.mybluemix.net/rpeng/home . Login with your id and click on \"Create Examples\" for a quick start on using the new web interface. We look forward to your feedback on the design and usability of the system. If there are any use cases in your organization that is not addressed with the current system, please write to us at rpe20_beta_support@wwpdl.vnet.ibm.com In this release, we have adopted IBM Design Thinking methodology for the first time. We have received valuable inputs from our clients and our stakeholders through the design partner program. I would like to thank all our clients who participated in the design partner and in the Beta program. We look forward to your inputs on the GA version of Rational Publishing Engine 2.0."},{"title":"Jazz Team Blog  Big changes coming to jazz.net","tags":"ciandcd","url":"http://ciandcd.github.io/jazz-team-blog-big-changes-coming-to-jazznet.html","text":"From: https://jazz.net/blog/index.php/2015/04/22/big-changes-coming-to-jazz-net/ You may have noticed some changes when you came to visit Jazz.net today. After 8 years, the jazz.net team thought it was time to give the site a more thorough overhaul then it has had in the past. Keeping Agile and Lean principles in mind, we're starting small and looking to get early feedback on the changes we make. To start, we are rolling out a new look for our Home page, a combined Product and Download landing page, as well as a new navigation bar and footer. For the new look and feel, we want to incorporate current IBM Design principles. This will help us to be more consistent with other IBM sites, and pare down some of the content that has made the pages get more crowded over the years. We also want to have our site be more mobile-friendly, to reflect the growing portion of our audience accessing our site from mobile devices. Over the coming months, we'll start rolling out updates to the rest of the site. In addition to updating the look for all the pages on the site, we'll be reviewing the content in the jazz.net Library and on the Deployment Wiki to make sure the content is up to date. We also want to make sure we have our development team engaged with the community, so look for more content about our Jazz based products over the coming months as well. As I mentioned above, we want to incorporate Agile and Lean principles in this rollout, and this is where you come in. We're looking for your feedback on the changes we're making. Hopefully you like it, and let us know if you do! If there are things we should do differently, or you have other thoughts about the overhaul, we want to hear that as well. The sooner we get the feedback, the better job we can do of incorporating it and making this site as useful to you as possible. So drop a comment on this blog, tell us on Facebook or Twitter ( @JazzDotNet ), or post in the forums using the tag \"jazz-dot-net-overhaul\"! We look forward to hearing from you!"},{"title":"Jazz Team Blog  Bluemix Devops Services is hiring!","tags":"ciandcd","url":"http://ciandcd.github.io/jazz-team-blog-bluemix-devops-services-is-hiring.html","text":"From: https://jazz.net/blog/index.php/2015/04/08/bluemix-devops-services-is-hiring/ We're hiring! IBM is serious about the Cloud. It's where business is headed and you should be there too. Developing and deploying web and mobile applications in a continuous delivery environment is complex and challenging. IBM Bluemix cloud technology is based on open standards such as Cloud Foundry and Docker, with industrial strength software-defined infrastructure provided by IBM SoftLayer. Bluemix DevOps Services combines a core devops tool chain and integrations with popular tools like GitHub, into a single highly productive environment built on a modern micro-services architecture. Work here to develop the future and change the computing landscape. If you have the drive, IBM has the grit and the clout to make it happen. Contact us at idsorg@us.ibm.com Dave Thomson Director and Distinguished Engineer, IBM Bluemix DevOps Services"},{"title":"Jazz Team Blog  Collaborative Lifecycle Management 6.0 in a nutshell","tags":"ciandcd","url":"http://ciandcd.github.io/jazz-team-blog-collaborative-lifecycle-management-60-in-a-nutshell.html","text":"From: https://jazz.net/blog/index.php/2015/06/26/collaborative-lifecycle-mangagement-6-0-in-a-nutshell/ Since our announcement on June 9th, we've spent the last couple of weeks here on Jazz.net sharing with all of you more information about the new features and their value. We're planning on spending some more time in the next couple of weeks continuing that sharing. But today is an exciting day for all of us on the Collaborative Lifecycle Management (CLM) team— Version 6.0 is generally available! This is a culmination of a lengthy development cycle for us. If you were following along here on Jazz.net, we developed new capabilities over the course of 11 sprints. Much of that, which you'll see as you look through the content in v6.0, was required for the effort—from the Jazz Foundation on up—to provide configuration management across the integrated solution. But what you don't see, or what maybe isn't immediately apparent, is all the work occurring behind the scenes. So, to highlight just a few of the changes: During v6.0, we shifted to building twice per day—10 times per week. This allowed us to slow the amount of feature change occurring in just daily builds to ensure a more stable experience. We expanded our development pipeline to 14 different tests, and those tests run against every build. If you look back as far as five years ago, we had no automated development pipeline. So from zero automated pipeline testing to running 140 pipeline tests (14 tests for 10 builds every week) in five years. We built a federated environment in which to test configuration management. This distributed environment was built to mimic some of our more complex customer shops and allow us a much greater depth of testing for v6.0 than we've had in the past. Feature development included Configuration Management as well as a number of DevOps enhancements. You'll see a lot of Configuration Management content on Jazz.net related to CLM v6.0. Rather than try to give it a full summary here, I'll point you to some key information: We've put Configuration Management behind an activation key. The intent here was to ensure you are walking through all the considerations related to CM before you turn it on. If you're worried that CM is really just for clients who are building complex hardware devices, it's not. You can use CM in simple scenarios as well. And finally, of course, once you've considered it and understand how you might use it, visit Getting Started with Configuration Management . But CM wasn't the only set of feature work in CLM v6.0. SAFe 3.0 brings a new template to Rational Team Concert (RTC) and extends a project to the business teams involved in development. You can get out of the box and up and running with the SAFe template, improving agility and predictability with role-based dashboards. Continued scalability improvements to Enterprise Edition, integration with UrbanCode Deploy, Single Sign-On (SSO) for Kerberos and OIDC for all RTC clients are numbered among other enhancements. Finally, our Reporting team improved setup and configuration, simplified the customer experience, integrated reports into QuickPlanner, and have given you \"near live\" operational reporting. I urge you to stop by our Deployment wiki , one of our most popular destinations. All in all, it's been a very active release for both Development and Operations, and one we're very proud of."},{"title":"Jazz Team Blog  Configuration management – it's not just for building airplanes or cars","tags":"ciandcd","url":"http://ciandcd.github.io/jazz-team-blog-configuration-management-its-not-just-for-building-airplanes-or-cars.html","text":"From: https://jazz.net/blog/index.php/2015/06/15/configuration-management-%e2%80%93-it%e2%80%99s-not-just-for-building-airplanes-or-cars/ Can you imagine a software team working without a configuration management system? Could your team do any collaboration without versions, streams and baselines? Do you ever need to reverse a change that introduced a defect? If so, then you need to keep track of versions with a history of who changed what, when, and why. Do you want some of your engineers to be able to work on the next release while the current release is in final stabilization and testing? If so, then you need multiple streams of development. Do you ever want to produce a report on the changes in a new release? If so, then you need to compare the versions in a previous baseline to the new baseline, or to the current state of the development stream. Do you ever need to create a patch to a previous release? If so, you would need a baseline of that previous release, and a way to branch a new stream from it. Do you ever need to try an experiment, or make a one-off change? If so, you need to create a new stream, make some changes there, and then possibly merge the contents of that experimental stream into the main line of development. Software Configuration Management provides these and other benefits, such as improved collaboration and reuse. Wouldn't you want these same capabilities and benefits in your other tools, including requirements management, modeling, and quality testing? DOORS Next Generation 6.0 and Rational Quality Manager 6.0 provide these features, just as Rational Rhapsody Design Manager has done since release 4.0. And now that you have configuration management in each tool, think about all these configuration management concepts but in the broader sense – the ability to do it across the lifecycle – for the overall combination of your requirements, designs, test plans, and more. Global Configurations, introduced in Collaborative Lifecycle Management 6.0, provides the ability to do just that – to keep track of versions, streams, and baselines of your entire system. Whether you are building airplanes, cars, financial systems, web sites, health care systems, or entertainment systems, configuration management is a key part of your path to success. Nick Crossley"},{"title":"Jazz Team Blog  IBM Rational Publishing Engine 2.0 M5 Beta – Improved document styling","tags":"ciandcd","url":"http://ciandcd.github.io/jazz-team-blog-ibm-rational-publishing-engine-20-m5-beta-improved-document-styling.html","text":"From: https://jazz.net/blog/index.php/2015/03/25/ibm-rational-publishing-engine-2-0-m5-beta-improved-document-styling/ We're glad to announce that the Rational Publishing Engine (RPE) 2.0 Beta on Bluemix has been updated to the M5 build ! There is no registration or special process required in order to access the beta. Aside from announcing this update, the intention of this post is to provide a little extra help to those looking for guidance on getting started using using the RPE 2.0 M5 Beta via some helpful resources. We'll also touch on what's new in this build of the beta. Goal of the beta This is meant to give RPE users an opportunity to provide feedback on the features and usability of our new web interface. The focus is on the report designer and end user scenarios. What we'd like to learn is: Is the design simple enough for our end users? Do you see all the capabilities that your report designers need? Is the overall interface intuitive enough? What is missing from this new component of RPE to make it attractive for your organization? We welcome all your feedback, so please send your thoughts to rpe20_beta_support@wwpdl.vnet.ibm.com ! Resources Help Guide: IBM Rational Publishing Engine 2 M5 Help Video Tutorials: What's new in this build NOTE : Due to the new features implemented in M5, we had to recreate the database which means that assets and documents created using the previous build have been lost. (i) Notifications The notifications widget at the top of the screen is now active and will show all new events that occurred since you last checked its contents. The only event supported for now is the completion (successful or not) of a document generation. (ii) Stylesheet support You can now upload stylesheets and use them in your reports. (iii) Advanced Configuration Mode for reports Report designers now have access to an advanced edit mode where the report configuration can be tailored in detail. It is now possible to: Set the default connection for the report data sources Set the default values for the report variables Rename variables and data sources to be more meaningful to the end user Hide variables and data sources from the end user In the image above, the report designer modifies the \"News Glimpse\" report by setting its data source connection to the BBC News Feed and at the same time hiding the data source. This will effectively set the report to this configuration and end users can run it without further configuration. (iv) Other changes The action list has been pruned with many of the redundant actions removed or redistributed. The default action in the Design page has been changed to \"Edit\". The user can view all documents generated for a report from the Generate page by clicking \"View Generated documents\" in the Actions menu. Users can create examples multiple times. The assets created in each run of \"Create Examples\" are independent of one another. Internet Explorer 10 and 11 are now supported. Error messages should be more informative now. Bug Fixes The following limitations of the previous build have been addressed: On the Generate page, if you select the Generate Later action, the scheduled run of the report is created but you cannot download the documents or the logs If there is an error while scheduling a report using the Generate Later action, check if the date is in the past. IE is supported Known Issues If after signing on you are redirected to an empty page by the IBM Single Sign-On you need to issue the original request in the browser, https://rpe.mybluemix.net/rpeng"},{"title":"Jazz Team Blog  Introducing SAFe® with the Power of IBM DevOps","tags":"ciandcd","url":"http://ciandcd.github.io/jazz-team-blog-introducing-safer-with-the-power-of-ibm-devops.html","text":"From: https://jazz.net/blog/index.php/2015/06/24/introducing-safe%c2%ae-with-the-power-of-ibm-devops/ Do you need to orchestrate software development and delivery in your complex, heterogeneous environment but you're not sure where to start? Have you heard about SAFe but you don't know what it is or why you should care? New in Rational Team Concert V6.0 is a feature that supports the Scaled Agile Framework ® (SAFe) out of the box. This feature enables you to explore the framework and establish a SAFe Program of your own, complete with the infrastructure, artifacts, best practices, and guidance prescribed by SAFe built right into the tooling. SAFe is the market-leading process framework for scaling lean and agile across the enterprise, providing guidance and best practices to help organizations realize success. SAFe with the Power of IBM DevOps is the combination of SAFe plus IBM DevOps to provide a comprehensive process and tooling framework that helps your organization balance the efficiencies gained through adoption of lean principles with the effectiveness realized through agile adoption to deliver the right things right . The result is a framework which synchronizes development, testing and deployment across teams in heterogeneous environments, and enables collaboration of all roles in the organization in the planning and execution of software delivery. Get Up and Running Quickly The SAFe support in Rational Team Concert V6.0 provides an easy way for you to get up and running quickly to lead a SAFe-based transformation. Whether you are a SAFe novice who is just starting to explore, or an expert who is quite familiar with what it means to \"do SAFe\", our support combines the flexibility and scale you need to support true enterprise teams using multiple processes, technologies, and tooling. Let's take a look at a few details. Project Area Initialization Rational Team Concert 6.0 includes a new SAFe 3.0 Program process template that helps you to easily establish a SAFe Program-level tooling infrastructure. When you use the template to create a Rational Team Concert project area, you will get out-of-the-box support for Programs and Teams wanting to incorporate agile and lean practices into the development and delivery of software with SAFe best practices and guidance. This includes an Agile Release Train timeline, SAFe roles and permissions, and a SAFe Program/Team hierarchy with associated work item categories. While the project area is configured to support SAFe Programs with Teams in the same Rational Team Concert project area, it can easily be used to support Programs that are tracking the work of existing Teams in separate project areas as well. Furthermore, it can be used to establish project areas for SAFe Teams separate from the Program because the process template includes updated Scrum elements that are aligned with SAFe. Simple editing of the project area configuration provides you with the flexibility you need to support multiple different Program and Team topologies. SAFe Artifacts SAFe artifacts are manifested in Rational Team Concert through work item types, plan types, and plan views. These artifacts support the most critical SAFe best practices, including relative ranking based on Weighted Shortest Job First (WSJF), Kanban planning, and Program value-based delivery. Work Item Types The process template includes Program Epic and Feature work item types for the SAFe Program level, Story and Task work item types for the SAFe Team level, and PI Objectives for both Program and Team levels. Additional work item types for Defect, Risk, and Retrospective are also included. Program Epics and Features include the WSJF attribute to help ease adoption of lean thinking by providing an economic means of decision-making. This enables the prioritization of work that maximizes business benefit. The WSJF attribute is calculated using this formula: The WSJF component attributes (User/Business Value, Time Criticality, RR/OE, and Job Size) are provided as Fibonacci enumerations, which can be tailored to suit your organization's preferences. For the Epic, we also provide a Value Statement template to remind you of how best to articulate Program Epics. The PI Objective is a way to track value delivery, which is a critical aspect of Enterprise Scaled Agile and IBM DevOps principles. By capturing the notion of planned and actual value delivery, we can provide you with insights into your organization's ability to deliver value to your business and to customers. Plan Views Plan Views provided by the process template enable high-value SAFe processes. The SAFe Kanban System for Program Epics is a visual representation of rank and status. The ability to capture and enforce Work in Progress (WIP) limits enables your team to practice the lean principle of \"just enough\" investment, reminding you again to take an economic view in decision-making. The WSJF Ranked List view takes this concept a step further by helping your team learn how to quickly and relatively rank Features based on WSJF. Remember, it is not about high value alone, but the \"biggest bang for the buck\". This plan view helps your team learn how to consistently rank Features to maximize benefit—to your customers as well as to your business. Dashboards, Queries, Reports No tooling infrastructure provides value unless it can turn your planning and tracking data into actionable analysis, so the process template helps you get started by providing Program and Team dashboards, along with queries and reports that are consistent with the SAFe Program and Team metrics. Here are some examples: We also augment the SAFe guidance with our own, applying the knowledge of our experience working with enterprise customers to create incisive reports that you can use to drive decision-making. In particular, we provide insight into cross-team dependencies to help you address a critical pain point in managing software development and delivery across the Program. Learn by Doing So, the \"process\" and \"tooling\" to help your Program adopt lean and agile principles is provided—but what about the \"people\" aspect? The cultural transformation is perhaps the hardest part of an agile transformation initiative, no doubt about it. To help you with this, we incorporate process mentoring with the SAFe support in Rational Team Concert by providing you with work item templates that remind you of the typical tasks related to specific SAFe events and activities. The process template includes Work Item Templates for: Program Initiation Activities . Creates tasks that guide you on the activities required to initiate your SAFe Program Program Increment. Creates tasks that guide your team to lead a Program Increment, from the Release Planning Event through delivery Development Iteration. Creates tasks to help teams plan development iterations through delivery of functionality Innovation and Planning. Creates tasks for the Innovation and Planning sprints All of the work item tasks describe activities consistent with those prescribed by SAFe and include in-context mentoring via a direct link to the topic described by the task on the SAFe website . This enables you to learn as you go and helps to ease adoption of SAFe, step by step. Plan SAFely While no tool alone can plan for you or make your teams collaborate, IBM's SAFe solution can greatly simplify the transformation through tooling integrated with process guidance, coupled with the right data to help with your decision-making. I hope you like what you see. We would appreciate any and all feedback as we enhance our SAFe support going forward. As always, visit the Collaborative Lifecycle Management and Rational Team Concert What's Happening sections for the latest news. And to keep abreast of everything related to our SAFe support now and in the future, please visit our SAFe landing page on the IBM DevOps Community and check back often for updates! Thank you!"},{"title":"Jazz Team Blog  New single sign-on options in CLM 6.0","tags":"ciandcd","url":"http://ciandcd.github.io/jazz-team-blog-new-single-sign-on-options-in-clm-60.html","text":"From: https://jazz.net/blog/index.php/2015/06/19/new-single-sign-on-options-in-clm-6-0/ Collaborative Lifecycle Management (CLM) has directly supported two types of single sign-on (SSO) authentication for some time. First, all applications installed in the same application server (whether IBM WebSphere Application Server or Tomcat) automatically share login sessions, such that if you log in to one application, you are also logged into all the other applications deployed in the same server. Second, when all applications are deployed in one or more WebSphere Application Servers, you can configure Lightweight Third Party Authentication (LTPA) SSO so that login sessions are shared across all the WebSphere servers. We are excited to announce that we have added two new SSO options in the CLM 6.0 release. You'll now have the option to use either Kerberos authentication, or what we call \"Jazz Security Architecture Single Sign-On\", which is based on the OpenID Connect standards. Kerberos is a well-established SSO protocol that is also the default authentication protocol used by Microsoft Windows, so if your organization uses Windows workstations, Microsoft Active Directory for user management, and deploy Jazz applications in WebSphere 8 or later, it will be possible to configure CLM so that your Windows login session is automatically used to log in to CLM. Kerberos can also be used with non-Windows workstations, as long as you use a Microsoft Active Directory server to manage your user accounts. While it has been possible to configure CLM servers to use Kerberos for some time, only web browser clients could take advantage of Kerberos login sessions – the RTC Eclipse client, the Microsoft Visual Studio RTC client, the other Windows .NET clients, the RTC build engine and clients, and the various command-line clients could not use Kerberos. Now, all those RTC clients will work with Kerberos. See Single sign-on authentication in CLM for the complete list of RTC clients that support Kerberos, and Configuring Kerberos for details on setting up Kerberos for CLM. The OpenID Connect (OIDC) authentication protocol was established in early 2014 as an extension of the OAuth 2.0 protocol, designed to be easier to adopt across a wide range of clients (native applications, browsers, browser-based applications, and mobile devices). It is extensible and configurable (with optional features). Jazz Security Architecture (JSA) is a particular profile of OIDC, specifying which optional features are included, and a few extensions. Authentication is handled by the Jazz Authorization Server (JAS); Jazz applications delegate to that server instead of relying on the application server to handle authentication. Single sign-on is supported across all applications that are configured to use the same JAS, independent of what sort of application servers they are deployed in, and what platform they are running on. To use Jazz Security Architecture SSO, you must install the Jazz Authorization Server , configure it and start it up , and either enable JSA SSO in CLM applications when installing (for a new installation), or enable JSA SSO after upgrading to the 6.0 release (for existing installations). The login form for the JAS looks very similar to the Jazz application login form, but you'll know that you're using the JAS for authentication if the login form says \"AUTHORIZATION SERVER\" instead of \"TEAM SERVER\": You can find more information on how authentication works in general, and the various options available, in the Jazz Server Authentication Explained article. We're hoping these new SSO options provide increased flexibility and ease-of-use for our users. John Vasta Senior Software Engineer"},{"title":"Jazz Team Blog  Raising your game with configuration management in and across your tools","tags":"ciandcd","url":"http://ciandcd.github.io/jazz-team-blog-raising-your-game-with-configuration-management-in-and-across-your-tools.html","text":"From: https://jazz.net/blog/index.php/2015/06/09/raising-your-game-with-configuration-management-in-and-across-your-tools/ High-performing teams are never satisfied with their performance. They regularly ask, \"How can we do better?\" They have a certain restlessness. A sense of mission, stewardship, and empathy for the people who will use the things they design and build. It's our mission to provide tools and make possible development practices that help your high-performing teams improve their game. To that end, Collaborative Lifecycle Management (CLM) 6.0 brings significant new capabilities for configuration management within and across your Jazz tools — with the potential for tools from other vendors to participate in these innovations. In our days you'd be hard pressed to find a software development team that would undertake serious work without using a SCM system. In CLM 6.0, we are extending configuration management capabilities (including development streams, baselines, branching, merging, change sets, comparing across streams) to other tools, so practitioners in other disciplines can gain the same kinds of efficiencies. We are solving this in an open, federated way through (1) new implementations of configuration management in Rational DOORS Next Generation and Rational Quality Manager; and (2) support for Global Configurations as defined in in the OASIS OSLC Configuration Management specification. We expect these capabilities will help teams be more effective in using baselines, doing parallel development, working in large programs of projects, and doing product line engineering. Look for baby steps you can take. Walk now; run later. To learn more, check out the Continuous Engineering blog on developerWorks, or see the short videos on developerWorks . Daniel Moul Senior Product Manager P.S. Many thanks to those of you who downloaded beta milestones and provided feedback, or sat down with us to share your insights about what your engineers need to raise their game. We know you are on a journey; we are too. It's a privilege to run together."},{"title":"Jazz Team Blog  Success with System Verification Test (SVT) Automation","tags":"ciandcd","url":"http://ciandcd.github.io/jazz-team-blog-success-with-system-verification-test-svt-automation.html","text":"From: https://jazz.net/blog/index.php/2015/06/05/success-with-system-verification-test-svt-automation/ Back when the System Verification Test (SVT) team started thinking about Collaborative Lifecycle Management (CLM) 6.0, they put some automation goals in place. Having identified a key bottleneck of long periods of time being spent on setting up the configurations for scenario testing, the first goal was to build automated scripts to speed up the configurations in order for the manual testers to start testing faster and reduce the test cycle time. The second automation goal was aimed more conventionally at uncovering defects in code changes and increasing automated test coverage. This was focused on adding automation to cover PLE scenarios and also to enhancing scenarios to include new features like configuration management. Prior to 2014, System Test focus was on deploying configurations in an automated way. At the beginning of 2014, System Test had one and a half automation engineers who were automating the out of the box customer experience, the Money that Matters scenario. While that goal of ensuring that scenario was mostly automated was achieved by end of year, the process was painful and slow. In light of the DevOps transformation, at the and of 2014, SVT decided to invest more in automation. They added three more engineers to work on this. That wasn't an easy decision, but a necessary one, and it meant taking on a risk as less manual test coverage was provided as a result of this move. 2015 has been a great year and the five engineers working on automation have already commissioned seven automated scenarios this year. These have been focused on data generation, to help manual testers and shorten the test cycle in addition to the automated configurations. There is good communication with development teams. An automation team is mostly useless if you don't get the right amount of support from development when you raise concerns or defects if they don't follow up quickly. The close relationships with development allows the System Team to cover the cutting edge use cases and quickly respond to defects found. Now, turn around time is typically within a day. The teams have come from a time where there was little support for automated testing to today with a fully automated set of tests. They didn't have a BVT, or rather it was manual. Each team would blindly take the build and hope it installed. They moved to automation where the build is created, gets picked up, automation runs jUnits, then it gets deployed to our golden topologies, using different app servers, different tests, and they are all designed according to how customers use the products. The team is also using our tools, Rational Quality Manager (RQM) for test execution and reporting, and Rational Team Concert (RTC) for test development. They have gone from no automation to now where a large number of automated tests are executed daily against both maintenance and new release streams. This has been worth the investment, but it has not been easy. For one thing, there have been some plan changes along the way. Trying to keep up with developing the new automation while product plans are changing makes it difficult to make sure that the teams are covering the right user experience. Also, there are new members to the automation team who need to be trained not only on the development of the automation, but also the framework for the automated tests. There is a process in place to get the automation engineers up to speed quickly, but probably the most important part is working together as a team and starting simple before moving to the more complex areas, like the framework. There is a review process with the team so that everyone learns from one another and if there is a problem the reviewer will identify the area in need of improvement and also make a suggestion about how to solve the issue. Given that there is a good mix of manual testers and now test automation developers, it's important to work together for the least risk and the most coverage. This has paid off for the team, so that the team is building automation to help the manual testers find defects quickly, build automation to find regressions in existing code and to find defects in new features. There has been a significant number of defects found prior to GA so far, and the majority of them have been critical and higher severity. Just knowing that they won't be released is a huge confidence builder for the teams. The System Test team will continue to invest in automation, making use of the automation community that has been built up out of these efforts. The community is geographically distributed, under different leadership and has great collaboration with development. The openness and transparency allows for the development teams to execute newly created automated tests against their code changes prior to delivery. The increased confidence in quality has been and continues to be worth the investment. Beth Zukowsky Program Director and Rational DevOps Protagonist"},{"title":"Jazz Team Blog  Transforming your product development for the IoT","tags":"ciandcd","url":"http://ciandcd.github.io/jazz-team-blog-transforming-your-product-development-for-the-iot.html","text":"From: https://jazz.net/blog/index.php/2015/06/25/transforming-your-product-development-for-the-iot/ Development Practices for the IoT Era The Internet of Things (IoT) is not just about connecting things to the Internet and controlling them remotely. It is a major opportunity for developers of things (makers) and operators of things to unlock new value propositions from the lifecycle of things, and to improve innovation and the quality of things. In our context, things are products such as automobiles, medical devices, consumer goods, factory machines, etc. To leverage the value of connected and instrumented devices, there are several important aspects to consider, one of which is a proper digital product development process. Also, it is no longer a secret that in today's advanced products most of the innovation comes from the embedded software, so the effectiveness of the software development process is an important parameter of the overall product development process. Here are some key leverage points of connected products: It is possible to continuously collect operational data from devices when in operation. It is possible to remotely update the software that is embedded in the products. With the addition of social media, product makers can also get continuous information on how products are used in different market segments. However, to leverage such enablers, product makers need to change their traditional development processes. The development processes need to be much more dynamic and agile to leverage the connectivity and advanced engagement of connected products. Here are some key transformational aspects for the development process: Connected product complexity —Complexity is increasing with the additional functionalities provided by new interactions between products and product to cloud. Practices for handling this complexity rely on digital systems engineering processes that are based on digital representations of product requirements, product architecture, and product verification plans. Such digital systems engineering approaches enable continuous verification of product designs to eliminate risks early in the process as part of addressing this complexity. Continuous verification utilizes techniques such as simulation and rules-based checking to validate the requirements and the system architecture. Transforming data into engineering insight —The amount of operational data available from connected systems is overwhelming and typically engineering information is locked in isolated silos. Data coming from operations and manufacture may trace to product requirements, product design, and product test. Being able to properly analyze all those product engineering aspects requires complete digitalization, traceability, and analytics of all product development aspects. Increasing speed of development —The connected world increases the need to respond much more quickly to market findings and demands. The ability to effectively respond to change in multidisciplinary products depends on an effective change management process, where impact analysis of the change is conducted in a completely digital manner based on query across lifecycle data. It also relies on the ability to create change contexts across the lifecycle, without interfering with the overall system state before the change is actually approved. Creating such change contexts is enabled by configurations across the lifecycle. Specialization —With the advancement of social media around connected products there is going to be higher demand to create more specialized products to deal with competition and optimize product revenue. That requires capabilities to properly manage reuse and variation as part of the product development process. Ineffective ways to manage variation limit the ability of product makers to effectively leverage product variation. Streamlined process with continuous integration —IoT architectures require both proper support for embedded software that can be updated on devices, as well as software on cloud that analyzes and controls devices. In order to achieve this transformation of the engineering development process, customers should look to the IBM Internet of Things Continuous Engineering Solution . The IBM Continuous Engineering (CE) platform, which is based on the Rational solution for Collaborative Lifecycle Management (CLM), provides the infrastructure and capabilities to enable a digital engineering lifecycle, which is necessary to meet the challengse of rapid and effective multidisciplinary development. Any activity and artifact as part of the process are digital and cross-linked—whether those artifacts are requirements, product designs and architectures, test plans or change history. There is no need to rely on traditional documents in the process, which are typically the main blocker for digitalization of the lifecycle. Open, standards-based lifecycle data indexing, query, reporting, and analysis are also key to effectively supporting the stream of incoming changes as part of the connected lifecycle. Recent updates to the CE platform now provide the new capability to define cross-lifecycle configurations, enabling parallel work on new innovations as well as effectively handling product variations by efficient reuse. To summarize, the new generation of connected products that makes the Internet of Things is a major opportunity for product makers if they properly adapt their product development practices to leverage the opportunities and meet the challenges. As already identified by some key IoT-related initiatives, such as Industrie 4.0 in Germany, and Industrial Internet of Things (IIoT) initiatives in the United States, the required transformation is to shift product development to a digital platform."},{"title":"Jazz Team Blog  Try out the new configuration management features in CLM 6.0 RC1!","tags":"ciandcd","url":"http://ciandcd.github.io/jazz-team-blog-try-out-the-new-configuration-management-features-in-clm-60-rc1.html","text":"From: https://jazz.net/blog/index.php/2015/05/14/try-out-the-new-configuration-management-features-in-clm-6-0-rc1/ The recent release of Collaborative Lifecycle Management (CLM) 6.0 RC1 signals the release candidate (RC) phase of development, which means that the official CLM 6.0 release is in its final stages. The 6.0 release offers new and exciting configuration management capabilities. To learn more about these capabilities, read the Getting started with configuration management help topic or watch the Introduction to configuration management video, which is part of a larger video series . You can try the configuration management capabilities in an existing cloud sandbox , or you can evaluate them in your own environment by downloading CLM 6.0 RC1 , reading the considerations , and obtaining an activation key. Tim Feeney Executive IT Specialist"},{"title":"Jazz Team Blog  Unlocking engineering insight for an IoT world","tags":"ciandcd","url":"http://ciandcd.github.io/jazz-team-blog-unlocking-engineering-insight-for-an-iot-world.html","text":"From: https://jazz.net/blog/index.php/2015/06/24/unlocking-engineering-insight-for-an-iot-world/ Gary Cernosek Sr. Product Manager IBM IoT Continuous Engineering I'm writing this series of posts over the next few weeks to spur dialog on the topic of how organizations practicing in an IoT world need better ways to extract and derive value from the many sources that comprise their environment of engineering information. I plan to post this topic in four parts: Part 1: The State of engineering information . I open this series of blog entries with the context of why customers care about gaining insight from their engineering data now more than ever. Part 2: Aggregating engineering information from multiple sources . I'll next address the problems indicated in Part 1 by discussing old and new ways for bringing multiple sources of information together and unifying the way engineers gain access to and work with such information. Part 3: Integrating across disparate tools . Here I'll address the need to gain access to engineering information from tools not originally designed to be integrated with the outside world. Part 4: From information to insight . Imagine a world where we connect everything that needs to be connected. Then what? Here I'll discuss the vision for IBM's Continuous Engineering solution as it provides the basis for analytics and turning raw tool data into true engineering insight. Part 1: The State of engineering information Gary Cernosek Sr. Product Manager IBM IoT Continuous Engineering I know it's here, somewhere… Workers across engineering disciplines spend a lot of time (not) finding the information they need to do their jobs. Consider these statistics captured by KMWorld and The Ridge Group:1 Knowledge workers spend 15% to 35% of their time searching for information 40% of corporate users report they cannot find the information they need to do their jobs 50% of Internet searches are abandoned 90% of the time that knowledge workers spend in creating new reports is recreating information that already exists The problem is not creating good information, it's finding it! What's the root cause of these problems? Much of it has to do with how engineering environments tend to be highly fragmented across disparate tools. And the challenge to connect them is growing exponentially. Each engineering tool comes with its own user interface, and often multiple interfaces for use on the Web vs. desktop application. Behind the scenes, the tools offer various presentations of views and tasks, and often proprietary logic for workflow, process, search, query, scale, security, and collaboration. Storage methods vary from use of individual files on workstation or servers to databases with proprietary interfaces. This degree of variance makes it very difficult for organizations to ensure that engineering information is available to users and traceable across different tools—even when the tools come from the same vendor! The results are brittle/poor integrations, silos everywhere, high cost to maintain and administer the tools, and little reuse. What's so special about IoT? Maybe your organization has handled these challenges fine up to now. But many are finding that IoT presents new or amplified issues that challenge their status quo. Two trends tend to stand out for organizations delivering products and systems connected to the Internet: Market pressure to increase product delivery frequency and compress cycle time Sheer volume and complexity of software required in modern products and systems These trends are depicted in the graphics below and illustrate the need for organizations to be more ‘agile' and to build ‘smarter' products: Projects that used to take years are expected to deliver in months, and those previously completed in months now have make at least incremental progress in weeks, or even days. Environments that historically treated software as a ‘part' that was captured once per product release cycle and stored off in the product data management tool for simple compliance now require greater granularity of lifecycle assets and tighter coordination between software and hardware engineering processes. These factors are driving organizations to reevaluate the way they develop and deliver their products and systems. Do you identify with these issues and trends? If these issues and challenges resonate with your experience, comment in the blog. Let me and others know how it's affecting your day-to-day worklife and your organization's business results. And stay tuned for the next parts of my entries for \"Unlocking engineering insight for an IoT world.\" 1Sources: KMWorld, \"The high cost of not finding information,\" http://bit.ly/1AnNGZO Information Gathering in the Electronic Age: The Hidden Cost of the Hunt, The Ridge Group"},{"title":"Jazz Team Blog  Using dashboards for status reviews: An interview with CLM Product Delivery Lead Brian Lang","tags":"ciandcd","url":"http://ciandcd.github.io/jazz-team-blog-using-dashboards-for-status-reviews-an-interview-with-clm-product-delivery-lead-brian-lang.html","text":"From: https://jazz.net/blog/index.php/2015/05/12/using-dashboards-for-status-reviews-an-interview-with-clm-product-delivery-lead-brian-lang/ Having experience with different types of reporting and status updates, Brian Lang, the Collaborative Lifecycle Management (CLM) Product Delivery Leader, spent some time discussing with me some of the key productivity gains he's noticed in using Rational Team Concert (RTC). The focus here is on the benefits of having a collaborative and transparent workflow, especially when it comes to meeting preparations. Brian shares with us the benefits of using RTC Dashboards so that the data is available, current, and updated as part of the natural development workflow. Brian, what types of meetings have you needed to pull together presentations in the past and today? In the past, when I was using a chart-based status process, leading up to the Senior VP's Monthly Operations Review (MOR), there would be a review with my VP where we would review the status of everything in my portfolio. This included key dates of delivery, status (red, yellow, green), and any additional key points that we wanted to raise to an executive level. These topics might typically included key capabilities, beta feedback, or what kinds of achievements or accomplishments the team had this month. Also, we prepared risks and mitigations. Each product would have their own chart. Additionally, we covered other things like burn down charts, user stories, velocity, story points, and month to month trends. We also covered various forms of technical debt in the form of APARs, defect backlogs which rounded out the reviews. Now, in CLM, while we still have MORs, the process is different even though the types of data are the same. Rather than create a separate set of material, the materials are baked into our workflow with regular reviews with teams. The same sorts of materials that would have been prepared and reviewed are part of the workflow including scrum meetings, release team meetings, and daily communications and work activities. The schedule is laid out and published for the whole team. Risks and mitigations are captured and updated in plan items. If a plan item's risk changes from green to yellow or red, the viewlet in the dashboard will pull in that plan item, with the latest updates about actions. What kinds of preparation do you do for these meetings? When pulling together charts, we would work our way backwards from the review with the Senior VP. We ended up with four separate meetings to review material specific to the status update. With the CLM dashboards, we can be ready for an update at any time. It's transparent and anyone in the team can check on the overall status because transparency is built into the product and supports our process. How long did it take to create the dashboards? Viewlets are part of the RTC capabilities and some are out of the box. For example, risk and issues are available out of the box. It doesn't take long to set up the queries against project areas. Depending on how many and the complexity, it typically takes less than an hour to get them put up. Once they're there, they are automatically updated, so you don't have to repeat creating them. One example is that we have a tab on our dashboard for risks and issues. When we are tracking multiple releases, like we did with 5.0.2 and 6.0, we can separate them out, while keeping them on the same tab. Going back to collecting the data for the charts, how many people are typically involved in gathering and consolidating that information? First, the offering team pulls together the information. They usually have a release engineer, a project management, a development lead or chief programmer, an architect, a test lead, and a support lead, six people typically. Sometimes there is the first line manager too so about six to seven people gathering and generating content to be reviewed. The managers were the ones who presented the data to me, they are responsible for the content being presented. How many teams did you have for a typical monthly status review? When I was doing chart-based reporting, I had about three key teams. This meant that 18-21 people were preparing for the meeting with me. How long does it take to pull that information together for each team? It takes a couple of hours for each person, approximately two to three hours to verify and answer are we green, yellow, or red?; how many test cases did we run?; what kinds of things are blocking us, etc. That was spent running reports and following up with people for verification. If things are not looking good, we may need more activity, up to maybe four hours. That's about 24 hours per team. Add on a 60-90 minute review in the offering team meeting making up another nine hours of people time (33 hours), plus another three hours for turnaround time on any follow ups. We're looking at maybe 36 – 40 person hours to generate that first set of data for review. Then there is the review with me. The least number of people in the room are the three managers and maybe someone from their team if there was a key technical issue to discuss, so five to six people for another 90 minutes making up another nine to ten hours. Then another set of questions, and we're up to 45-50 hours. We have another review including me and another person or two, the support and development leads, for another hour. After all of that, it adds up to about 60-65 person hours to get ready for the Sr. VP MOR. Talk a little more about how the preparation differs. The preparation is baked into the process. I update the executive status on the dashboard weekly as part of my preparation for the ALM release team weekly meeting. That is where I update my overall take on the release. If we're yellow, I share why I think we're yellow and what we're doing to mitigate the risk. It takes about 15 minutes for me to update that and it's part of my workflow. The release team meeting is where we talk about the release and where we need to focus. It's less about status and more about what the team needs to work on accomplishing that week. The chart-based reporting teams were also working on those things, but they needed to document it outside their natural process. How they met and communicated was a key difference because these types of updates were not part of their daily or weekly workflow. It's hard to do an apples to apples comparison because they are such different ways of working. In the CLM world, everyone can see when risks and issues are updated. In the chart world, the risks and issues were only seen by about 10% of the team. If only a subset of the team understands the risk, it's hard to go to a newly hired developer and ask about the work they were doing on risk \"X\" because they might not know what you're talking about, or know about it in the same context. Also, the \"man hours\" involved may be similar with the dashboards because the whole team is involved in updating issues regularly, but it's a natural part of the workflow so it doesn't feel like anyone on the team is spending time gathering status. We use Jazz.net to build Jazz.net, using the dashboards to drive our work. Once that work is done, we all can see our prioritized backlog to pull off the next thing to do more work. The collaborative nature of the design makes transparency, priorities, and risk part of the process vs ppt where only a select number of people see it. Most meetings result in follow up actions, how are actions from the meetings captured? Actions in the chart world would have been tracked in an email. I'd get an email saying that the VP met with Sr. vP and here are the two to three things to work on based on the review. Then I'd assign the work, and follow up. If my VP doesn't remember, or loses the email, the actions fall through the cracks. We'd report on those actions through email and follows up in status meetings or 1×1s. In the RTC world, actions are captured as work items. We review the actions to pursue which are tracked and prioritized with other work items in RTC. So, if anyone has any questions, they can revert to the work item. There is a live audit trail, when it was opened, who opened it, who owns, what has happened, etc. It becomes part of the workflow. I can also add that in test we have major productivity gains from dashboards usage that are linked to test plans. They help to eliminate the manual chart compilation work and results are kept live and up to date, which is imperative as our releases and test cycles are getting shorter. I'd like to thank Brian for taking the time to discuss this topic. Using RTC for dashboards enables a natural development workflow supporting open communication and transparency while reducing overhead for the organization. Understanding the priority of the work, the status of the work, and the risks of the work all help keep teams focused. By making it part of the natural development workflow it increases productivity. I'm not sure who to attribute this quote to: \"If it hurts, do it more often\" ( I'd like to thank Brian for taking the time to discuss this topic. Using RTC for dashboards enables a natural development workflow supporting open communication and transparency while reducing overhead for the organization. Understanding the priority of the work, the status of the work, and the risks of the work all help keep teams focused. By making it part of the natural development workflow it increases productivity. I'm not sure who to attribute this quote to: \"If it hurts, do it more often\" ( maybe ?). But, it definitely has reduced stress and overhead in this context! Beth Zukowsky Program Director and Rational DevOps Protagonist"},{"title":"Jazz Team Blog  What's new in DOORS Next Generation 6.0?","tags":"ciandcd","url":"http://ciandcd.github.io/jazz-team-blog-whats-new-in-doors-next-generation-60.html","text":"From: https://jazz.net/blog/index.php/2015/06/26/what%e2%80%99s-new-in-doors-next-generation-6-0/ This release could well be introducing the most fundamental function into a requirements management (RM) tool since RM systems began. While we continue our drive with usability and productivity, we are also providing support for requirements configuration management (CM), built from the ground up as part of the native tool, rather than simply integrating an external CM system. Configuration Management DOORS Next Generation 6.0 is our first release providing native configuration management of requirements, enabling functionality for strategic reuse and Product Line Engineering. RM offers the ability to define the scope of a project, program, or deliverable—but unless you have control over change this scope could well consume more of your project costs than you expected. Placing requirements under CM allows for the scope to be controlled, while enabling your teams to work in parallel versions at the same time without the need to make project copies to handle variants. On a basic level, CM provides a way to manage groups of artifacts and their versions. Versions can be split to support different variants and merged in order to deliver changes of requirements back to accepted releases. With CM, users can efficiently create different streams to help manage parallel versions without the need to make copies of their requirements. Additionally, there is robust linking that is specific to the stream the user is working in. This means that following the link shows users the requirement, test, or design element on the other end of the link that is appropriate for the stream they are working in. The ability to compare across streams and baselines helps users to understand how their versions or variants are different and to correct any mistakes. The functionality has been designed expecting only a small number of people to engage directly with parallel versions and CM, while the rest of the engineers continue with their work, mostly as if nothing has changed. By default, these new capabilities are turned off and project administrators have to enable them for the server and for each project area where users want to take advantage of them. Global Configuration Management For some time we have been discussing the benefits of being able to support use cases such as \"link a version of a test case with a version of a requirement\". 6.0 extends CM with a federated approach to lifecycle information. Strategic reuse for complex systems and software is now possible for requirements (RDNG), design (RDM), test (RQM) and software development (RTC). Plan and manage the reuse of configurations in the many versions or variants of the product or software line. Define complex products and applications as hierarchies of components and reuse those components in multiple products and applications. Automatically handle links between artifacts when branching or delivering changes to another stream such as, links between tests and requirements. Create cross lifecycle baselines to support parallel development of multiple versions and variants, branching and merging, and change management. Diagram editor Users can create many types of diagrams to refine and communicate their ideas and to elaborate their requirements. The new diagram editor provides high-quality diagrams that are simple to create in all supported browsers without the need for a Java™ plug-in. The large selection of shapes and themed styles make it easy to create many types of diagrams with eye-catching color palettes. To increase productivity, users can use the keyboard to create all the diagrams. They can comment on and create links between individual diagram elements and other artifacts. Diagrams can be added to modules and embedded in rich-text artifacts. See the Release Notes for further information! This release could well be introducing the most fundamental function into a requirements management tool since RM systems began. While we continue our drive with usability and productivity we are also providing support for requirements configuration management, built from the ground up as part of the native tool, rather than simply integrating to an external CM system."},{"title":"Jazz Team Blog  What's new in Rational Quality Manager 6.0?","tags":"ciandcd","url":"http://ciandcd.github.io/jazz-team-blog-whats-new-in-rational-quality-manager-60.html","text":"From: https://jazz.net/blog/index.php/2015/06/26/whats-new-in-rational-quality-manager-6-0/ IBM Rational Quality Manager (RQM) 6.0 brings both incremental improvements over v5 and a completely new dimension to managing test artifacts. On the incremental improvements front, the team has done a great job to continue to work closely with our customers to further improve the user experience on specific areas such as dashboard widgets or collaboration. At the same time, v6.0 introduces the support of configuration management for test artifacts, linked to other domains such as requirements or design, and contributing to the notion of global configurations across our Collaborative Lifecycle Management (CLM) solution. This version marks the beginning of a completely new set of capabilities that empower users to do parallel development and test, and to reuse artifacts in an effective way. Improved user experience Continuing on the work done in RQM 5.0.1 with the introduction of the Test Artifact dashboard widget, RQM 6.0 offers the new Test Statistics widget. Both of those widgets leverage the saved queries created in one of the test artifact views, such as the Test Case Execution Record view. Those live queries can now be reused with their results displayed directly on a dashboard. The Test Statistic widget offers the option to display the result in table format or in graphs such as bar charts, pie charts, or column charts. The team has also worked on improving collaboration support, specifically for concurrent modification of test artifacts. If two people modify the same test plan, suite, or case at the same time, the second person to attempt to save is now prompted with the option to merge the changes. Single sign-on authentication On top of the existing types of single sign-on (SSO) authentication already supported, this version introduces the Jazz Security Architecture SSO based on the OpenID Connect authentication protocol. The new Jazz Authorization Server simplifies the authentication administration. Read more in John Vasta's post about SSO options in CLM 6.0 . Reporting In v5, we introduced Jazz Reporting Service (JRS) as a new option to generate customizable analytics reports with a user-friendly web-based report builder. In v6.0, JRS is now included directly in the CLM package. This increases the availability for better reporting services for all of your applications without requiring the additional effort of organizing, installing, and configuring reporting capability through a separate download and install. There are also a number of new capabilities which make JRS the recommended analytics reporting solution for CLM, such as calculation and roll-up along with graphical report drill-through, new out-of-the-box reports, and interactive runtime filters in the dashboard widget. If you'd like to learn more, see Ernest Mah's post on reporting in CLM 6.0 . Configuration management RQM 6.0 is the first release that includes capabilities to aid in configuration management of test artifacts. On a basic level, configuration management enables users to better manage changes and to go back in time if needed through the creation of baselines and the ability to compare and merge. Users can create baselines to record a state in time of a Quality Management (QM) project area. Baselines are immutable and defined for an entire project area. Test artifacts of a baseline cannot be modified. Users can then compare the current state of a QM project area with any previous baseline. The comparison provides both a high-level view of the differences at the project area level as well as a detailed side-by-side comparison at the artifact level. Then, users have the opportunity to roll back some of the changes by merging a previous baseline into the current state and replacing some artifacts with the previous versions from that baseline. In a more advanced usage model, configuration management offers the ability for testers to work independently and in parallel on multiple versions or variants of test artifacts. Users can create parallel streams by branching from an existing baseline. Streams are versions of all the test artifacts of a project area that can be changed. Users can merge changes made in one stream into another by first looking at the differences and then replacing all or some of the current versions by the versions of the artifacts in the baseline. The local Quality Management configurations, both streams and baselines, can then be contributed to global configurations. Global configurations are cross-domain configurations that can link to multiple local configurations from the Requirements Management, Design Management, Software Configuration Management, and Quality Management applications. Global configurations are used to define a common context in which users can work and create all the deliverables for a given version or variant. Finally, by creating a hierarchy of global configurations, it is possible to manage composite product definitions and configurations that enable complex reuse scenarios at the subsystems and components levels. Learn more about RQM 6.0 For details of what's new in 6.0, check out New & Noteworthy in the Downloads section. See the Release Notes for a list of fixes. To see what's supported, see the System Requirements ."},{"title":"JetBrains 2014: The Year in Review","tags":"ciandcd","url":"http://ciandcd.github.io/jetbrains-2014-the-year-in-review.html","text":"From: http://blog.jetbrains.com/blog/2015/01/21/jetbrains-2014-the-year-in-review/ Our team was hard at work in 2014 delivering new releases of our existing products and bringing new ones to market. In this post we are going to take a look at those and some of the other biggest moments of the year. If you remember back to JetBrains Day @ FooCafé in September 2013, we announced several new projects. 2014 saw those plans come to fruition. In February, ReSharper C++ Early Access Program went live, in May Nitra was made open source , September brought CLion EAP , and in December Upsource , our repository browsing and code review tool, reached a stable 1.0 build. We didn't stop there. Two new products were also added to our portfolio: 0xDBE , JetBrains' brand new IDE for DBAs and SQL developers, was first announced in June and in October PyCharm Education Edition , a free IDE for learning and teaching programming with Python, went public. You may be interested in checking out our Interactive Python Programming Course Contest . In 2014 we continued our commitment to the open source community through our various projects and by providing free open source licenses to non-commercial OS software projects. In July a new open source Kotlin website went live and steady releases of JetBrains Meta Programming System (MPS) continued to go out the door. Travel again back in time to May 2013 when Google announced their selection of IntelliJ IDEA as the base of Android Studio . Well, in December, the highly anticipated Android Studio 1.0 release hit the virtual shelves! This is a great example of open source collaboration working both ways with the work being done on Android Studio being incorporated back into IntelliJ IDEA Ultimate Edition and the free and open source Community Edition . One of the proudest moments of the year came in September when we announced JetBrains Student License Program . Through this program students and teachers have access to our entire product line of IDEs and .NET Tools. Within the first two weeks of the program, more than 34,000 students were approved and now there are nearly 100,000 students using JetBrains tools for free! Nearly a decade and a half on we haven't forgotten our startup roots. In February we announced JetBrains Startup Discount Plan . Software startup businesses that meet straightforward criteria get a 50% discount on all of our products . If your startup is just getting off the ground, this is a great place to begin. Lastly, here are some of the honors that our products picked up in 2014 . 2014 was a fantastic year and we expect more of the same excitement in 2015. We sincerely thank you; all of our friends and colleagues, for your continued support and wish you all the best in the coming year. Here's to another outstanding and productive year in 2015!"},{"title":"JetBrains Night in Munich Recap, Raffle Winners and Recording","tags":"ciandcd","url":"http://ciandcd.github.io/jetbrains-night-in-munich-recap-raffle-winners-and-recording.html","text":"From: http://blog.jetbrains.com/blog/2015/04/29/jetbrains-night-in-munich-recap-raffle-winners-and-recording/ In March 2015, we announced an evening event at our JetBrains office in Munich where we would show our guests how to Use ReSharper Effectively, and Perform Exploratory Code Reviews with Upsource. The level of interest was so great that we decided to hold an additional night to accommodate the volume of demand. We could have filled a much larger venue, but we wanted to provide an opportunity to mingle with the team in a relaxed informal atmosphere. In hindsight, this was a good decision. Over two nights, March 24th and 25th, 120 participants gathered at JetBrains office to see in action Upsource and ReSharper Ultimate (ReSharper, dotTrace, dotMemory and dotCover). The feedback that we received on location and through a follow-up survey were overwhelmingly positive, so much so that we are currently planning to extend the same concept with 3 hour in-depth workshops . JetBrains, and our Munich team in particular, would like to thank all of the participants for their time, great conversations and overall positive sentiment that contributed to making the evenings successful. As promised, today we are announcing the winners of our free personal license raffle , along with their product of choice: Maik Heller – dotCover Michael Baur – IntelliJ IDEA Lastly, we would like to share the recorded session featuring Matt Ellis ( @citizenmatt ), Using ReSharper Effectively. Enjoy the video and we hope to meet you at an upcoming event near you! Develop with Pleasure! - The JetBrains Team"},{"title":"Live Webinar: Reactive Stream Processing with Akka Streams","tags":"ciandcd","url":"http://ciandcd.github.io/live-webinar-reactive-stream-processing-with-akka-streams.html","text":"From: http://blog.jetbrains.com/blog/2015/01/14/live-webinar-reactive-stream-processing-with-akka-streams/ We are pleased to invite you to our upcoming webinar, Reactive Stream Processing with Akka Streams , featuring Konrad Malawski of Typesafe. Register now and join us Tuesday, January 27th, 15:00 – 16:00 GMT (10:00 AM – 11:00 AM EST). In this webinar, Konrad will give an overview of the Reactive Streams specification (with teams from Netflix, Pivotal, RedHat, Typesafe and the others), the issues it addresses and how all the implementations aim to consolidate on a shared streaming protocol. Also, you'll learn how to use Akka Streams for working with streaming data in an asynchronous type-safe and back-pressured manner. Hurry up and register now , space is limited! About the presenter Konrad Malawski is a late-night passionate dev living by the motto, \"Life is Study!\", hacking on the Akka toolkit at Typesafe. While working on Akka Streams he also implemented the Reactive Streams specifications Technology Compatibility Kit. You can follow him on Twitter – is a late-night passionate dev living by the motto, \"Life is Study!\", hacking on the Akka toolkit at Typesafe. While working on Akka Streams he also implemented the Reactive Streams specifications Technology Compatibility Kit. You can follow him on Twitter – @ktosopl Follow IntelliJ IDEA on our blog , Twitter @IntellIJIDEA , and our product pages ."},{"title":"Live Webinar: Software Architecture as Code, February 12th","tags":"ciandcd","url":"http://ciandcd.github.io/live-webinar-software-architecture-as-code-february-12th.html","text":"From: http://blog.jetbrains.com/blog/2015/01/29/live-webinar-software-architecture-as-code-february-12th/ We are pleased to invite you to our upcoming webinar, Software Architecture as Code , featuring Simon Brown. Register now and join us Thursday, February 12th, 15:00 – 16:00 GMT (10:00 AM – 11:00 AM EST). It's 2015 and with so much technology at our disposal, we're still manually drawing software architecture diagrams in tools like Microsoft Visio. Furthermore, these diagrams often don't reflect the implementation in code, and vice versa. This session will look at why this happens and how to resolve the conflict between software architecture and code through the use of architecturally-evident coding styles and the representation of software architecture models as code. Space is limited; learn more and register now . Simon Brown is an independent consultant and helps organizations to build better software by adopting a lightweight, pragmatic approach to software architecture. He is the creator of the C4 software architecture model and the author of \"Software Architecture for Developers,\" a developer-friendly guide to software architecture, technical leadership and the balance with agility. is an independent consultant and helps organizations to build better software by adopting a lightweight, pragmatic approach to software architecture. He is the creator of the C4 software architecture model and the author of \"Software Architecture for Developers,\" a developer-friendly guide to software architecture, technical leadership and the balance with agility."},{"title":"Mocking Multiple Interfaces","tags":"ciandcd","url":"http://ciandcd.github.io/mocking-multiple-interfaces.html","text":"From: https://www.finalbuilder.com/resources/blogs/postid/716/mocking-multiple-interfaces-delphi-mocks Today we updated Delphi Mocks to enable the Mocking of multiple interfaces. This is useful when the interface you wish to Mock is cast to another interface during testing. For example you could have the following system you wish to test. type {$M+} IVisitor = interface; IElement = interface ['{A2F4744E-7ED3-4DE3-B1E4-5D6C256ACBF0}'] procedure Accept(const AVisitor : IVisitor); end; IVisitor = interface ['{0D150F9C-909A-413E-B29E-4B869C6BC309}'] procedure Visit(const AElement : IElement); end; IProject = interface ['{807AF964-E937-4A8A-A3D2-34074EF66EE8}'] procedure Save; function IsDirty : boolean; end; TProject = class(TInterfacedObject, IProject, IElement) protected function IsDirty : boolean; procedure Accept(const AVisitor : IVisitor); public procedure Save; end; TProjectSaveCheck = class(TInterfacedObject, IVisitor) public procedure Visit(const AElement : IElement); end; {$M-} implementation { TProjectSaveCheck } procedure TProjectSaveCheck.Visit(const AElement: IElement); var project : IProject; begin if not Supports(AElement, IProject, project) then raise Exception.Create('Element passed to Visit was not an IProject.'); if project.IsDirty then project.Save; end; The trouble previously was that when testing TProjectSaveCheck a TMock<IElement> would be required, as well as a TMock<IProject>. This is brought about by the Visit procedure requiring the IElement its passed to be an IProject for the work its going to perform. This is now very simple with the Implement<I> method available off TMock<T>. For example to test that Save is called when IsDirty returns true, the following test could be written; procedure TExample_InterfaceImplementTests.Implement_Multiple_Interfaces; var visitorSUT : IVisitor; mockElement : TMock<IElement>; begin //Test that when we visit a project, and its dirty, we save. //CREATE - The visitor system under test. visitorSUT := TProjectSaveCheck.Create; //CREATE - Element mock we require. mockElement := TMock<IElement>.Create; //SETUP - Add the IProject interface as an implementation for the mock mockElement.Implement<IProject>; //SETUP - Mock project will show as dirty and will expect to be saved. mockElement.Setup<IProject>.WillReturn(true).When.IsDirty; mockElement.Setup<IProject>.Expect.Once.When.Save; //TEST - Visit the mock element to see if our test works. visitorSUT.Visit(mockElement); //VERIFY - Make sure that save was indeed called. mockElement.VerifyAll; end; The Mock mockElement \"implements\" two interfaces IElement, and IProject. IElement is done via the constructor, and IProject is added through the Implement<I> call. The Implement<I> call adds another sub proxy to the mock object. This sub proxy then allows all the mocking functionality to be performed with the IProject interface. To access the Setup, and Expects behaviour there are overloaded generic calls on TMock. These return the correct proxy to interact with, and generic type ISetup<I> and IExpect<I>. This is seen in the call to mockElement.Setup<IProject>. This returns a ISetup<IProject> which allows definition of what should occur when IProject is used from the Mock. This feature is really useful when there is a great deal of casting of interfaces done in the system you wish to test. It can save having to mock base classes directly where multiple interfaces are implemented. The way this works under the hood is fairly straight forward. TVirtualInterfaces are used when an interface is required to be mocked. This allows the capturing of method calls, and the creation of the interface instance when its required. The Implement<I> functionality simply extends this so that when a TProxyVirtualInterface (inherited from TVirtualInterface) has QueryInterface called it also looks to its owning Proxy. If any other Proxies implement the requested interface its that TProxyVirtualInterface which is returned. In essence this allows us to fake the Mock implementing multiple interfaces, when in fact there are a list of TVirtualInterface's all implementing a single interface."},{"title":"Plugin Settings","tags":"ciandcd","url":"http://ciandcd.github.io/plugin-settings.html","text":"From: http://www.go.cd/2015/06/18/plugin-settings.html Go is continously improving its plugin infrastructure. Starting 15.2.0 Go will support \"Plugin Settings\" that will allow plugins developers to accept global settings. Currently these configurations had to be supported via system properties or a file that is in specified format in a specified location, which makes it a little haphazard. With this feature \"all\" plugins will have one approach to accept plugins settings from user & access plugin settings from Go Server. How does it work? On plugin listing page users will see a gear icon (similar to one on the pipeline dashboard) for the plugins that accept plugin settings. Figure 1: Plugin listing with gear icon (Click to enlarge) Clicking on the gear icon opens a pop-up that renders \"Plugin Settings\" template that is provided by the plugin. Figure 2: Configure plugin pop-up (Click to enlarge) On \"Save\" the user inputs are validated by plugin. Figure 3: Configure plugin pop-up with errors (Click to enlarge) We hope plugin developers are able to use this feature to provide a better experience to their users. References: As always, Go questions can be asked on the mailing list ."},{"title":"Refuge for Automated Build Studio Users","tags":"ciandcd","url":"http://ciandcd.github.io/refuge-for-automated-build-studio-users.html","text":"From: https://www.finalbuilder.com/resources/blogs/postid/708/refuge-for-automated-build-studio-users Refuge for Automated Build Studio Users By Vincent Parrett On April 13, 2014 SmartBear recently discontinued development of Automated Build Studio and deleted pretty much all references to it from their website. Within hours of the smartbear email going out to ABS users, we started getting questions about crossgrade discounts, and we're more than happy to help. If you are an ABS user, contact us (sales @ finalbuilder.com ) with proof of purchase (invoice) for ABS and we'll provide you with a 50% off discount coupon for FinalBuilder 7 Professional Edition. This is a limited time once only offer, valid until May 15th 2014. Spread the word to your fellow ABS users. While you you are checking out FinalBuilder, be sure to take a look at our Continuous Integration Server product, Continua CI. It's vastly superior to the CI features in ABS, but still allows you to create your build process using a visual build tool (FinalBuilder)."},{"title":"Sample Go CD Virtualbox based environment","tags":"ciandcd","url":"http://ciandcd.github.io/sample-go-cd-virtualbox-based-environment.html","text":"From: http://www.go.cd/2014/09/09/Go-Sample-Virtualbox.html If you're interested in checking out Go but don't want to spend the time automating your own system, this might be a great option for you. Edit on 11 November, 2014 - This box has been updated to Go version 14.3. For information about what's new in this release please see http://www.go.cd/releases/ We've created an environment using Vagrant and Virtualbox. Once it's up, you'll have a full Go installation including several example pipleines. System Requirements In order to run this you'll need Virtualbox and Vagrant . Both of these are available for most operating systems. Using the box To get started, open a command prompt in an empty directory and type... vagrant init gocd/gocd-demo This will create a file called Vagrantfile in your current directory. Next, type... vagrant up Completion of this (especially the first time) will take quite a while, depending on your bandwidth. Vagrant will be downloading the full box image (almost 1.4GB) from Vagrantcloud while you wait. Note: If you have an existing Go installation on the same machine as this virtual machine you may get a port conflict. Vagrant will automatically map to a new port which will be shown in the startup messages. After a few minutes, you should be able to navigate to http://localhost:8153/ on your local machine and see the following... These pipelines are all related, as shown in the following value stream map screenshot... Feel free to play around with the installation and see how everything works. You can always reset the box to it's orginal state if you need to! What's on the machine? The box will be updated as new things come out, but as of this writing... Go 14.3 Server Go 14.3 Agent 3 very small PHP applications Basic Capistrano deployment scripts Local Git repo using Gitolite to manage permissions A couple simple phpunit tests A couple simple watir scripts All of the code is on the Virtualbox machine at /home/vagrant/projects. The easiest way to access this is to type 'vagrant ssh' at the command prompt in the same place you started the machine. The hope is that using this box you can see how real applications (even if they are small) are built, tested and deployed with Go. As always, Go questions can be asked at https://groups.google.com/forum/#!forum/go-cd"},{"title":"Stopping support for Java","tags":"ciandcd","url":"http://ciandcd.github.io/stopping-support-for-java.html","text":"From: http://www.go.cd/2014/07/09/stopping-support-for-java-jdk-6.html There was a recommendation , in April 2013, that all users move their Go Server and Go Agent installations to Java 7. Oracle and OpenJDK no longer support Java 6. So it is time for Go to stop supporting it. 14.2.0 will be last Go release which will work with Java 6. Any new Go release beyond 14.2.0 might not work with Java 6. If you have not already moved to Java 7, we request you to do so. Should you face any issues please do write to the community mailing list ."},{"title":"Upcoming API Changes","tags":"ciandcd","url":"http://ciandcd.github.io/upcoming-api-changes.html","text":"From: http://www.go.cd/2015/06/17/Upcoming-API-Changes.html With the upcoming release of Go 15.2, we'd like to begin unifying and improving some of the existing APIs that Go supports. Go's APIs are fairly old, have inconsistent and unpredictable content types (csv, xml, json, plain text). Going forward, we would like to announce an ongoing effort to improve these APIs to use something that is more modern, easy to discover, learn and build API clients for. We would be using the JSON HAL specification . Our API guidelines are published on our RFC . This will give us the opportunity to leverage Ruby and Rails to build these APIs, which should make it easier to incrementally iterate through and improve existing APIs to bring them to parity with our new guidelines. We welcome any feedback to improve our guidelines and contributions to improve existing APIs."},{"title":"Using Skip and Promote Conditions in Continua CI 1.5","tags":"ciandcd","url":"http://ciandcd.github.io/using-skip-and-promote-conditions-in-continua-ci-15.html","text":"From: https://www.finalbuilder.com/resources/blogs/postid/713/using-skip-and-promote-conditions-in-continua-ci-15 Skip Conditions Skip Conditions allow you to controll whether a Stage is skipped or run based on expressions. All the expressions must evaluate to true for the stage to run (if there are no expressions then the stage will not be skipped). In the above example, we have a Stage called Obfuscate, and we want it to be skipped if you turn off obfuscation (by setting a variable) or if we are not deploying a build (again, controlled by a variable). You can also disable a stage completely so it is always skipped. Promote Conditions In Continua CI 1.0, you can chose if the next Stage is automatically run, or the builds stops and requires a manual promotion to continue to the next Stage. In Continua CI 1.5, Promote Conditions allow you control whether to automatically promote or not, based on expressions. All the expressions must evaluate to true for the build to continue to the next stage (if there are no expressions then the build will stop with a status of waiting for promotion). In the above example, our build will continue on to the next Stage if the Deploy variable is set to true. Continua CI 1.5 is currently in Beta - you can get it here : http://www.finalbuilder.com/downloads/continuaci/continuaci-beta-version-history One of the most asked for features in Continua CI 1.0 was the ability to control which stages run, ie the ability to skip stages dynamically, based on what happened earlier in the build, and to be able to control whether the build should continue on to the next stage or wait for user intervention. In Continua CI 1.5, we made this possible with Skip and Promote Conditions.Skip Conditions allow you to controll whether a Stage is skipped or run based on expressions. All the expressions must evaluate to true for the stage to run (if there are no expressions then the stage will not be skipped).In the above example, we have a Stage called Obfuscate, and we want it to be skipped if you turn off obfuscation (by setting a variable) or if we are not deploying a build (again, controlled by a variable). You can also disable a stage completely so it is always skipped.In Continua CI 1.0, you can chose if the next Stage is automatically run, or the builds stops and requires a manual promotion to continue to the next Stage. In Continua CI 1.5, Promote Conditions allow you control whether to automatically promote or not, based on expressions.In the above example, our build will continue on to the next Stage if the Deploy variable is set to true.Continua CI 1.5 is currently in Beta - you can get it here :"},{"title":"Using Windows PowerShell tasks","tags":"ciandcd","url":"http://ciandcd.github.io/using-windows-powershell-tasks.html","text":"From: http://www.go.cd/2015/06/13/using-windows-powershell-tasks.html Some things to be aware of when using Windows PowerShell tasks. Go Agent default installation The default installation of a Go agent will use a 32-bit JRE unless you indicate otherwise. This JRE is embedded in the Go agent installer. If you want to use an alternative JRE (must satisfy Go's JRE requirements) after the initial installation, you can alter the \"wrapper.java.command\" key's value in the [InstallDirectory]\\config\\wrapper-agent.conf file to point to a different JRE. You will then need to restart the Go agent service to start using the alternative JRE. The [InstallDirectory] refers to the Go agents installation directory which by default is \"C:\\Program Files (x86)\\Go Agent\" . Pre-requisites for running PowerShell task commands You can only run on Windows based agents You should tag the agents if your are also using linux agents You probably want to ensure your agents all have the same version of PowerShell 32-bit Go agent If you are running a default Go agent installation then you will be running a 32-bit JRE. The 32-bit JRE will try to run PowerShell tasks in the 32-bit version of PowerShell, even if you give the full path to the 64-bit PowerShell executable in the task. If you need to execute a PowerShell script then you will need to alter the execution policy as follows: Open 32-bit version of PowerShell as an administrator: Start -> All Programs -> Accessories -> Windows Powershell -> Windows Powershell (x86) and type: # Alter execution policy set-executionpolicy remotesigned -force This will allow you to run local scripts on the Windows Go agent box. 64-bit Go agent If you are running a Go agent using a 64-bit JRE, it will run PowerShell tasks in the 64-bit version of PowerShell. If you need to execute a PowerShell script, then you will need to alter the execution policy as follows: Open 64-bit version of PowerShell as an administrator: Start -> All Programs -> Accessories -> Windows Powershell -> Windows Powershell and type: # Alter execution policy set-executionpolicy remotesigned -force This will allow you to run local scripts on the Windows Go agent box. PowerShell task commands You can configure the task as follows: command: powershell arg: .\\run.ps1 arg1value This assumes that the run.ps1 script is in the task's working directory. If you create the run.ps1 file with the following content you can see details of the execution context in the console log for the pipeline: param ( [string] $arg1 ) write-host \"Script: \" $MyInvocation . MyCommand . Path write-host \"Pid: \" $pid write-host \"Host.Version: \" $host . version write-host \"Is 64-bit process: \" $( [Environment] :: Is64BitProcess ) write-host \"Execution policy: \" $( get-executionpolicy ) write-host \"Arg1: \" $arg1 Propagating failures You need to ensure that PowerShell exits with an exit code that is not 0 in the event of a failure, this needs to cater to: Script errors External process calls that indicate failure You will need to decide how to handle these failures and if they should indicate the PowerShell task has been successful or not. This may mean that some script errors and external process calls failing is okay in your context. The following script demonstrates a strategy I use where I exit with a non zero code if any script error was encountered or an external process call fails: set-strictmode -version latest $ErrorActionPreference = 'Stop' function execute-externaltool ( [string] $context , [scriptblock] $actionBlock ) { # This function exists to check the exit code for the external tool called within the script block, so we don't have to do this for each call & $actionBlock if ( $LastExitCode -gt 0 ) { throw \"$context : External tool call failed\" } } try { write-host \"Script: \" $MyInvocation . MyCommand . Path write-host \"Pid: \" $pid write-host \"Host.Version: \" $host . version write-host \"Execution policy: \" $( get-executionpolicy ) # Query a service that does not exist, sc.exe will return with a non 0 exit code execute-externaltool \"Query a non existent service, will return with exit code != 0\" { c : \\ windows \\ system32 \\ sc . exe query service_does_not_exist } } catch { write-host \"$pid : Error caught - $_\" if ($? -and ( test-path variable : LastExitCode ) -and ( $LastExitCode -gt 0 )) { exit $LastExitCode } else { exit 1 } } This script uses a try catch block to handle all errors The $? and $LastExitCode caters to both script and external process calls We fall back on an exit code of 1 if we do not have an external process exit code This script uses an execute-externaltool function which takes a script block argument The script will invoke the script block It will then check for a non zero exit code (Assumes the script block just calls an external process), if so it will throw an exception. See also PowerShell execution policy Bypassing PowerShell execution policy Setting execution policy directly in the registry Go PowerShell runner plugin - I believe it can only be configured on Windows based Go servers About the author This is a guest post by Pat Mc Grath. You can find Pat on GitHub ."},{"title":"Webinar Recording: Reactive Stream Processing with Akka Streams","tags":"ciandcd","url":"http://ciandcd.github.io/webinar-recording-reactive-stream-processing-with-akka-streams.html","text":"From: http://blog.jetbrains.com/blog/2015/01/29/webinar-recording-reactive-stream-processing-with-akka-streams/ On Tuesday we had the pleasure to host a webinar together with Typesafe where Konrad Malawski, a Scala enthusiast who works on the Akka toolkit, gave a very comprehensive overview of the Reactive Streams specification and one of its implementations — Akka Streams . The slides from Konrad's presentation can be found at SlideShare . About the presenter Konrad Malawski is a late-night passionate dev living by the motto, \"Life is Study!\", hacking on the Akka toolkit at Typesafe. While working on Akka Streams he also implemented the Reactive Streams specifications Technology Compatibility Kit. You can follow him on Twitter – is a late-night passionate dev living by the motto, \"Life is Study!\", hacking on the Akka toolkit at Typesafe. While working on Akka Streams he also implemented the Reactive Streams specifications Technology Compatibility Kit. You can follow him on Twitter – @ktosopl Develop with Pleasure!"},{"title":"Webinar Recording: What's New in TeamCity 9","tags":"ciandcd","url":"http://ciandcd.github.io/webinar-recording-whats-new-in-teamcity-9.html","text":"From: http://blog.jetbrains.com/blog/2015/01/26/webinar-recording-whats-new-in-teamcity-9/ The recording of our recent webinar with Wes Higbee, What's New in TeamCity 9 , is now available on JetBrains YouTube Channel . In this webinar, Wes goes over the new features of TeamCity 9, namely: rearranging projects with Project Import; storing settings in VCS; creating and editing Custom Charts; running builds in Cloud Agents; as well as some other improvements. Below are a selection of some of the most frequently asked questions. Versioned Settings: Q: When is this feature going to be available with Perforce? A: We're considering support for Subversion and Perforce in TeamCity 9.1, but can't make any guarantees at this point. Q: What about TFS? A: TFS might be supported in the future but after Subversion and Perforce. Q: Given your VCS support priorities for new features, are Git, Mercurial and TFS your recommended VCS combinations with TeamCity? A: Apart from storing settings in VCS, Git, Mercurial, Subversion and Perforce, they are all supported greatly with TFS catching up. CVS, Vault and ClearCase support can be a bit limited. Q: How does TeamCtiy handle it if somebody corrupts the configuration in the repository? A: If there are errors while applying changes then TeamCity will not change project settings and will show an error. Q: Is it possible to use branches, e.g. to test a change in the build configuration before making it productive? A: Not yet, but we definitely want to add this feature. Q: How does the VCS know which branch to use? A: It uses default branch specified in VCS root. Q: If you want to revert to an earlier version, you need to find the desired version from within the version control system. That is, there's no GUI way to see the previous version from TeamCity perspective? A: There is a changelog tab where you can see all the changes. Q: What about parameters? Are they synced too? If not, the new features could replace templates, couldn't they? A: All the settings are synced form the VCS as if you edit the settings right in the TeamCity data directory (or change them in UI). Q: If a have sync on the root project, are all sub-projects synced too? Is it recommended to have VCS for the sub projects, or a VCS for the root project only? Or are both recommended? A: By default sub projects will be placed into the same version control. If you don't want it for some projects, you can disable synchronization in them. Project Import Q: Can I import a TC8 backup in TC9? It would make testing easier A: No, both servers should have the same version. 9.0 and 9.0.x are compatible, but 8.1 and 9.0 not. Cloud Agents Q: How does that work out with the licensing system? I guess the number of agents licensed its the hard limit of active agents, including from the cloud? A: Yes. The total number of agents (real and virtual) connected at any given time should not exceed the total agent licenses limit. When there are enough agent licences, TeamCity automatically authorizes new cloud agents and unauthorizes the stopped ones. Q: How do we create the VM image?Is a build image template available for the Azure VM image? A: There are no templates with TeamCity agents for now Q: Can we configure the \"on-demand\" agents to use the same VM with more than one agent, until that max out? i.e. 3 agents per Azure VM. A: TeamCity assumes there is a single agent per instance. The recommended setup is to have a single agent per machine. Q: Can the on-demand agents be used with on-premise vCenter? A: Yes we have vSphere plugin doing the same. Q: Does the agent spun up hang around for a while or is it immediately removed? A: There is an idle timeout setting to shutdown the agent instance after. When stopped, the agent is deleted from TeamCity list of agents. Q: EC2 cloud agents auto-shutdown policy is not the best it could be meaning that it doesn't take into account that you always get charged for an hour of usage of an EC2 instance. Any plans to improve this? A: Yes, we do have plans to adjust the shut down to the hour limit. You are welcome to vote for https://youtrack.jetbrains.com/issue/TW-9680 . Custom Charts Q: Is it possible to have time instead of build number on the X axis? A: No, this is not possible Q: I have a configuration that has different snapshot dependencies, is there any way to show the build time from beginning to end, meaning when the first dependency run to the end? A: No, statistics charts operate per build. Theoretically you can calculate this time and report it as statistic value in the last build of the chain. Q: Can you export the stats data? A: Yes, there is a \"download\" action on the chart. Also available via REST API. Q: Does TeamCity have possibility to show charts with time that took to run every test? A: On the build's Tests tab, you can see the duration chart for each test. As well on the Test history page. Wes Higbee is passionate about helping companies achieve remarkable results with technology and software. He's had extensive experience developing software and working with teams to improve how software is developed to meet business objectives. Wes launched Full City Tech to leverage his expertise to help companies rapidly deliver high quality software to delight customers. He has a strong background in using Continuous Integration with TeamCity to bring quality to the table. is passionate about helping companies achieve remarkable results with technology and software. He's had extensive experience developing software and working with teams to improve how software is developed to meet business objectives. Wes launchedto leverage his expertise to help companies rapidly deliver high quality software to delight customers. He has a strong background in using Continuous Integration with TeamCity to bring quality to the table. Follow TeamCity updates on our blog , Twitter @TeamCity , and our product pages ."},{"title":"Adding custom reports to Continua CI Build Results","tags":"ciandcd","url":"http://ciandcd.github.io/adding-custom-reports-to-continua-ci-build-results.html","text":"From: https://www.finalbuilder.com/resources/blogs/postid/701/adding-custom-reports-to-continua-ci-build-results It's not uncommon for tools run during a build process to create log files, xml or html files etc that you might want to keep, in other words Artifacts . Continua CI already has a mechanism for registering Artifacts, which enables them to be viewed/downloaded from the Build Artifacts page. Continua CI also has another way of viewing those files, Reports. Reports are viewed in an iframe, which allows you to stay within the Continua CI UI, but still naviate within the report files. A typical use of the Report feature is showing exported FinalBuilder html logs, or Code Coverate html reports (for example those produced by OpenCover). Defining a Report Defining a report is relatively simple. Just point it at a file you expect to appear in the Builds Workspace on the Server. In this case we're expecting our build process to place a file named FinalBuilderReport.html in the $Workspace$\\Reports folder. $Workspace$ will be replaced at run time with the builds workspace path on the server. The next step is to make sure that the file actually gets copied to where we said it would be. We define a Workspace Rule to copy the file back to the build workspace on the server when the Stage completes. If your tool generates .css and image files then don't forget to add rules to copy those files into place too. The final step is to make sure that the report file is actually produced. In this example, we are using an exported FinalBuilder Log file, which is created in $Workspace$\\Output\\FB7 - we tell FinalBuilder where the workspace is by setting a FinalBuilder variable (along with a buch of other stuff like version numbering etc) in the FinalBuilder Action . Viewing the Report All thats left to do now is run the build, and view the report, If everything went to plan, then when you click on the builds Report Tab, you should see something like this : Notice the Home/Back/Forward Buttons. If your report comprises of multiple pages, and all the links in the html files are relative, then you get full history support in the iframe. FWIW, Open Cover with ReportGenerator does just that. We'll take a look at using Open Cover with Continua CI in a future post."},{"title":"Adding NTLM SSO to Nancyfx","tags":"ciandcd","url":"http://ciandcd.github.io/adding-ntlm-sso-to-nancyfx.html","text":"From: https://www.finalbuilder.com/resources/blogs/postid/730/adding-ntlm-sso-to-nancyfx Nancyfx is a Lightweight, low-ceremony, framework for building HTTP based services on .Net and Mono. It's open source and available on github. Nancy supports Forms Authentication, Basic Authentication and Stateless Authentication \"out of the box\", and it's simple to configure. In my application, I wanted to be able to handle mixed Forms and NTLM Authentication, which is something nancyfx doesn't support. We have done this before with asp.net on IIS, and it was not a simple task, involving a child site with windows authentication enabled while the main site had forms (IIS doesn't allow both at the same time) and all sorts of redirection. It was painful to develop, and it's painful to install and maintain. Fortunately with Nancy and Owin , it's a lot simpler. Using Microsoft's implementation of the Owin spec, and Nancy's Owin support, it's actually quite easy, without the need for child websites and redirection etc. I'm not going to explain how to use Nancy or Owin here, just the part needed to hook up NTLM support. In my application, NTLM authentication is invoked by a button on the login page (\"Login using my windows account\") which causes a specific login url to be hit. We're using Owin for hosting rather than IIS and Owin enables us to get access to the HttpListener, so we can control the authentication scheme for each url. We do this by adding an AuthenticationSchemeSelectorDelegate. internal class Startup { public void Configuration(IAppBuilder app) { var listener = (HttpListener)app.Properties[\"System.Net.HttpListener\"]; //add a delegate to select the auth scheme based on the url listener.AuthenticationSchemeSelectorDelegate = request => { //the caller requests we try windows auth by hitting a specific url return request.RawUrl.Contains(\"loginwindows\") ? AuthenticationSchemes.IntegratedWindowsAuthentication : AuthenticationSchemes.Anonymous; } app.UseNancy(); } } What this achieves is to invoke the NTLM negotiation if the \"loginwindows\" url is hit on our nancy application. If the negotiation is successful (ie the browser supports NTLM and is able to identify the user), then the Owin environment will have the details of the user, and this is how we get those details out of Owin (in our bootstrapper class). protected override void ApplicationStartup(TinyIoCContainer container, IPipelines pipelines) { pipelines.BeforeRequest.AddItemToStartOfPipeline((ctx) => { if (ctx.Request.Path.Contains(\"loginwindows\")) { var env = ((IDictionary<string,>)ctx.Items[Nancy.Owin.NancyOwinHost.RequestEnvironmentKey]); var user = (IPrincipal)env[\"server.User\"]; if (user != null && user.Identity.IsAuthenticated) { //remove the cookie if someone tried sending one in a request! if (ctx.Request.Cookies.ContainsKey(\"IntegratedWindowsAuthentication\")) ctx.Request.Cookies.Remove(\"IntegratedWindowsAuthentication\"); //Add the user as a cooking on the request object, so that Nancy can see it. ctx.Request.Cookies.Add(\"IntegratedWindowsAuthentication\", user.Identity.Name); } } return null;//ensures normal processing continues. }); } Note we are adding the user in a cookie on the nancy Request object, which might seem a strange thing to do, but it was the only way I could find to add something to the request that can be accessed inside a nancy module, because everything else on the request object is read only. We don't send this cookie back to the user. So with that done, all that remains is the use that user in our login module Post[\"/loginwindows\"] = parameters => { string domainUser = \"\"; if (this.Request.Cookies.TryGetValue(\"IntegratedWindowsAuthentication\",out domainUser)) { //Now we can check if the user is allowed access to the application and if so, add //our forms auth cookie to the response. ... } } Of course, this will probably only work on Windows, not sure what the current status is for System.Net.HttpListener is on Mono. This code was tested with Nancyfx 1.2 from nuget. What this achieves is to invoke the NTLM negotiation if the \"loginwindows\" url is hit on our nancy application. If the negotiation is successful (ie the browser supports NTLM and is able to identify the user), then the Owin environment will have the details of the user, and this is how we get those details out of Owin (in our bootstrapper class).Note we are adding the user in a cookie on the nancy Request object, which might seem a strange thing to do, but it was the only way I could find to add something to the request that can be accessed inside a nancy module, because everything else on the request object is read only. We don't send this cookie back to the user. So with that done, all that remains is the use that user in our login moduleOf course, this will probably only work on Windows, not sure what the current status is for System.Net.HttpListener is on Mono. This code was tested with Nancyfx 1.2 from nuget."},{"title":"Authentication end-point","tags":"ciandcd","url":"http://ciandcd.github.io/authentication-end-point.html","text":"From: http://www.go.cd/2015/06/18/authentication-end-point.html Starting 15.2.0 Go Server will expose authentication end-point. What this means is Go users can add \"custom\" authentication schemes through plugins. With plugin settings & web request handling ability plugin developers get enough flexibility to write any authentication plugin they intend to write. Examples of integrations possible: OAuth Login - GitHub , Google , Hotmail, Yahoo! etc. Single Sign-on (SSO) - LDAP, Okta etc. 2-factor authentication - SMS verification etc. Custom username & password authentication How does it work? Below is an explanation of how GitHub OAuth Login plugin works. Generate OAuth token on GitHub. Figure 1: Generate oauth token (Click to enlarge) On plugin listing page users will see a gear icon (similar to one on the pipeline dashboard). Figure 1: Plugin listing with gear icon (Click to enlarge) Clicking on the gear icon opens a pop-up that renders \"Plugin Settings\". Figure 2: Configure plugin pop-up (Click to enlarge) Login Page Figure 3: Login Page with GitHub icon (Click to enlarge) Click on GitHub icon Figure 3: Authorize Go Server to access GitHub (Click to enlarge) Successful login Figure 3: On successful login (Click to enlarge) Ability to Search & Add users Figure 3: Search User (Click to enlarge) We hope plugin developers are able to use this feature to support their organizations authentication mechanism. References: As always, Go questions can be asked on the mailing list ."},{"title":"Automated Builds vs Continuous Integration","tags":"ciandcd","url":"http://ciandcd.github.io/automated-builds-vs-continuous-integration.html","text":"From: https://www.finalbuilder.com/resources/blogs/postid/703/automated-builds-vs-continuous-integration Roy Osherove posted an interesting video on youtube recently, talking about the difference between Automated Builds and Continuous Integration. This is a subject that comes up often in emails from existing and potential customers. Why do I need FinalBuilder if I have Continua CI, or, why haven't we added all the functionality of FinalBuilder to Continua CI? Roy sums up the differences and reasoning in this video quite nicely. Roy goes into more detail in his \"newish\" book , Beautiful Builds . Lots of interesting food for thought. Lets face it, most developers probably don't spend too much time \"deep thinking\" how the build side of things should be done. Roy's book is full of \"deep thinking\", but Roy sums it all up quite nicely! It's not a long book, around 40 pages and definitely worth a read. FinalBuilder and Continua CI The workflow functionality in Continua CI is inspired by FinalBuilder (which is probably stating the obvious to FinalBuilder users), but the functionality that is there is fairly high level. Your Continua CI Stage workflow can be as simple as a single action (for example a FinalBuilder Action or MSBuild, or Rake or Powershell....), or you can make use of the flow control, logic, file operation actions etc and keep your build scripts as simple as your .sln file (or .dproj for delphi developers). That's for you to decide. Of course our recommendation is to use FinalBuilder (and I say \"of course\" because I have wages to pay!) but there are obvious benefits to using FinalBuilder for your automated build scripts. Develop and debug your build scripts on your machine Using FinalBuilder, you can develop and debug your build script on your machine. FinalBuilder allows you to step through the build, set breakpoints, see what is happening, how variables are changing etc. The value of this shouldn't be underestimated; developing a FinalBuilder script is fast, easy and provides immediate feedback. No Web based CI Server (is there any other kind?) can give you this level of immediate feedback. This also has the added benefit in that the FinalBuilder project will be runnable on other developers machines (assuming they have the required tools installed) All of your build script is versioned with your source code. If all of your build script functionality is in a FinalBuilder script, and that script is checked into version control (alongside your source code), then if the structure of your source code changes, so can your FinalBuilder script. The CI server will checkout the correct version of the build script for the version of source code it's checking out. Better Integration We're currently working on better integration between Continua CI and FinalBuilder. Some of our FinalBuilder Server customers have complained that they lose functionality when moving to Continua CI. We understand that, FinalBuilder Server and FinalBuilder are tightly coupled. When we started working on Continua CI, we made a conscious decision to not do that. That means Continua CI doesn't know about the internals of a FinalBuilder project, doesn't know what variables are declared. I still stand by that decision; it made Continua CI a better product. That said, there is more we can do to improve how the two products work together. This work will show up in an update over the coming weeks (when it's ready!)."},{"title":"Automated UI Testing Done Right with ContinuaCI","tags":"ciandcd","url":"http://ciandcd.github.io/automated-ui-testing-done-right-with-continuaci.html","text":"From: https://www.finalbuilder.com/resources/blogs/postid/709/automated-ui-testing-done-right-with-continuaci Automated UI Testing Done Right with ContinuaCI You have just completed an awesomely complex change to your shinny new webapp! After running all your unit tests things are in the green and looking clean. Very satisfied at the quality of your work you fire up the application to verify that everything is still working as advertised. Below is what greets you on the root path of your app: We have all been here at some time or another! What happened! Perhaps it was not your code that broke it! Maybe the issue originated from another part of your organisation, or maybe it came from somewhere on the \"inter-webs\". Its time to look at the underlying problem however ..... testing web user interfaces is hard! Its time consuming and difficult to get right. Manual clicks, much typing, cross referencing client specifications etc, surely there must be an easier way. At the end of the day we DO need to test our user interfaces Automated Web UI Testing Thankfully UI testing today can be Automated, running real browsers in real end to end functional tests, to ensure our results meet (and continue to meet) expectations. You have just completed an awesomely complex change to your shinny new webapp! After running all your unit tests things are in the green and looking clean.Very satisfied at the quality of your work you fire up the application to verify that everything is still working as advertised. Below is what greets you on the root path of your app:We have all been here at some time or another! What happened! Perhaps it was not your code that broke it! Maybe the issue originated from another part of your organisation, or maybe it came from somewhere on the \"inter-webs\".Its time to look at the underlying problem however ..... testing web user interfaces is hard! Its time consuming and difficult to get right. Manual clicks, much typing, cross referencing client specifications etc, surely there must be an easier way. At the end of the day we DO need to test our user interfacesThankfully UI testing today can be Automated, running real browsers in real end to end functional tests, to ensure our results meet (and continue to meet) expectations. For the sake of brevity and clarity in this demonstration we will focus on testing an existing endpoint. It is considered common place to find functional tests included as part of a wider build pipeline, which may consist of such steps as: Build Unit Test Deploy to Test Environment Perform Functional Tests Deploy to Production In this article we will be focusing on the functional testing component of this pipeline. We will proceed on the assumption that your code has already been, built unit tested and deployed to a Functional Test environment. Today we will; Add Automated UI testing to an existing endpoint google.com Configure ContinuaCI to automatically build our project, and perform the tests Software Requirements: Visual Studio 2010 Express Edition SP1 or greater ( visualstudio.com ) Microsoft Dot Net Framework version 4 or greater Java JRE ( http://www.oracle.com/technetwork/java/javase/downloads/index.html ) Mercurial ( mercurial.selenic.com ) Mozilla Firefox ( getfirefox.com ) Nuget ( docs.nuget.org/docs/start-here/installing-nuget ) Step1: Prepare a Selenium endpoint Firstly we will prepare for our UI tests by setting up a Selenium server. Selenium is a browser automation framework which will be used to 'remote control' a real browser. Log into the machine you have chosen for the Selenium server with administrator privileges Download and install Mozilla Firefox (getfirefox.com), this will be the browser that we target as part of this example, however Selenium can target lots of other browsers. For a full breakdown please Download Selenium Server ( Place it into a permanent location of you choosing, in our example (\"C:\\Program Files (x86)\\SeleniumServer\") Download NSSM ( Ensure that port 4444 is set to allow traffic (this is the default communications port for Selenium) Open a console and run the following commands: \"C:\\Program Files (x86)\\nssm-2.22\\win64\\nssm.exe\" install Selenium-Server \"java\" \"-jar \\\"C:\\Program Files (x86)\\SeleniumServer\\selenium-server-standalone-2.41.0.jar\\\"\" net start Selenium-Server In order to uninstall the Selenium server service, the following commands can be run: net stop Selenium-Server \"C:\\Program Files (x86)\\nssm-2.22\\win64\\nssm.exe\" remove Selenium-Server Step2: Create a test class and add it to source control Create a new class library project in Visual Studio, lets call it 'tests' Open the Nuget Package Manager Console window (tools menu-> library package manager -> package manager console), select the test project as the default project and run the following script: Install-Package Selenium.Automation.Framework Install-Package Selenium.WebDriver Install-Package Selenium.Support Install-Package NUnit Install-Package NUnit.Runners Create a new class called within the tests project (lets call it tests) and place the below code (Note: line 23 should be changed with location to the Selenium-Server we setup in the previous step): using System; using System.Text; using NUnit.Framework; using OpenQA.Selenium.Firefox; using OpenQA.Selenium; using OpenQA.Selenium.Remote; using OpenQA.Selenium.Support.UI; namespace SeleniumTests { [TestFixture] public class test { private RemoteWebDriver driver; [SetUp] public void SetupTest() { // Look for an environment variable string server = null; server = System.Environment.GetEnvironmentVariable(\"SELENIUM_SERVER\"); if (server == null) { server = \"http:// *** PUT THE NAME OF YOUR SERVER HERE ***:4444/wd/hub\"; } // Remote testing driver = new RemoteWebDriver(new Uri(server), DesiredCapabilities.Firefox()); } [TearDown] public void TeardownTest() { try { driver.Quit(); } catch (Exception) { // Ignore errors if unable to close the browser } } [Test] public void FirstSeleniumTest() { driver.Navigate().GoToUrl(\"http://www.google.com/\"); IWebElement query = driver.FindElement(By.Name(\"q\")); query.SendKeys(\"a test\"); Assert.AreEqual(driver.Title, \"Google\"); } } } Step3: Test the test! Build the solution Open NUnit build runner (by default this is located at ~\\packages\\NUnit.Runners.2.6.3\\tools\\nunit.exe) , Select file -> Open Project, and locate the tests dll that you have build in the previous step click the run button ~ 15 seconds or so you should have one green test! So what just happened? Behind the scenes an instance of firefox was opened (on the Selenium Server), perform a simple google search query and undertook a simple Nunit assertion has verified the name of the window was equal to \"Google\", very cool!. Now lets make the test fail, go ahead and change line 78, lets say \"zzGoogle\", build, and rerun the test. We now have a failing test. Go ahead and change it back so that we have a single passing test. Create a source control repository In this article we will be focusing on the functional testing component of this pipeline. We will proceed on the assumption that your code has already been, built unit tested and deployed to a Functional Test environment. Today we will;Firstly we will prepare for our UI tests by setting up a Selenium server.This machine will be designated for performing the UI tests against, preferably this will be a machine separate from your ContinuaCI server.Log into the machine you have chosen for the Selenium server with administrator privilegesDownload and install Mozilla Firefox (getfirefox.com), this will be the browser that we target as part of this example, however Selenium can target lots of other browsers. For a full breakdown please see the docs page : .Download Selenium Server ( docs.seleniumhq.org/download ), at the time of writing the latest version is 2.41.0.Place it into a permanent location of you choosing, in our example (\"C:\\Program Files (x86)\\SeleniumServer\")Download NSSM ( nssm.cc/download ), unzip it and place into a permanent location of you choosing \"C:\\Program Files (x86)\\nssm-2.22\\\"Ensure that port 4444 is set to allow traffic (this is the default communications port for Selenium)Open a console and run the following commands:In order to uninstall the Selenium server service, the following commands can be run:Create a new class library project in Visual Studio, lets call it 'tests'Open the Nuget Package Manager Console window (tools menu-> library package manager -> package manager console), select the test project as the default project and run the following script:Install-Package Selenium.Automation.FrameworkInstall-Package Selenium.WebDriverInstall-Package Selenium.SupportInstall-Package NUnitInstall-Package NUnit.RunnersCreate a new class called within the tests project (lets call it tests) and place the below code (Note: line 23 should be changed with location to the Selenium-Server we setup in the previous step):Build the solutionOpen NUnit build runner (by default this is located at ~\\packages\\NUnit.Runners.2.6.3\\tools\\nunit.exe) , Select file -> Open Project, and locate the tests dll that you have build in the previous stepclick the run button~ 15 seconds or so you should have one green test!So what just happened? Behind the scenes an instance of firefox was opened (on the Selenium Server), perform a simple google search query and undertook a simple Nunit assertion has verified the name of the window was equal to \"Google\", very cool!.Now lets make the test fail, go ahead and change line 78, lets say \"zzGoogle\", build, and rerun the test. We now have a failing test. Go ahead and change it back so that we have a single passing test. In this example, we're using mercurial open a command prompt at ~\\ type \"hg init\" add a .hgignore file into the directory. For convenience we have prepared one for you here type \"hg add\" type \"hg commit -m \"initial commit\"\" Step 4: Setting up Automated UI testing in ContinuaCI Navigate to the ContinuaCI web interface Create a project Open ContinuaCI Select \"Create Project\" from the top tasks dropdown menu Name the project something memerable; In our case: \"pete sel test 1\" Click the \"Save & Complete Wizard\" button Create a configuration for this project Click \"Create a Configuration\" Name the config something memorable; in our case \"sel-testconfig-1\" Click save & Continue Click the 'Enable now' link at the bottom of the page to enable this configuration Point to our Repository under the section \"Configuration Repositories\", select the \"Create\" link Name the repository \"test_repo\" Select \"Mercurial\" from the \"type\" dropdown list Select the Mercurial\" tab from the top of the dialogue box Enter the repository location under \"source path\" in our case '\\\\machinename\\c$\\sel-blog-final' Click validate to ensure all is well Click save, your repository is now ready to go! Add actions to our build Click on the Stages tab We will add a nuget restore action, click on the \"Nuget\" section from the categories on the left Drag and drop the action \"Nuget Restore\" onto the design surface Enter the location of the solution file: \"$Source.test_repo$\\tests.sln\" Click Save Build our tests Click on the \"Build runners\" category from the categories on the left hand menu Drag and drop a Visual Studio action onto the design surface (note that the same outcome can be achieved here with an MSBuild action). Enter the name of the solution file: \"$Source.test_repo$\\tests.sln\" Specify that this should be a 'Release' build under the configuration option Click save Setup ContinuaCI to run our Nunit tests Select the 'unit testing' category from the left hand menu Drag and drop an NUnit action onto the design surface Name our action 'run UI tests' Within the files: option, specify the name of the tests project '$Source.test_repo$\\tests\\tests.csproj' Within the Project Configuration section specify 'Release' Specify which version of NUnit In order to provide greater configuration flexibility we can pass in the location of our Selenium server to the tests at runtime. This is done within the 'Environments' tab. In our case the location of the Selenium server is http://SELSERVER:4444/wd/hub . Click Save Click save and complete Wizard We are now ready to build! Start a build immediately by clicking the top right hand side fast forward icon A build will start, and complete! When viewing the build log (this can be done by clicking on the green build number, then selecting the log tab) we can see that our UI tests have been run successfully. They are also visible within the 'Unit Tests' tab which displays further metrics around the tests. Step 5: Getting more advanced Navigate to the ContinuaCI web interfaceOpen ContinuaCISelect \"Create Project\" from the top tasks dropdown menuName the project something memerable; In our case: \"pete sel test 1\"Click the \"Save & Complete Wizard\" buttonClick \"Create a Configuration\"Name the config something memorable; in our case \"sel-testconfig-1\"Click save & ContinueClick the 'Enable now' link at the bottom of the page to enable this configurationunder the section \"Configuration Repositories\", select the \"Create\" linkName the repository \"test_repo\"Select \"Mercurial\" from the \"type\" dropdown listSelect the Mercurial\" tab from the top of the dialogue boxEnter the repository location under \"source path\" in our case '\\\\machinename\\c$\\sel-blog-final'Click validate to ensure all is wellClick save, your repository is now ready to go!Click on the Stages tabWe will add a nuget restore action, click on the \"Nuget\" section from the categories on the leftDrag and drop the action \"Nuget Restore\" onto the design surfaceEnter the location of the solution file: \"$Source.test_repo$\\tests.sln\"Click SaveClick on the \"Build runners\" category from the categories on the left hand menuDrag and drop a Visual Studio action onto the design surface (note that the same outcome can be achieved here with an MSBuild action).Enter the name of the solution file: \"$Source.test_repo$\\tests.sln\"Specify that this should be a 'Release' build under the configuration optionClick saveSelect the 'unit testing' category from the left hand menuDrag and drop an NUnit action onto the design surfaceName our action 'run UI tests'Within the files: option, specify the name of the tests project '$Source.test_repo$\\tests\\tests.csproj'Within the Project Configuration section specify 'Release'Specify which version of NUnitIn order to provide greater configuration flexibility we can pass in the location of our Selenium server to the tests at runtime. This is done within the 'Environments' tab. In our case the location of the Selenium server isClick SaveClick save and complete WizardWe are now ready to build!Start a build immediately by clicking the top right hand side fast forward iconA build will start, and complete!When viewing the build log (this can be done by clicking on the green build number, then selecting the log tab) we can see that our UI tests have been run successfully. They are also visible within the 'Unit Tests' tab which displays further metrics around the tests. Lets try a slightly more advanced example. This time we will examine a common use case. A physical visual inspection test needs to be conducted before a release can progress in the pipeline. Place the following code within our test class. using System; using System.Text; using NUnit.Framework; using OpenQA.Selenium.Firefox; using OpenQA.Selenium; using OpenQA.Selenium.Remote; using OpenQA.Selenium.Support.UI; namespace SeleniumTests { [TestFixture] public class test { private RemoteWebDriver driver; [SetUp] public void SetupTest() { // Look for an environment variable string server = null; server = System.Environment.GetEnvironmentVariable(\"SELENIUM_SERVER\"); if (server == null) { server = \"http:// *** PUT THE NAME OF YOUR SERVER HERE ***:4444/wd/hub\"; } // Remote testing driver = new RemoteWebDriver(new Uri(server), DesiredCapabilities.Firefox()); } [TearDown] public void TeardownTest() { try { driver.Quit(); } catch (Exception) { // Ignore errors if unable to close the browser } } [Test] public void FirstSeleniumTest() { driver.Navigate().GoToUrl(\"http://www.google.com/\"); IWebElement query = driver.FindElement(By.Name(\"q\")); query.SendKeys(\"a test\"); Assert.AreEqual(driver.Title, \"Google\"); } [Test] public void MySecondSeleniumTest() { // Navigate to google driver.Navigate().GoToUrl(\"http://www.google.com/\"); IWebElement query = driver.FindElement(By.Name(\"q\")); // Write a query into the window query.SendKeys(\"a test\"); // wait at maximum ten seconds for results to display var wait = new WebDriverWait(driver, TimeSpan.FromSeconds(10)); IWebElement myDynamicElement = wait.Until< IWebElement >((d) => { return d.FindElement(By.Id(\"ires\")); }); // take a screenshot of the result for visual verification var fileName = TestContext.CurrentContext.Test.Name + \"-\" + string.Format(\"{0:yyyyMMddHHmmss}\", DateTime.Now) + \".png\"; driver.GetScreenshot().SaveAsFile(fileName, System.Drawing.Imaging.ImageFormat.Png); // perform an code assertion Assert.AreEqual(driver.Title, \"Google\"); } } } Build, and run the test. In this example we added an additional test to perform a google search, wait at maximum 10 seconds for results to display, take a screenshot (stored it to disk), and perform an NUnit assertion. The screenshot output from the test can be made available as an artifact within Continua! Firstly lets commit our changes; \"hg commit -m \"added a more advanced test\"\" Open the configuration in Continua CI (clicking the pencil icon) Navigate to the stages section Double click on the stage name (which will bring up the edit stage Dialogue box) Click on the Workspace rules tab Add the following line to the bottom of the text area: \"/ < $Source.test_repo$/tests/bin/Release/**.png\". This will tell Continua to return any .png files that we produced from this test back to the ContinuaCI Server. Click on the artifacts tab. Add the following line : **.png\" This will enable any .png files within the workspace to be picked up and displayed within the Artifacts tab. **.png Click save Click Save & Complete Wizard Start a new build Sweet! A screenshot of our test was produced, and can be seen within the Artifacts tab! Clicking on 'View' will display the image: We have put the sourcecode of this article up on Github . Please subscribe and comment! We are very excited to see what you guys come up with on Continua, happy testing! Some additional considerations: The user which the Selenium service runs under should have correct privileges The machine designated as the Selenium server may require access to the internet if your webapp has upstream dependencies (eg third party API's like github) Build, and run the test.In this example we added an additional test to perform a google search, wait at maximum 10 seconds for results to display, take a screenshot (stored it to disk), and perform an NUnit assertion. The screenshot output from the test can be made available as an artifact within Continua!Firstly lets commit our changes; \"hg commit -m \"added a more advanced test\"\"Open the configuration in Continua CI (clicking the pencil icon)Navigate to the stages sectionDouble click on the stage name (which will bring up the edit stage Dialogue box)Click on the Workspace rules tabAdd the following line to the bottom of the text area: \"/"},{"title":"Fight or Flight Response: Disaster Drones","tags":"scm","url":"http://ciandcd.github.io/fight-or-flight-response-disaster-drones.html","text":"From: https://www.perforce.com/blog/160722/fight-or-flight-response-disaster-drones The Future of Disaster Relief \"We always overestimate the change that will occur in the next two years and underestimate the change that will occur in the next ten. Don't let yourself be lulled into inaction.\" Bill Gates When we first introduced you to the Innovation Studio at the College of Alameda, we were teaming up to provide the community's young gaming enthusiasts with mentorship and instruction. Most recently, Perforce and the Innovation Studio put technology in the hands of those who need it most. The Challenge Natural disasters, armed conflicts, and economic instability threaten humanity's survival. It only takes minutes for disaster to strike, but it can take weeks to grapple with the aftermath as the military and other nongovernmental organizations (NGOs) struggle to respond to the needs of those hardest hit. In the digital age that is 2016, there is still no standard way to track patients; we rely on toe tagging and permanent markers to track patients as rescue staging care units perform basic medical services from triage to transport and hospital arrival. Perforce, the Innovation Studio, and the Navy believe that emerging technology can and should reshape the way we approach disaster response to speed up recovery efforts and reduce loss of life. The Solution Our goal is to get patient data, no matter how voluminous in size — photos, videos, voice recordings, vital stats — to the next stage of care with or before the patient's arrival so that hospitals can prepare to treat patients as quickly and efficiently as possible. Moreover, this advance has the potential to connect medical experts to rescue workers to offer differential diagnoses or treatments from halfway around the world using our DVCS model and Perforce servers, mobile devices available in the disaster area, ad hoc mesh networks, and drones. Let's break it down. Ad hoc mesh networks provide basic infrastructure for connectivity during emergency situations by establishing temporary cell towers in drones equipped with cell technology. These drones connect to one another and other hot spots to create the mesh network. Distributed Version Control Systems (DVCS) make it easy to update and share files as they move from one person to the next in the field. Our experiment consists of collecting and collating patient data with mobile devices such as phones or EKG machines, into a Perforce server, and moving patient data through the mesh network using peer-to-peer Perforce server technology to move a patient's data from one DVCS server to another. These DVCS servers can be carried on the drones themselves, or other moving vehicles, including Jeeps and helicopters. If our experiments are successful, we will empower more civilians to get involved with NGO humanitarian aid efforts by equipping them with the means to provide better care during the acute phase of disaster relief without requiring specialized military equipment or training. We'll be heading back to Camp Roberts from August 8-12 with several other companies from the public and private sectors to continue exploring post-disaster patient data tracking. Read more about our ongoing involvement in the JIFX 16-3 experiment."},{"title":"Blue Ocean July development update","tags":"ciandcd","url":"http://ciandcd.github.io/blue-ocean-july-development-update.html","text":"From: http://feedproxy.google.com/~r/ContinuousBlog/~3/5dLOYMEMF9w/ The team have been hard at work moving the needle forward on the Blue Ocean 1.0 features. Many of the features we have been working on have come a long way in the past few months but here's a few highlights: SSE is a great technology choice for new web apps as it only pushes out events to the client when things have changed on the server. That means there's a lot less traffic going between your browser and the Jenkins server when compared to the continuous AJAX polling method that has been typical of Jenkins in the past. Building upon Tom 's great work on Server Sent Events (SSE) both Cliff and Tom worked on making the all the screens in Blue Ocean update without manual refreshes. Keith has been working with Vivek to drive out a new set of extension points that allow us to build a new rest reporting UI in Blue Ocean. Today this works for JUnit test reports but can be easily extended to work with other kinds of reports. Thorsten and Josh have been hard at work breaking down the log into steps and making the live log tailing follow the pipeline execution - which we've lovingly nicknamed the \"karaoke mode\" Tom has been on allowing users to trigger jobs from Blue Ocean, which is one less reason to go back to the Classic UI :) Many of you have asked us questions about how you can try Blue Ocean today and have resorted to building the plugin yourself or running our Docker image. We wanted to make the process of trying Blue Ocean in its unfinished state by publishing the plugin to the experimental update center - it's available today! So what is the Experimental Update Center? It is a mechanism for the Jenkins developer community to share early previews of new plugins with the broader user community. Plugins in this update center are experimental and we strongly advise not running them on production or Jenkins systems that you rely on for your work. That means any plugin in this update center could eat your Jenkins data, cause slowdowns, degrade security or have their behavior change at no notice. You can learn how to activate the experimental update center on this post. Stay tuned for more updates!"},{"title":"Helix DVCS - Push Like a Pro","tags":"scm","url":"http://ciandcd.github.io/helix-dvcs-push-like-a-pro.html","text":"From: https://www.perforce.com/blog/160718/helix-dvcs-push-pro Helix DVCS – Push Like a Pro This post was a long time coming, but we are a bit busy at the moment. Nevertheless: if you read my previous posts on DVCS, you are probably curious how to push your changes to a remote server. Push You should all have successfully created local servers by now, either through an init or a clone. It is time to share your magnum opus with the rest of the world. p4 push There! That wasn't that hard, was it? Yes, I admit, there is a lot more magic going on in the background to make sure this command works as simple as that. So let's review the settings to guarantee success. Prerequisites First of all, your local server must have matching case sensitivity and Unicode settings to be able to push to another Helix server. If you cloned from that server you are fine, but if you initialised your independent copy first, you should use the \"-p\" option to extract the settings from the target or specify them by hand. Reread the \" initialise like a pro \" post if you are unsure. You also have to have permission to push your changes and the target has to be configured to accept your changes (\" administer like a pro \" post). Remote spec You need to have a remote spec with the name \"origin\" defined that maps your local files to the correct location on the target server. If you cloned your repository, this remote will have been created for you automatically, otherwise you need to set up this remote. If you remote is not called \"origin\", or if you want to push to a different server, you need to specify the remote explicitly. p4 push –r <remote-name> Remote user You need to be logged into the remote server, that is, have a valid ticket. If you remember the discussion about users in remote servers from the \" initialise \" post, you know that your local user and your remote user do not have to coincide. Since my original post, the remote spec has picked up a new field: remoteUser. In my case, if I want to push anything into the workshop, I tend to do that under the user \"sven_erik_knop\", while my local user is \"sknop\". Now I simply update my remote spec (\"p4 remote origin\" or \"p4 remote <remote spec>\") and set \"RemoteUser: sven_erik_knop\" and login to the remote server with p4 login –r <remote-spec> If you are using 2016.1 P4 and P4D, you may have noticed a new feature: if you do not have a valid ticket, then P4 will prompt you for a password automatically when you issue a command; after successful authentication, P4 will proceed immediately. This is very handy when pushing to a remote server – no need for a separate login. A push can fail if you have changes waiting for you in the target codeline on the remote server. In that case, you will need to fetch and potentially merge those changes first; but this is a topic for the next post. Finally, your push is not permitted if you have unsubmitted changes in your server. This can happen because you forgot to submit your local changes – or because you need to run a resubmit because of a previous unsubmit – also a topic for a separate post. If all these prerequisites are met, the push command will (finally) succeed as planned. Details on push What is it we are actually pushing? How do we know which changes need to be pushed and what happens to the change numbers themselves? And what about integrations, attributes, labels and fixes? Let's look at these questions a little more in detail. What is it we are actually pushing? When you run ‘p4 push', you will push changes, revisions and archives, together with potential integration records, attributes and job fixes. As part of a change, the user name, server id (recorded as the client workspace name), timestamps and of course the description are pushed to the remote server. How do we know what to push? Have a look at your remote spec. It contains the fields \"LastFetch\" and \"LastPush\", both of which either contain the word \"default\" or a change number. LastFetch contains the last change you pulled down from the remote server (more about this in the next post), LastPush is the last local change you have pushed to that remote server. Determining what needs to be pushed is then simply a matter of comparing the latest change number with LastPush, and transmitting all changes not previously pushed. If you run ‘p4 help push' you can see that there a couple of interesting options for this command. You can push individual streams (remember that local DVCS servers are always stream based, even if the remote server is not). You can push individual changes and even individual files. In both cases, the LastPush field does not get updated. Instead, push will determine which changes have already made it to the remote and ignore these changes. Only when all changes are pushed up does the local server update the field as well. If you want to know more details about which changes are pushed and which are ignored, run the push command with the verbose (-v) option. This option will also give you details about the resources consumed for the push: Change 1840 was already present in the target repository. Change 1841 imported as change 101354. Resource usage: qry/zip/db/arch/fin/tot=4ms/0ms/7ms+1.1K/13ms+22B/8ms/34ms What happens to the change numbers? The output for the change 1841 was interesting: the change was imported as change 101354. What is going on here? The point about DVCS is that you are normally not connected to the remote server, sitting in a plane or on a train instead. So local change numbers and change numbers on the remote server will diverge (remember that in Perforce Helix change numbers are strictly monotonic increasing). When I push a change from my local server to the remote server, the change will usually be renumbered. This is similar to Mercurial (which locally uses strictly monotonic increasing changes as well) but quite different from Git, which insists on SHA hash numbers as a globally unique identifier of a change. This does not make sense for Perforce Helix – since I can push part of a change or individual files, the SHA has no meaning. Nonetheless, there is a way to link a remote change to a local change, by setting a local configurable: submit.identity. This configurable can be set to one of three different strategies: UUID A unique identifier created as a random number (it is a type 4 UUID). The UUID is created using a small, fast pseudo-random number generator (PRNG), which guarantees uniqueness to a high degree but is not necessarily sufficient for cryptography. Checksum This is an MD5 checksum hashed over the details of the change, including depot path, timestamp, content and revision of each file in the change, so two users submitting identical files with identical content will still get different checksums. Serverid This is a concatenation of your local server.id (that is, typically your local workspace name) and your local change number. The change identity is stored in your local change in the field Identity, which will be pushed to the remote server. You can use the command \"p4 describe –I <identity>\" to find a change with a given identity. Which strategy you use is up to you. Why don't you let us know which option is most useful to you? What happens to integrations? Just like in the ‘p4 clone' case, integrations within the view of a remote spec are pushed. To preserve integrations between streams, make sure to map source and target stream in your remote spec. If one-half of the integration is not mapped, the integration will be downgraded to add/edit/delete in the remote server upon push. Most importantly, renames within a stream are preserved when pushed. How about attributes? Since 2015.2 attributes set against a file revision on your local server will be pushed to the remote server, but only if that server is at least at 2015.2 as well (otherwise attributes will be silently ignored). And fixes? Yes, you can push fixes (links between jobs and changes) as well, but you need to copy the jobspec and the individual jobs across to the local server before you can do so. This is an exercise best left to the advanced user (and maybe subject to another post if you so wish). What about labels? Err. No. Keep in mind that Perforce Helix is different to your usual DVCS setup. The central usually has many projects in different paths to your local server, so translation of static labels is difficult. Automatic labels can be easily recreated on the remote server, though. Triggers and push There are triggers on the remote site that can fire when changes are pushed in, and these triggers will have to be used instead of the usual submit triggers to enforce policy. I'll cover the details in a separate post. Conclusion A simple push is very easy, but the devil is, as usual, in the details. I hope this post gives you an idea of the preparation stages and the result of a push. As usual, tweet feedback to @p4sven, please. Happy hacking!"},{"title":"Sending Notifications in Pipeline","tags":"ciandcd","url":"http://ciandcd.github.io/sending-notifications-in-pipeline.html","text":"From: http://feedproxy.google.com/~r/ContinuousBlog/~3/svzD4AgT-Us/ I thing we can all agree getting notified when events occur is preferable to having to constantly monitor them just in case. I'm going to continue from where I left off in my previous post with the hermann project. I added a Jenkins Pipeline with an HTML publisher for code coverage. This week, I'd like to make Jenkins to notify me when builds start and when they succeed or fail."},{"title":"New packages for Jenkins 2.7.1","tags":"ciandcd","url":"http://ciandcd.github.io/new-packages-for-jenkins-271.html","text":"From: http://feedproxy.google.com/~r/ContinuousBlog/~3/w9oVEycgQZU/ We created new native packages for Jenkins 2.7.1 today. These replace the existing packages. Due to a release process issue, the packaging (RPM, etc.) was created the same way as Jenkins 1.x LTS, resulting in problems starting Jenkins on some platforms: While we dropped support for AJP in Jenkins 2.0, some 1.x packages had it enabled by default, resulting in an exception during startup. These new packages for Jenkins 2.7.1, dated July 14, have the same scripts and parameters as Jenkins 2.x and should allow starting up Jenkins without problems. If you notice any further problems with the packaging, please report them in the packaging component."},{"title":"Jenkins 2 hits LTS","tags":"ciandcd","url":"http://ciandcd.github.io/jenkins-2-hits-lts.html","text":"From: http://feedproxy.google.com/~r/ContinuousBlog/~3/J9Mf0dutW_E/ Jenkins 2 hits LTS Tweet Published on 2016-07-07 by kohsuke It's been almost three months since we've released Jenkins 2.0 , the first ever major version upgrade for this 10 year old project. The 2.x versions since then has been adopted by more than 20% of the users, but one segment of users who haven't seen the benefits of Jenkins 2 is those who has been running LTS releases. But that is no more! The new version of Jenkins LTS release we just released is 2.7.1, and now LTS users get to finally enjoy Jenkins 2. This release also officially marks the end-of-life for Jenkins 1.x. There won't be any future release of Jenkins 1.x beyond this point. If you are worried about the upgrade, don't be! The core of Jenkins is still the same, and all the plugins & existing configuration will just work."},{"title":"Perforce Hires Its First Chief Product Officer","tags":"scm","url":"http://ciandcd.github.io/perforce-hires-its-first-chief-product-officer.html","text":"From: https://www.perforce.com/blog/160706/perforce-hires-its-first-chief-product-officer As we continue to look for new ways to provide greater value for our customers, we're excited to welcome our first Chief Product Officer ever, Tim Russell, to spearhead initiatives to expand our solutions that automate and protect the technology innovation and delivery lifecycle. Tim joins us with extensive experience leading technology teams in high-growth software companies, including NetApp, SafeNet, and Secure Computing. Most recently, he was Vice President of Product Management for NetApp's Data Fabric Group where he led the definition and engineering of customer solutions for data center, cloud, and hybrid cloud data management. Read the full press release here to learn more about Tim and hear what Janet Dryer, CEO, has to say."},{"title":"Take a DevOps Road Trip with Scania","tags":"scm","url":"http://ciandcd.github.io/take-a-devops-road-trip-with-scania.html","text":"From: https://www.perforce.com/blog/160706/take-devops-road-trip-scania There's nothing quite like standing at the top of a mountain, miles from where your journey started, thinking back at all you've accomplished to get there. But long road trips in cramped cars during the sweltering summer months can quickly get bumpy. Backseat passengers start bickering over leg room and imaginary lines that mustn't be crossed. Front seat passengers frantically try to decipher which exit the GPS really wants them take. It's a lot like trying to implement a successful DevOps cultural transformation across an entire global enterprise. It might be tempting to just go, but without a plan, unexpected roadblocks could mean returning to your silo-ed homelands with nothing to show for your troubles. \"Scania: Adopting DevOps for Auto Production\" explores the DevOps journey of Scania, one of the world's leading manufacturer's of trucks for heavy transportation vehicles, as it went from three painful releases a year to deploying successfully several times a day by using Perforce Helix as the focal point of its continuous integration environment and processes. Set a few checkpoints of your own and get your DevOps compass pointed in the right direction. Buckle up and enjoy the ride by clicking here !"},{"title":"Streams in Parallel Development","tags":"scm","url":"http://ciandcd.github.io/streams-in-parallel-development.html","text":"From: https://www.perforce.com/blog/160701/streams-parallel-development This week, we took a look at Perforce Streams and the many updates that have been implemented to make your project manager's lives easier when managing streams in parallel development. Our presenter Sven Erik Knop, Technical Marketing Manager at Perforce, dove into the details around making these updates work for you and your team and also answered questions from the audience. Here are two questions that we thought would be beneficial to our readership as answered by Sven Knop: How are streams used in continuous integration? Each stream, be it mainline, release or development stream, will be fed into the continuous integration engine (CI) to be built automatically. Streams can be discovered programmatically to generate build scripts automatically as well to ensure that any release and development branch is covered by the CI server. It is also reasonably easy to automate the process of propagating the changes between streams via a review process using shelves that will be picked up by the CI server as well. Can a depot be used by the conventional P4 as well as streams? Yes, for read-only purposes. You can point a \"classical\" client workspace (we call these \"unmanaged\" workspaces) to a depot path within a streams depot and sync files to your workspace, but you cannot check these files out and submit them outside of a streams workspace. Missed the presentation? No worries, there are two ways that you can get the latest on Perforce Streams. Register for our on-demand viewing and watch on your own time, or you can join us at Perforce on Tour 2016 where we'll revisit Streams for our live audience. Not only will you get a chance to catch up on what you missed at our latest DevTalk, but why not take this opportunity to discover why 1/3 of the earth's population now uses software developed on Perforce Helix to achieve DevOps harmony? Curious to learn more? Join us for Perforce on Tour 2016 in London and Berlin: London, UK - Thursday 22 September, 2016 at CodeNode Berlin, Germany - Thursday 29 September, 2016 at Hotel Sofitel Berlin Gendarmenmar"},{"title":"Publishing HTML Reports in Pipeline","tags":"ciandcd","url":"http://ciandcd.github.io/publishing-html-reports-in-pipeline.html","text":"From: http://feedproxy.google.com/~r/ContinuousBlog/~3/WZrkjtfbi7k/ Most projects need more that just JUnit result reporting. Rather than writing a custom plugin for each type of report, we can use the HTML Publisher Plugin . I've found a Ruby project, hermann , I'd like to build using Jenkins Pipeline. I'd also like to have the code coverage results published with each build job. I could write a plugin to publish this data, but I'm in a bit of hurry and the build already creates an HTML report file using SimpleCov when the unit tests run. Now I just need to add the step to publish the code coverage report. I know that rake spec creates an index.html file in the coverage directory. I've already installed the HTML Publisher Plugin . How do I add the HTML publishing step to the pipeline? The plugin page doesn't say anything about it. I'm going to use the HTML Publisher Plugin to add the HTML-formatted code coverage report to my builds. Here's a simple pipeline for building the hermann project. Then it shows me a UI similar to the one used in job configuration. I fill in the fields, click \"generate\", and it shows me snippet of groovy generated from that input. Documentation is hard to maintain and easy to miss, even more so in a system like Jenkins with hundreds of plugins the each potential have one or more groovy fixtures to add to the Pipeline. The Pipeline Syntax Snippet Generator helps users navigate this jungle by providing a way to generate a code snippet for any step using provided inputs. HTML Published I can use that snippet directly or as a template for further customization. In this case, I'll just reformat and copy it in at the end of my pipeline. (I ran into a minor bug in the snippet generated for this plugin step. Typing error string in my search bar immediately found the bug and a workaround.) /* ...unchanged... */ // Archive the built artifacts archive ( includes : ' pkg/*.gem ' ) // publish html // snippet generator doesn't include \"target:\" // https://issues.jenkins-ci.org/browse/JENKINS-29711. publishHTML ( target : [ allowMissing : false , alwaysLinkToLastBuild : false , keepAll : true , reportDir : ' coverage ' , reportFiles : ' index.html ' , reportName : \" RCov Report \" ]) } When I run this new pipeline I am rewarded with an RCov Report link on left side, which I can follow to show the HTML report. I even added the keepAll setting to let I can also go back an look at reports on old jobs as more come in. As I said to to begin with, this is not as slick as what I could do with a custom plugin, but it is much easier and works with any static HTML."},{"title":"External Workspace Manager Plugin alpha version","tags":"ciandcd","url":"http://ciandcd.github.io/external-workspace-manager-plugin-alpha-version.html","text":"From: http://feedproxy.google.com/~r/ContinuousBlog/~3/uz3BMcwgrOI/ Currently it's quite difficult to share and reuse the same workspace between multiple jobs and across nodes. There are some possible workarounds for achieving this, but each of them has its own drawback, e.g. stash/unstash pre-made artifacts, Copy Artifacts plugin or advanced job settings. A viable solution for this problem is the External Workspace Manager plugin, which facilitates workspace share and reuse across multiple Jenkins jobs and nodes. It also eliminates the need to copy, archive or move files. You can learn more about the design and goals of the External Workspace Manager project in this introductory blog post. I'd like to announce that an alpha version of the External Manager Plugin has been released! It's now public available for testing. To be able to install this plugin, you must follow the steps from the Experimental Plugins Update Center blog post. Please be aware that it's not recommended to use the Experimental Update Center in production installations of Jenkins, since it may break it. The plugin's wiki page may be accessed here . The documentation that helps you get started with this plugin may be found on the README page. To get an idea of what this plugin does, which are the features implemented so far and to see a working demo of it, you can watch my mid-term presentation that is available here . The slides for the presentation are shared on Google Slides . My mentors, Martin and Oleg , and I have set up public meetings related to this plugin. You are invited to join our discussions if you'd like to get more insight about the project. The meetings are taking place twice a week on the Jenkins hangout , every Monday at 12 PM UTC and every Thursday at 5 PM UTC . If you have any issues in setting up or using the plugin, please feel free to ask me on the plugin's Gitter chat . The plugin is open-source, having the repository on GitHub , and you may contribute to it. Any feedback is welcome, and you may provide it either on the Gitter chat, or on Jira by using the external-workspace-manager-plugin component."},{"title":"Migrating from chained Freestyle jobs to Pipelines","tags":"ciandcd","url":"http://ciandcd.github.io/migrating-from-chained-freestyle-jobs-to-pipelines.html","text":"From: http://feedproxy.google.com/~r/ContinuousBlog/~3/w0VIYgV6amg/ This is a guest post by R. Tyler Croy , who is a long-time contributor to Jenkins and the primary contact for Jenkins project infrastructure. He is also a Jenkins Evangelist at CloudBees, Inc. For ages I have used the \"Build After\" feature in Jenkins to cobble together what one might refer to as a \"pipeline\" of sorts. The Jenkins project itself, a major consumer of Jenkins, has used these daisy-chained Freestyle jobs to drive a myriad of delivery pipelines in our infrastructure. One such \"pipeline\" helped drive the complex process of generating the pretty blue charts on stats.jenkins-ci.org . This statistics generation process primarily performs two major tasks, on rather large sets of data: Generate aggregate monthly \"census data.\" Process the census data and create trend charts The chained jobs allowed us to resume the independent stages of the pipeline, and allowed us to run different stages on different hardware (different capabilities) as needed. Below is a diagram of what this looked like: The infra_generate_monthly_json would run periodically creating the aggregated census data, which would then be picked up by infra_census_push whose sole responsibility was to take census data and publish it to the necessary hosts inside the project's infrastructure. The second, semi-independent, \"pipeline\" would also run periodically. The infra_statistics job's responsibility was to use the census data, pushed earlier by infra_census_push , to generate the myriad of pretty blue charts before triggering the infra_checkout_stats job which would make sure stats.jenkins-ci.org was properly updated. Suffice it to say, this \"pipeline\" had grown organically over a period time when more advanced tools weren't quite available. When we migrated to newer infrastructure for ci.jenkins.io earlier this year I took the opportunity to do some cleaning up. Instead of migrating jobs verbatim, I pruned stale jobs and refactored a number of others into proper Pipelines , statistics generation being an obvious target! Our requirements for statistics generation, in their most basic form, are: Enable a sequence of dependent tasks to be executed as a logical group (a pipeline) Enable executing those dependent tasks on various pieces of infrastructure which support different requirements Actually generate those pretty blue charts If you wish to skip ahead, you can jump straight to the Jenkinsfile which implements our new Pipeline. The first iteration of the Jenkinsfile simply defined the conceptual stages we would need: node { stage ' Sync raw data and census files ' stage ' Process raw logs ' stage ' Generate census data ' stage ' Generate stats ' stage ' Publish census ' stage ' Publish stats ' } How exciting! Although not terrifically useful. When I began actually implementing the first couple stages, I noticed that the Pipeline might sync dozens of gigabytes of data every time it ran on a new agent in the cluster. While this problem will soon be solved by the External Workspace Manager plugin , which is currently being developed. Until it's ready, I chose to mitigate the issue by pinning the execution to a consistent agent. /* `census` is a node label for a single machine, ideally, which will be * consistently used for processing usage statistics and generating census data */ node( ' census && docker ' ) { /* .. */ } Restricting a workload which previously used multiple agents to a single one introduced the next challenge. As an infrastructure administrator, technically speaking, I could just install all the system dependencies that I want on this one special Jenkins agent. But what kind of example would that be setting! The statistics generation process requires: JDK8 Groovy A running MongoDB instance Fortunately, with Pipeline we have a couple of useful features at our disposal: tool auto-installers and the CloudBees Docker Pipeline plugin . Tool Auto-Installers Tool Auto-Installers are exposed in Pipeline through the tool step and on ci.jenkins.io we already had JDK8 and Groovy available. This meant that the Jenkinsfile would invoke tool and Pipeline would automatically install the desired tool on the agent executing the current Pipeline steps. The tool step does not modify the PATH environment variable, so it's usually used in conjunction with the withEnv step, for example: node( ' census && docker ' ) { /* .. */ def javaHome = tool( name : ' jdk8 ' ) def groovyHome = tool( name : ' groovy ' ) /* Set up environment variables for re-using our auto-installed tools */ def customEnv = [ \" PATH+JDK= ${ javaHome } /bin \" , \" PATH+GROOVY= ${ groovyHome } /bin \" , \" JAVA_HOME= ${ javaHome } \" , ] /* use our auto-installed tools */ withEnv(customEnv) { sh ' java --version ' } /* .. */ } CloudBees Docker Pipeline plugin Satisfying the MongoDB dependency would still be tricky. If I caved in and installed MongoDB on a single unicorn agent in the cluster, what could I say the next time somebody asked for a special, one-off, piece of software installed on our Jenkins build agents? After doing my usual complaining and whining, I discovered that the CloudBees Docker Pipeline plugin provides the ability to run containers inside of a Jenkinsfile . To make things even better, there are official MongoDB docker images readily available on DockerHub! This feature requires that the machine has a running Docker daemon which is accessible to the user running the Jenkins agent. After that, running a container in the background is easy, for example: node( ' census && docker ' ) { /* .. */ /* Run MongoDB in the background, mapping its port 27017 to our host's port * 27017 so our script can talk to it, then execute our Groovy script with * tools from our `customEnv` */ docker.image( ' mongo:2 ' ).withRun( ' -p 27017:27017 ' ) { container -> withEnv(customEnv) { sh \" groovy parseUsage.groovy --logs ${ usagestats_dir } --output ${ census_dir } --incremental \" } } /* .. */ } The beauty, to me, of this example is that you can pass a closure to withRun which will execute while the container is running. When the closure is finished executin, just the sh step in this case, the container is destroyed. With that system requirement satisfied, the rest of the stages of the Pipeline fell into place. We now have a single source of truth, the Jenkinsfile , for the sequence of dependent tasks which need to be executed, accounting for variations in systems requirements, and it actually generates those pretty blue charts ! Of course, a nice added bonus is the beautiful visualization of our new Pipeline !"},{"title":"Continua CI and FinalBuilder integration improvements","tags":"ciandcd","url":"http://ciandcd.github.io/continua-ci-and-finalbuilder-integration-improvements.html","text":"From: https://www.finalbuilder.com/resources/blogs/postid/747/continua-ci-and-finalbuilder-integration-improvements Continua CI 1.8 Continua CI 1.8.0.176 and FinalBuilder 8.0.0.1817 (both released today) provide somewhat better integration between the two products. The FinalBuilder Action in Continua CI now produces an xml file that is consumed by FinalBuilder when run under Continua CI. This xml file contains all the useful information about a build, such as version numbers, changesets, variables etc. There is also a new option on the action that will automatically apply Continua CI variable values to matching FinalBuilder variables. What this means is, if you have a variable declared in both FinalBuilder and Continua CI with the same name, FinalBuilder's variable will automatically get the value of the Continua CI variable at runtime. This option is only available for FinalBuilder 8, if you select FinalBuilder 7 the option will not be visible. If the version of FinalBuilder 8 you are running does not support this integration, a warning will appear in the Continua CI build log. FinalBuilder 8 FinalBuilder 8 has two new actions. The \"Is Running Under Continua\" action is an If Then style action, i.e. the children of this action will only run if FinalBuilder is running under Continua CI. FinalBuilder 8 has two new actions.The \"Is Running Under Continua\" action is an If Then style action, i.e. the children of this action will only run if FinalBuilder is running under Continua CI. The other action is the Continua CI - Get Version Info action - this action will take the version info from Continua CI and apply it to a version info property set in your FinalBuilder project.; The action is smart enough to use the correct version info scheme depending on whether the propertyset is a win32 or dotnet propertyset. This greatly simplifies getting the version info from Continua into FinalBuilder, no need to declare 4 variables on both sides and set them in the FinalBuilder Action in Continua CI. Scripting That xml file I mentioned above is loaded into a Continua object model that is available in action script events. If you do make use of it, you should be sure to check the Continua.IsRunningUnderContinua property before using the rest of the object model (the script editor provides intellisense for the model. That xml file I mentioned above is loaded into a Continua object model that is available in action script events. If you do make use of it, you should be sure to check the Continua.IsRunningUnderContinua property before using the rest of the object model (the script editor provides intellisense for the model."},{"title":"GSoC: Mid-term presentations by students on June 23 and 24","tags":"ciandcd","url":"http://ciandcd.github.io/gsoc-mid-term-presentations-by-students-on-june-23-and-24.html","text":"From: http://feedproxy.google.com/~r/ContinuousBlog/~3/Sos1yGo_VtY/ As you probably know, on this year Jenkins projects participates in Google Summer of Code 2016 . You can find more information about the accepted projects on the GSoC subproject page and in the Jenkins Developer mailing list. On this week GSoC students are going to present their projects as a part of mid-term evaluation, which covers one month of community bonding and one month of coding. We would like to invite Jenkins developers to attend these meetings. There are two additional months of coding ahead for successful students, so any feedback from Jenkins contributors and users will be appreciated. Jenkins WebUI: Improving Job Creation/Configuration by Samat Davletshin Intro blogpost Q&A session Meeting link Both meetings will be conducted and recorded via Hangouts on Air . The recorded sessions will be made public after the meetup. The agenda may change a bit."},{"title":"Your Questions Answered","tags":"scm","url":"http://ciandcd.github.io/your-questions-answered.html","text":"From: https://www.perforce.com/blog/160620/questions-answered The Changing Role of Release Engineering in a DevOps World Here at Perforce, DevOps expert J. Paul Reed recently joined us for a broadcast on the changing role of Release Engineering in the word of DevOps . In this 30-minute webinar, Paul dove into topics such as: How release engineering and DevOps intersect What DevOps and Continuous Delivery mean for the future of release engineering Tips for grappling with release engineering in a DevOps world We had a great audience for our live broadcast and received some insightful questions from our viewers. Here are a few of our favorite live questions answered by DevOps consultant J. Paul Reed. Where do you see release engineering in 10 years? In 2026, everything will be based on server-less architectures, meaning we can get rid of all those pesky operations and release engineers. No, but seriously: I think the work currently associated with \"release engineering\" may start to be called something else, especially as we attempt to \"continuous delivery\" everything; but the work will be very similar, and in fact, as the Internet-of-Things heats up, I think the entire industry is in for a rude awakening when they realize that the web, in many ways, has coddled us such that we could mostly ignore all of the release engineering discipline and everything would still be \"mostly OK.\" (And easily fixable, if not!) Of course, us release engineers know that companies made their lives harder by doing so, but when you start shipping software to your toaster and your pacemaker and your electric car, suddenly, all those considerations around the mechanics of release software become incredibly important again. So in that regard, I wouldn't be surprised if we experience a bit of a renaissance in release engineering; what remains to be seen is whether the industry will look backward 10 years to the lessons we've already learned, or do what it usually does, and try to re-learn everything from first principles again and, of course, with the pain, gnashing of teeth, and customer-affecting failure that comes with that. Can DevOps succeed without a centralized enterprise DevOps team? Most certainly! In fact, many would say a \"centralized enterprise DevOps team\" is an anti-pattern. In large enterprises, I have seen the creation of centralized DevOps teams (or even worse: a \"DevOps Center of Excellence\") and it tends to be fraught with all sorts of problems: It's yet another team: one of the problems DevOps tries to solve is the siloed nature of development and operations teams; but when you create another team in between these teams, that's just another team to manage, with additional complexities to address. And it doesn't solve the original silo problem. There's been much discussion recently of the problems with \"bi-modal IT,\" the idea that there's \"one speed\" for \"legacy\" IT, and the shiny, new DevOps way is another mode. This has a tendency to engender all sorts of animosity between the teams responsible for the \"two modes\" of IT, and a centralized DevOps team is likely to promote this separation, which is starting to be widely recognized as problematic. Lastly, a centralized DevOps team doesn't allow local development and operations teams to experiment, communicate, learn, wrestle with, and work in their local contexts, precisely where it would be most valuable. On the contrary, it restricts such behavior. That's not to say if you have one of these teams, you're \"doing it wrong\" or it's going to be horrible. But I would say: a centralized DevOps team does not guarantee success, and in fact, in many ways, may impede it. Is Docker better than VM technology for DevOps? In a test environment? Is this the future? There's a lot of hype around Docker right now, and because of that there is confusion around what it's good for, what it isn't good for, and how it can best be leveraged in real-world environments. A tweet I saw just the other day was from an engineer with a fair amount of Docker experience; she asked the audience she was presenting to \"Who's using Docker?\" and noted that many hands went up. She then added \"... in production\": the hands all went down. There's a fair amount of management and infrastructure tooling that's currently missing in Docker-land; I think they're working hard and fast on fixing that, but that's also why you get a fairly wide range of \"Docker usage stories.\" For some users, the tools they need to manage at scale (in or out of production) simply aren't there. They either have to build them, or abandon Docker. Ultimately, Docker as a way to coordinate developers' environments is a great use case, and it's the one I commonly see. I don't believe Docker solves all the problems that virtualization solved and it doesn't solve all the problems that configuration management solves. But it does solve a problem, and the best Docker rollouts are those that use Docker for certain solutions, but can still produce VMs, or AMIs, or other operable artifacts that are not Docker containers. For more discussion on some of the concerns of rolling out Docker in an environment, read my good friend Julian Dunn's blog post: The Oncoming Train of Enterprise Container Deployments . For additional DevOps best practices, download J. Paul's publication by O'Reilly media \" DevOps in Practice \", and for more webcasts on hot topics please visit perforce.com/events to register for upcoming webinars."},{"title":"I Test, You Test","tags":"scm","url":"http://ciandcd.github.io/i-test-you-test.html","text":"From: https://www.perforce.com/blog/160617/i-test-you-test In a previous blog post, I briefly mentioned the fact that as customers reported problems with the old v2 engine, these cases were logged in the form of detailed test cases. More than one customer has asked how we test the integration logic, so I thought this might be a good opportunity to go into the integration test harness I use, \"itest\", in a little more detail. I first became involved in the problem of how to test our integration logic when I was in technical support, serving as the main point of contact for any customer encountering problems with integration. When customers encountered problems that didn't have workarounds, I would attempt to advocate for specific fixes; sometimes in the form of small tweaks that would solve a particular problem, sometimes in the form of more fundamental changes that seemed like the only way to solve the more fundamental problems. Development was hesitant to attempt to implement any of these suggestions, largely because even if they were good suggestions (and one or two of them might have been), it would be difficult to verify their efficacy. Before itest existed, the body of tests for integration (and everything else) worked by running a large set of commands and diffing the output vs a large set of canonical output. Adding a new test in the middle of this suite and making sure all of the tests that followed it still functioned correctly could be nontrivial. Testing the semantic effect of a change that touched a large number of systems could also be nontrivial -- if a low-level change caused a set of commands to produce functionally the same result but with a different number of changelists, or selecting a different (but identical) revision as a base, the entire test would \"fail\" and a large amount of human effort would be needed to see whether the change worked as intended in all of those cases. Add into this state of affairs the situation when we did make a few fundamental changes to the v2 algorithm and customers began to report behavioral differences. The changes in question are described in this 2007.2 relnote: #119955 (Bug #23698, #24251, #24207, #23469, #24150) ** The 'p4 integrate' algorithms for suppressing reintegration and for picking the optimum base, which were reimplemented in 2006.1, have been tuned significantly for this release. The following new changes have been made: Integrating a single 'copy from' revision now gives credit for all earlier revisions, so that a subsequent 'p4 integrate' of any earlier revision will find no work to do. This can only come about by 'cherry picking' (providing to 'p4 integrate' specific revisions to integrate). Pending integration records (files opened with 'p4 integrate' but not yet submitted with 'p4 submit') are now considered when finding the most appropriate base. This makes integrating into a related file already opened for branch possible without the 'p4 integrate -i' flag. 'p4 integrate' follows indirect integrations through complicated combinations of merge/copy/ignore integration records better. This should result in fewer integrations being scheduled, and closer bases being picked, for integration between distant files. 'p4 integrate' could wrongly choose a base on the source file after the revisions needing to be integrated if the revisions needing to be integrated were before revisions already integrated. This normally only comes about in cases of 'cherry picking' (providing to 'p4 integrate' specific revisions to integrate). 'p4 integrate' in certain cases wouldn't find a base (or choose a poorer base) if the source file was branched and reopened for add, and then the original file was changed further and branched again. As customers reported that merges in some cases had become more difficult after the upgrade to 2007.2, I wanted to get hard data to determine if there had been a verifiable regression in behavior and possibly make a case to development to roll some of the changes back, so I got a lot of practice at looking at the data I had available (mostly screenshots of Revision Graph) and coming up with sets of commands that would reconstruct the same scenario -- rather than logging a bug report that said \"some merges are different after the upgrade\", or including a full customer checkpoint as an attachment, I had something that development could simply copy and paste into a shell to recreate the situation that the customer was experiencing. I started writing the itest.pl test suite while I was on the long flight back from the 2008 European User Conference; we had just had a long conversation about the possibility of making deep changes to the integration algorithm to cut down on conflicts, and the problem had come up of not having adequate test data to verify that sort of a deep change. The following requirements were foremost in my mind: · It needed to be easier for me to generate test cases than my then-current method of typing out individual shell commands. · The tests needed to be able to check for semantic correctness rather than relying on byte-for-byte equality of output. · I needed to be able to easily run the same set of tests in different environments to report on differences between versions. The script started with a simple method to generate text files with a sequence of non-conflicting edits that I could use to arbitrarily produce clean merges at will (try doing that in a simple batch script with \"echo\" statements and redirects; it's torment), and quickly evolved into a language that was able to represent everything in my library of batch scripts with a very small fraction of the typing. This is an example of a test script written using the itest tool: add X branch X Z branch X Y edit X 1 B1 edit Y 1 B2 edit Z 2 D dirty Y X 1 B3 merge Z Y merge Z X test base X Y Y#3 Y#2 Z#2 X#1 In English this would read as: \"add a file called X, create branches Z and Y, edit different text into X and Y at a common location to force a conflict, and something else into Z at a non-conflicting location. Do a dirty merge from Y to X, resolving the conflict with yet another edit in the same location, and then do clean merges from Z to both Y and X. When merging from X to Y, the ideal base is Y#3, with other acceptable (but not ideal) bases being, from best to worst: Y#2, Z#2, and X#1.\" When executing this script, the output will be a letter grade from A (for the ideal base being chosen) to F (for none of the presented options being chosen). The 2006.1 server receives a grade of \"C\" on this test; the server as of 2013.2 receives an \"A\". Once I had migrated all of my existing test cases into this tool, I was able to very quickly create a table (with color coding and everything) demonstrating differences in behavior between releases -- cases where 2006.1 was a regression from 2005.2, and cases where 2007.2 was a regression from 2006.2, as well as cases where the newer release was a step forward. As time went on, I continued adding more and more data to this test suite, and slowly started to assemble a picture of what an algorithm that addressed every case at once might look like. Having so many examples of cases that could \"fool\" each existing approach into picking a demonstrably non-optimal answer made it much easier to think about what approach we could use to produce an optimal answer in each instance. These examples also served me very well when I began work on a prototype for a system that I thought might produce that optimal answer; once my prototype was able to produce right answers to everything that the current software got wrong, it was easy to make a case to development that it was worth working to improve or rewrite what we had. A few years later, I myself had been absorbed into development to take over that task, and I hope this serves as a cautionary tale to others about the dangers of writing demonstrably useful prototypes, even just for fun. Here's an example of a table of test output, with the same set of tests run across three different server versions: The current version of itest.pl can be found in the Workshop . Some time ago I decided to put it out there so that customers who were experiencing problems with integration and wanted to document those cases for themselves, the same way that I used to when I was working directly with customers, would have access to the same tool that I used. More recently I've submitted a selection of the test cases that we have assembled over the years. This isn't the full suite (I've removed a large number of cases that are based directly on customer data, leaving only the more abstract cases) but hopefully it provides an idea of the breadth of situations that we test and develop for. As promised in my last post, I will soon be getting to a description of the current integration engine, which is based heavily on the prototype I mentioned in this post. Stay tuned!"},{"title":"Faster Pipelines with the Parallel Test Executor Plugin","tags":"ciandcd","url":"http://ciandcd.github.io/faster-pipelines-with-the-parallel-test-executor-plugin.html","text":"From: http://feedproxy.google.com/~r/ContinuousBlog/~3/bwPY9EQtqlA/ node { /* ...unchanged... */ } void runTests( def args) { /* Request the test groupings. Based on previous test results. */ /* see https://wiki.jenkins-ci.org/display/JENKINS/Parallel+Test+Executor+Plugin and demo on github /* Using arbitrary parallelism of 4 and \"generateInclusions\" feature added in v1.8. */ def splits = splitTests parallelism : [ $ class : ' CountDrivenParallelism ' , size : 4 ], generateInclusions : true /* Create dictionary to hold set of parallel test executions. */ def testGroups = [:] for ( int i = 0 ; i < splits.size(); i++) { def split = splits[i] /* Loop over each record in splits to prepare the testGroups that we'll run in parallel. */ /* Split records returned from splitTests contain { includes: boolean, list: List<String> }. */ /* includes = whether list specifies tests to include (true) or tests to exclude (false). */ /* list = list of tests for inclusion or exclusion. */ /* The list of inclusions is constructed based on results gathered from */ /* the previous successfully completed job. One additional record will exclude */ /* all known tests to run any tests not seen during the previous run. */ testGroups[ \" split- ${ i } \" ] = { // example, \"split3\" node { checkout scm /* Clean each test node to start. */ mvn ' clean ' def mavenInstall = ' install -DMaven.test.failure.ignore=true ' /* Write includesFile or excludesFile for tests. Split record provided by splitTests. */ /* Tell Maven to read the appropriate file. */ if (split.includes) { writeFile file : \" target/parallel-test-includes- ${ i } .txt \" , text : split.list.join( \" \\n \" ) mavenInstall += \" -Dsurefire.includesFile=target/parallel-test-includes- ${ i } .txt \" } else { writeFile file : \" target/parallel-test-excludes- ${ i } .txt \" , text : split.list.join( \" \\n \" ) mavenInstall += \" -Dsurefire.excludesFile=target/parallel-test-excludes- ${ i } .txt \" } /* Call the Maven build with tests. */ mvn mavenInstall /* Archive the test results */ step([ $ class : ' JUnitResultArchiver ' , testResults : ' **/target/surefire-reports/TEST-*.xml ' ]) } } } parallel testGroups } /* Run Maven */ void mvn( def args) { /* ... */ }"},{"title":"Jenkins Pipeline Scalability in the Enterprise","tags":"ciandcd","url":"http://ciandcd.github.io/jenkins-pipeline-scalability-in-the-enterprise.html","text":"From: http://feedproxy.google.com/~r/ContinuousBlog/~3/Jmt7It6o5TY/ Implementing a CI/CD solution based on Jenkins has become very easy. Dealing with hundreds of jobs? Not so much. Having to scale to thousands of jobs? Now this is a real challenge. This is the story of a journey to get out of the jungle of jobs…​ Start of the journey At the beginning of the journey there were several projects using roughly the same technologies. Those projects had several branches, for maintenance of releases, for new features. In turn, each of those branches had to be carefully built, deployed on different platforms and versions, promoted so they could be tested for functionalities, performances and security, and then promoted again for actual delivery. Additionally, we had to offer the test teams the means to deploy any version of their choice on any supported platform in order to carry out some manual tests. This represented, for each branch, around 20 jobs. Multiply this by the number of branches and projects, and there you are: more than two years after the start of the story, we had more than 3500 jobs. 3500 jobs. Half a dozen people to manage them all…​"},{"title":"Jenkins World Agenda is Live!","tags":"ciandcd","url":"http://ciandcd.github.io/jenkins-world-agenda-is-live.html","text":"From: http://feedproxy.google.com/~r/ContinuousBlog/~3/QycUsTgE6ew/ The objective of this session is to help you assess your level of readiness for the certification exam - either the Certified Jenkins Engineer (CJE/open source) certification or the Certified CloudBees Jenkins Platform Engineer (CCJPE/CloudBees-specific) certification. After an overview about the certification program, a Jenkins expert from CloudBees will walk you through the various sections of the exam, highlighting the important things to master ahead of time, not only from a pure knowledge perspective but also in terms of practical experience. This will be an interactive session."},{"title":"Support Core Plugin Improvements","tags":"ciandcd","url":"http://ciandcd.github.io/support-core-plugin-improvements.html","text":"From: http://feedproxy.google.com/~r/ContinuousBlog/~3/QGNUNOMUA4Q/ Automatic bundles: Bundles which are generated and get saved in $JENKINS_HOME/support once per hour starting 15 seconds after Jenkins starts the plugin. The automatic bundles are retained using an exponential aging strategy. Therefore it's possible to have a bunch of them over the entire lifetime after the plugin installing the plugin."},{"title":"Upcoming June Jenkins Events","tags":"ciandcd","url":"http://ciandcd.github.io/upcoming-june-jenkins-events.html","text":"From: http://feedproxy.google.com/~r/ContinuousBlog/~3/UkC6ZbWPxX0/ It is hard to believe that the first half of 2016 is almost over and summer is just around the corner. As usual, there are plenty of educational Jenkins events planned for this month. Below lists what's happening in your neck of the woods:"},{"title":"Usage Statistics Analysis","tags":"ciandcd","url":"http://ciandcd.github.io/usage-statistics-analysis.html","text":"From: http://feedproxy.google.com/~r/ContinuousBlog/~3/f3Cqs4raeOc/ This method is based on a properties or the content of the item for example recommending items that are similar to the those that a user liked in the past or examining in the present based upon some properties. Here, we are utilizing Jenkins plugin dependency graph to learn about the properties of a plugin. This graph tells us about dependent plugins on a given plugin as well as its dependencies on others. Here is an example to show, how this graph is use for content based filetring, suppose if a user is using \"CloudBees Cloud Connector\", then we can recommend them for \"CloudBees Registration Plugin\" as both plugins are dependent on \"CloudBees Credentials Plugin\"."},{"title":"Save up to 90% of CI cost on AWS with Jenkins and EC2 Spot Fleet","tags":"ciandcd","url":"http://ciandcd.github.io/save-up-to-90-of-ci-cost-on-aws-with-jenkins-and-ec2-spot-fleet.html","text":"From: http://feedproxy.google.com/~r/ContinuousBlog/~3/OO07WUzjFas/ After you have finished the previous step, you can view the EC2 Fleet Status in the left hand navigation pane on the Jenkins dashboard. Now, as you submit more jobs, Jenkins will automatically scale your Spot fleet to add more nodes. You can view these new nodes executing jobs under the Build Executor Status. After the jobs are done, if the nodes remain free for the specified idle time (configured in the previous step), then Jenkins releases the nodes, automatically scaling down your Spot fleet nodes."},{"title":"OOO (Out of Office)...","tags":"scm","url":"http://ciandcd.github.io/ooo-out-of-office.html","text":"From: https://www.perforce.com/blog/160607/ooo-out-office Ever looked into someone's calendar and see the OOO message – and wondered where they were? Well, here are some of the places we (Paul Allen, Senior Integration Engineer at Perforce and Sven Erik Knop, Senior Technical Specialist at Perforce) ended up on our visits to the Alameda office in California. Having explored downtown San Francisco to excess, we decided to expand our horizon and explore the national parks in California. Mt. Shasta First on the list was the big daddy, Mt. Shasta, a volcano that at 4,322 m (14,179 feet) feet stands out like a sore thumb in the landscape of Northern California, 5 hours north of San Francisco. It was the beginning of June and, this being California, we expected to be able to stroll up the mountain in shorts and T-Shirts. Or not. Turns out we were greeted with 2 meters (7 feet) of snow at the trail base. Thankfully we were paranoid enough to bring our snow gear, rented crampons and set off. We did not make it quite to the top, a blizzard with gale-force winds blowing down the mountain finally forced us down an hour from the summit, but we had a great day out anyway. Lake Tahoe Two years later, we decided to take an easier route and travel to Lake Tahoe instead. Wanting to maximize our weekend, we drove straight up after landing at SFO. Since both of us live in the UK (time difference of 8 hours), when we arrived at midnight, our body clocks insisted it was 8am in the morning. Not the best start for what turned out to be a more difficult hike than anticipated. See, if you hike with Paul, snow is always an option, even if it does not look this way to begin with. Still, the scenery rewarded us for the efforts of an 8-hour hike. We completed the trip the next day with a nice (and gentle) bike ride along the lake. Yosemite The next year I put my foot down: no more snow. Instead, we were joined by our colleague Jason Novecosky, Director of BC Operations at Perforce from our Canadian office, and we decided to travel down to the famous Yosemite National Park in the Sierra Nevada mountain and were greeted with spectacular views. Lassen Volcanic The \"failure\" of reaching the summit of Mt. Shasta wouldn't let us rest- we wanted to reach the top of a volcano. So we set our sights on a slightly smaller mountain: Lassen Peak in the Lassen Volcanic National Park. At 3,189 m (10,457 feet) this was considerably less challenging than Mt. Shasta and was also in a much more forgiving environment with respect to the weather, but there were a lot of forest fires. We were told that we would not see a thing if we climbed up the peak because of the smoke. Turns out the advice was wrong- we had some spectacular views again. On the way back to San Francisco we drove to the coast, now through thick smoke and haze due to the fires on our visit the redwoods (the tallest trees in the world). If you have a list of incredible drives that you want to take in your life, add the Avenue of the Giants. Simpy mind-boggling. Sequoia Redwood trees are great (literally) but compared to Sequoia trees they seem tiny. Sequoia trees are the largest trees (by volume) in the world, and the best place to see them is in the Sequoia National Park (5 hours south of San Francisco), also in the Sierra Nevada mountain range. They did promise us snow again (thanks, Paul) but in the end, it turned out to be just rain and fog. We ended up being unable to see many of the views, but we did see some very big trees. Next trip? We only just scratched the surface of the fantastic outdoors California has to offer. If you have any comments about our trips or if you have any suggestions where to go next, give us a shout at @p4sven or @pallen_tweet."},{"title":"What Are Your Options for a High Availability and Disaster Recovery Plan","tags":"scm","url":"http://ciandcd.github.io/what-are-your-options-for-a-high-availability-and-disaster-recovery-plan.html","text":"From: https://www.perforce.com/blog/160602/what-are-options-high-availability-disaster-recovery-plan There are a lot of things to consider when planning an enterprise Perforce server deployment. Two of those include High Availability and Disaster Recovery. When you think about your servers going down, your first instinct is probably to think about how lost or corrupted data will be recovered. But before you even begin to think about losing data, you should think about putting a plan in place to quickly recover the systems when they go down. In a Helix Enterprise environment, that plan must incorporate not only Helix P4D, but also the other range of applications that are part of your solution, like Git Fusion and GitSwarm. What options are available to construct a High Availability and Disaster Recovery plan for a Helix deployment that consists of Helix P4D, Git­Fusion, and GitSwarm? This Knowledge Base article, \" Helix High Availability/Reliability \" provides a list of options to choose from to meet the requirements of your deployment. The options provide a range of data protection features that provide backup, high availability, compliance, and disaster recovery solutions to safeguard critical data. Once you've selected the option that best meets your needs, the next step is to document and test the recovery process to assure your plan is ready. If you have further questions please contact support@perforce.com or if you need assistance in setting up a HA/DR plan, please contact consulting@perforce.com."},{"title":"Automatic Plugin Documentation","tags":"ciandcd","url":"http://ciandcd.github.io/automatic-plugin-documentation.html","text":"From: http://feedproxy.google.com/~r/ContinuousBlog/~3/hzDOw1V-x1g/ I am Cynthia Anyango from Nairobi, Kenya. I am a second year student at Maseno University. I am currently specializing on Ruby on Rails and trying to learn Python. I recently started contributing to Open source projects.My major contribution was at Mozilla, where I worked with the QA for Cloud services. I did manual and automated tests for various cloud services. I wrote documentation too. Above that, I am competent and I am always passionate about what I get my hands on. Project summary Currently Jenkins plugin documentation is being stored in Confluence. Sometimes the documentation is scattered and outdated. In order to improve the situation we would like to follow the documentation-as-code approach and to put docs to plugin repositories and then publish them on the project website using the awestruct engine. The project aims an implementation of a documentation continuous deployment flow powered by Jenkins and Pipeline Plugin. The idea is to automatically pull in the README and other docs from GitHub, show changelogs with versions and releases dates. I will be designing file templates that will contain most of the docs information that will be required from plugin developers. Initially the files will be written in AsciiDoc . Plugin developers will get a chance to review the templates. The templates will be prototyped by various plugin developers. The docs that will be automatically pulled from github and will be published on Jenkins.io under the Documentation section. My mentors are R.Tyler and Baptiste Mathus I hope to achieve this by 25th June when we will be having our mid-term evaluations. I will update more on the progress."},{"title":"New display of Pipeline's \"snippet generator\"","tags":"ciandcd","url":"http://ciandcd.github.io/new-display-of-pipelines-snippet-generator.html","text":"From: http://feedproxy.google.com/~r/ContinuousBlog/~3/s9rS1mBGz8g/ Those of you updating the Pipeline Groovy plugin to 2.3 or later will notice a change to the appearance of the configuration form. The Snippet Generator tool is no longer a checkbox enabled inside the configuration page. Rather, there is a link Pipeline Syntax which opens a separate page with several options. (The link appears in the project's sidebar; Jenkins 2 users will not see the sidebar from the configuration screen, so as of 2.4 there is also a link beneath the Pipeline definition.) Snippet Generator continues to be available for learning the available Pipeline steps and creating sample calls given various configuration options. The new page also offers clearer links to static reference documentation, online Pipeline documentation resources, and an IntelliJ IDEA code completion file (Eclipse support is unfinished). One motivation for this change ( JENKINS-31831 ) was to give these resources more visual space and more prominence. But another consideration was that people using multibranch projects or organization folders should be able to use Snippet Generator when setting up the project, before any code is committed. Those using Pipeline Multibranch plugin or organization folder plugins should upgrade to 2.4 or later to see these improvements as well."},{"title":"a new user experience for Jenkins","tags":"ciandcd","url":"http://ciandcd.github.io/a-new-user-experience-for-jenkins.html","text":"From: http://feedproxy.google.com/~r/ContinuousBlog/~3/ulSArRaMfCo/ In recent years developers have become rapidly attracted to tools that are not only functional but are designed to fit into their workflow seamlessly and are a joy to use. This shift represents a higher standard of design and user experience that Jenkins needs to rise to meet. We are excited to share and invite the community to join us on a project we've been thinking about over the last few months called Blue Ocean. Blue Ocean is a project that rethinks the user experience of Jenkins, modelling and presenting the process of software delivery by surfacing information that's important to development teams with as few clicks as possible, while still staying true to the extensibility that is core to Jenkins. While this project is in the alpha stage of development, the intent is that Jenkins users can install Blue Ocean side-by-side with the Jenkins Classic UI via a plugin. Not all the features listed on this blog are complete but we will be hard at work over the next few months preparing Blue Ocean for general use. We intend to provide regular updates on this blog as progress is made. Blue Ocean is open source today and we invite you to give us feedback and to contribute to the project. Blue Ocean will provide development teams: New modern user experience The UI aims to improve clarity, reduce clutter and navigational depth to make the user experience very concise. A modern visual design gives developers much needed relief throughout their daily usage and screens respond instantly to changes on the server making manual page refreshes a thing of the past. Advanced Pipeline visualisations with built-in failure diagnosis Pipelines are visualised on screen along with the steps and logs to allow simplified comprehension of the continuous delivery pipeline – from the simple to the most sophisticated scenarios. Scrolling through 10,000 line log files is a thing of the past. Blue Ocean breaks down your log per step and calls out where your build failed. Branch and Pull Request awareness Modern pipelines make use of multiple Git branches, and Blue Ocean is designed with this in mind. Drop a Jenkinsfile into your Git repository that defines your pipeline and Jenkins will automatically discover and start automating any Branches and validating Pull Requests. Jenkins will report the status of your pipeline right inside Github or Bitbucket on all your commits, branches or pull requests. Personalised View Favourite any pipelines, branches or pull requests and see them appear on your personalised dashboard. Intelligence is being built into the dashboard. Jobs that need your attention, say a Pipeline waiting for approval or a failing job that you have recently changed, appear on the top of the dashboard. You can read more about Blue Ocean and its goals on the project page and developers should watch the Developers list for more information. For Jenkins developers and plugin authors: Jenkins Design \"Language\" The Jenkins Design Language (JDL) is a set of standardised React components and a style guide that help developers create plugins that retain the look and feel of Blue Ocean in an effortless way. We will be publishing more on the JDL, including the style guide and developer documentation, over the next few weeks. Modern JavaScript toolchain The Jenkins plugin tool chain has been extended so that developers can use ES6 , React , NPM in their plugins without endless yak-shaving. Jenkins js-modules are already in use in Jenkins today, and this builds on this, using the same tooling. Client side Extension points Client Side plugins use Jenkins plugin infrastructure. The Blue Ocean libraries built on ES6 and React.js provide an extensible client side component model that looks familiar to developers who have built Jenkins plugins before. Client side extension points can help isolate failure, so one bad plugin doesn't take a whole page down. Server Sent Events Server Sent Events (SSE) allow plugin developers to tap into changes of state on the server and make their UI update in real time ( watch this for a demo ). To make Blue Ocean a success, we're asking for help and support from Jenkins developers and plugin authors. Please join in our Blue Ocean discussions on the Jenkins Developer mailing list and the #jenkins-ux IRC channel on Freenode!"},{"title":"GSoC Project Intro: Improving Job Creation","tags":"ciandcd","url":"http://ciandcd.github.io/gsoc-project-intro-improving-job-creation.html","text":"From: http://feedproxy.google.com/~r/ContinuousBlog/~3/vq_a-JXFsJI/ Jenkins has a lot of windows reloads that may time consuming. The creation of new job is a simple process requiring only job name and job type. This way UI may be improved by reducing page reloads and putting new job creation interface in a dialog window. Such popup would likely consist of three steps of implementation: rendering a dialog window, receiving JSON with job types, sending a POST request to create the job. Initially, job validation was unresponsive, job creation was still allowed with an invalid name, and some allowed characters even crashed Jenkins. Happily, two of this problems were fixed in recent improvements and I plan add only a real time name check for invalid characters. Configuration page Changing help information As reported by some users, it would be useful to have the functionality to change help information. Installation administrators would be able to change the help info and choose editing rights for other users. That would likely require a creation of extension points and a plugin using them. I also would like to include the ability to style the help information using markdown as shown above. [Optional] The functionality is extended to creation of crowd sourced \"wiki like\" documentation As in localization plugin the changes are gathered and applied beyond installation of a particular user. More intuitive configuration page. Pursuing to solve this issue Although there are a lot improvements in new configuration page, there is always a room for improvements. An advanced job still has a very complicated and hard to read configuration page. It is still open to discussion, but I may approach it by better division of configuration parts such as an accordion based navigation."},{"title":"Take Two: The Changing Role of Release Engineering in a DevOps World","tags":"scm","url":"http://ciandcd.github.io/take-two-the-changing-role-of-release-engineering-in-a-devops-world.html","text":"From: https://www.perforce.com/blog/160525/take-two-changing-role-release-engineering-devops-world Did you miss us at MERGE 2016? No worries, because we're bringing the conference experience straight to you! Revisit The Changing Role of Release Engineering in a DevOps World by attending our webinar on June 1st at 10:00 a.m. PDT with J. Paul Reed of Release Engineering Approaches, and Perforce's own John Williston! The rise of DevOps is revitalizing age-old topics in release engineering and application lifecycle management, and aspects of software delivery that DevOps doesn't magically solve. If you're responsible for the release engineering function in your organization, see what the new world looks like and which aspects of the industry it's leaving behind. So don't shy away from the challenges on the road towards DevOps and Release Engineering's happy union, and instead attack the hoard of new tools and methodologies with a powerful battle cry. Simply put, learn to thrive in this brave, new world. Here's what you can expect from J. Paul Reed aka Preed's mind-blowing broadcast. In this 30-minute webinar you will learn: · How to define DevOps and it's tools for success · How Release Engineering and DevOps collide, and their benefits · How continuous delivery will define your organization in the future Can't wait for your second chance at MERGE content? Save your seat now ! About our presenters: Presented by J. Paul Reed, Principal Consultant, Release Engineering Approaches J. Paul Reed has over 15 years' experience in the trenches as a build/release and tools engineer, working with such organizations as VMware, Mozilla, Postbox, and Symantec. In 2012, he founded Release Engineering Approaches, a consultancy incorporating a host of tools and techniques to help organizations \"Simply Ship. Every time.\" He's worked across a number of industries, from financial services to cloud-based infrastructure, with teams from 2 to 2,000 on everything from tooling, operational analysis and improvement, team culture transformation, and business value optimization. Hosted by John Williston, Product Marketing Manager, Perforce John is a veteran software developer for Windows, .NET, and the web as well as a musician, philosopher, gamer and all around self-admitted geek. As a Product Marketing Manager and Developer Evangelist at Perforce, he spends much of his time bringing those skills to bear on the challenges of marketing. Follow him on Twitter @p4jbw."},{"title":"Refactoring a Jenkins plugin for compatibility with Pipeline jobs","tags":"ciandcd","url":"http://ciandcd.github.io/refactoring-a-jenkins-plugin-for-compatibility-with-pipeline-jobs.html","text":"From: http://feedproxy.google.com/~r/ContinuousBlog/~3/GH2vCWRAagA/ In this blog post, I'm going to attempt to provide some step-by-step notes on how to refactor an existing Jenkins plugin to make it compatible with the new Jenkins Pipeline jobs. Before we get to the fun stuff, though, a little background. Spoiler: if you're just interested in looking at the individual git commits that I made on may way to getting the plugin working with Pipeline, have a look at this github branch . Eventually, I got it all sorted out. So, in hopes of saving the next person a little time, and encouraging plugin authors to invest the time to get their plugins working with Pipeline, here are some notes about what I learned. As best as I could tell from my Googling, the plugin was probably going to require some modifications in order to be able to be used with Pipeline jobs. However, I wasn't able to find any really cohesive documentation that definitively confirmed that or explained how everything fits together. So everything's going GREAT up to this point. I'm really happy with how it's all shaping up. But then…​ (you knew there was a \"but\" coming, right?) I started trying to figure out how to add the Gatling Jenkins plugin to the Pipeline jobs, and kind of ran into a wall. Over the last few days I've been putting some effort into getting things more automated and repeatable so that we can really maximize the value that we're getting out of the performance tests. With some encouragement from the fine folks in the #jenkins IRC channel , I ended up exploring the JobDSL plugin and the new Pipeline jobs . Combining those two things with some Puppet code to provision a Jenkins server via the jenkins puppet module gave me a really nice way to completely automate my Jenkins setup and get a seed job in place that would create my perf testing jobs. And the Pipeline job format is just an awesome fit for what I wanted to do in terms of being able to easily monitor the stages of my performance tests, and to make the job definitions modular so that it would be really easy to create new performance testing jobs with slight variations. Recently, I started working on a project to automate some performance tests for my company's products. We use the awesome Gatling load testing tool for these tests, but we've largely been handling the testing very manually to date, due to a lack of bandwidth to get them automated in a clean, maintainable, extensible way. We have a years-old Jenkins server where we use the gatling jenkins plugin to track the history of certain tests over time, but the setup of the Jenkins instance was very delicate and not easy to reproduce, so it had fallen into a state of disrepair. Creating a pipeline step The main task that the Gatling plugin performs is to archive Gatling reports after a run. I figured that the end game for this exercise was that I was going to end up with a Pipeline \"step\" that I could include in my Pipeline scripts, to trigger the archiving of the reports. So my first thought was to look for an existing plugin / Pipeline \"step\" that was doing something roughly similar, so that I could use it as a model. The Pipeline \"Snippet Generator\" feature (create a pipeline job, scroll down to the \"Definition\" section of its configuration, and check the \"Snippet Generator\" checkbox) is really helpful for figuring out stuff like this; it is automatically populated with all of the steps that are valid on your server (based on which plugins you have installed), so you can use it to verify whether or not your custom \"step\" is recognized, and also to look at examples of existing steps. Looking through the list of existing steps, I figured that the archive step was pretty likely to be similar to what I needed for the gatling plugin: So, I started poking around to see what magic it was that made that archive step show up there. There are some mentions of this in the pipeline-plugin DEVGUIDE.md and the workflow-step-api-plugin README.md , but the real breakthrough for me was finding the definition of the archive step in the workflow-basic-steps-plugin source code . With that as an example, I was able to start poking at getting a gatlingArchive step to show up in the Snippet Generator. The first thing that I needed to do was to update the gatling-plugin project's pom.xml to depend on a recent enough version of Jenkins, as well as specify dependencies on the appropriate pipeline plugins Once that was out of the way, I noticed that the archive step had some tests written for it, using what looks to be a pretty awesome test API for pipeline jobs and plugins. Based on those archive tests , I added a skeleton for a test for the gatlingArchive step that I was about to write. Then, I moved on to actually creating the step . The meat of the code was this: public class GatlingArchiverStep extends AbstractStepImpl { @DataBoundConstructor public GatlingArchiverStep() {} @Extension public static class DescriptorImpl extends AbstractStepDescriptorImpl { public DescriptorImpl() { super (GatlingArchiverStepExecution.class); } @Override public String getFunctionName() { return \" gatlingArchive \" ; } @Nonnull @Override public String getDisplayName() { return \" Archive Gatling reports \" ; } } } Note that in that commit I also added a config.jelly file. This is how you define the UI for your step, which will show up in the Snippet Generator. In the case of this Gatling step there's really not much to configure, so my config.jelly is basically empty. With that (and the rest of the code from that commit) in place, I was able to fire up the development Jenkins server (via mvn hpi:run , and note that you need to go into the \"Manage Plugins\" screen on your development server and install the Pipeline plugin once before any of this will work) and visit the Snippet Generator to see if my step showed up in the dropdown: GREAT SUCCESS! This step doesn't actually do anything yet, but it's recognized by Jenkins and can be included in your pipeline scripts at that point, so, we're on our way!"},{"title":".NET 4.6.1, WCF and self signed X509 certificates","tags":"ciandcd","url":"http://ciandcd.github.io/net-461-wcf-and-self-signed-x509-certificates.html","text":"From: https://www.finalbuilder.com/resources/blogs/postid/746/net-461-wcf-and-self-signed-x509-certificates If you use self signed X509 certificates and target the .net framework 4.6.1 then you are in some fun, especially if you used makecert to generate the certificate. There is a change in behaviour in the way certificates are validated, which will leave you pulling you hair out for hours. The error you may encounter will look something like this : \"The Identity check failed for the outgoing message. The remote endpoint did not provide a domain name system (DNS) claim and therefore did not satisfied DNS identity 'localhost'. This may be caused by lack of DNS or CN name in the remote endpoint X.509 certificate's distinguished name.\" If you google the error message, you will find plenty of references to using a DnsEndpointIdentity, only in our case, we were already doing exactly as the answers on stackoverflow were suggesting! Since we were migrating from .net 4.0 to .net 4.6.1, I started looking for info on the changes in each version of the .net framework. Eventually, I came across this page : https://msdn.microsoft.com/en-us/library/mt620030(v=vs.110).aspx This was the only mention of X509 certificates I could find in the change history, but it seems like it could be related, so I tried what it suggested, and low and behold, problem solved! With some further investigation of this work around, I found some issueson the wcf github repo with several references to the behaviour of certificate validation. https://github.com/dotnet/wcf/issues/321 https://github.com/dotnet/wcf/pull/603 So it turns out, in .NET 4.6.1 they broke the certificate validation, by only looking at the Subject Alternate Name (SAN) extension, and not falling back to the Subject Name CN field as it should. The pull request I linked to above, is for the WCF for .NET core, not 4.6.1 - so I have no way of knowing if this will be fixed for 4.6.x. Whilst an app.config change can work around the issue, the real fix is to generate certificates that include SAN extension. Generating a certificate without MakeCert Makecert.exe, which is commonly used (on windows at least) does not support the SAN extension, M akecert is deprecated, so it's unlikely there will be an update to make it able to generate certificates with the SAN certificate extension. Windows 8/Server 2012R2 & later versions of Windows include a new powershell cmdlet to generate certificates. For Windows 7 the best option is this : https://gallery.technet.microsoft.com/scriptcenter/Self-signed-certificate-5920a7c6 This appears to what the Windows 8 & later cmdlet is based on, it's relatively simple to use and generates certificates that validate properly with WCF on .NET 4.6.1 New-SelfSignedCertificateEx -Subject \"CN=MyServer\" -KeySpec Exchange -KeyUsage \"DataEncipherment, KeyEncipherment, DigitalSignature\" -Path c:\\certs\\example.pfx -Exportable -SAN \"MyServer\" -SignatureAlgorithm sha256 -AllowSMIME -Password (ConvertTo-SecureString \"abc123dontuseme\" -AsPlainText -Force) -NotAfter (get-date).AddYears(20) Note the above command is on multiple lines to make it easier to read, it can be on one line. We have been working on moving Continua CI to .net 4.6.1 for a future release, and during this conversion (so far, mostly just updating nuget packages), we discovered an issue that turned out to be caused by a change to .net certificate validation.If you use self signed X509 certificates and target the .net framework 4.6.1 then you are in some fun, especially if you used makecert to generate the certificate. There is a change in behaviour in the way certificates are validated, which will leave you pulling you hair out for hours. The error you may encounter will look something like this :\"The Identity check failed for the outgoing message. The remote endpoint did not provide a domain name system (DNS) claim and therefore did not satisfied DNS identity 'localhost'. This may be caused by lack of DNS or CN name in the remote endpoint X.509 certificate's distinguished name.\"If you google the error message, you will find plenty of references to using a DnsEndpointIdentity, only in our case, we were already doing exactly as the answers on stackoverflow were suggesting! Since we were migrating from .net 4.0 to .net 4.6.1, I started looking for info on the changes in each version of the .net framework. Eventually, I came across this page :This was the only mention of X509 certificates I could find in the change history, but it seems like it could be related, so I tried what it suggested, and low and behold, problem solved! With some further investigation of this work around, I found some issueson the wcf github repo with several references to the behaviour of certificate validation.So it turns out, in .NET 4.6.1 they broke the certificate validation, by only looking at the Subject Alternate Name (SAN) extension, and not falling back to the Subject Name CN field as it should. The pull request I linked to above, is for the WCF for .NET core, not 4.6.1 - so I have no way of knowing if this will be fixed for 4.6.x.Whilst an app.config change can work around the issue, the real fix is to generate certificates that include SAN extension.akecert is deprecated, so it's unlikely there will be an update to make it able to generate certificates with the SAN certificate extension. Windows 8/Server 2012R2 & later versions of Windows include a new powershell cmdlet to generate certificates. For Windows 7 the best option is this :This appears to what the Windows 8 & later cmdlet is based on, it's relatively simple to use and generates certificates that validate properly with WCF on .NET 4.6.1Note the above command is on multiple lines to make it easier to read, it can be on one line."},{"title":"Continua CI 1.8 Released","tags":"ciandcd","url":"http://ciandcd.github.io/continua-ci-18-released.html","text":"From: https://www.finalbuilder.com/resources/blogs/postid/745/continua-ci-18-released We are pleased to announce that Continua CI 1.8 has been released. It was actually released a couple of days ago, but for anyone who missed it, here is a heads up with and overview of the new features. We'd also like to thank all those who downloaded the beta - the time and effort spent reporting issues helped us to fix some important bugs and is most appreciated. Version 1.8 adds the following features which build upon all the improvements and fixes made to version 1.7.1. Dashboard Filtering We've added a new filter box to the dashboard so you can quickly find the configuration (or project) that you are looking for as you type. Use the shortcut key F on the dashboard pages to focus on the filter box and start typing. Shared Resources Many of you have requested more control over the number of builds which can run concurrently for some configurations. This may be to restrict the number of times a particular tool is run due to a license, memory or processor limit, or to prevent concurrency issues with multiple build stages simultaneously writing to the same file, folder or network resource. You can now allocate quotas to named Shared Resources and specify that builds and stages must acquire a lock on the Shared Resource before running. If all locks are allocated, then the build or stage will wait on the queue until a lock is released. Shared resources can be associated with the server or a particular agent in the Administration pages. Agent shared resources are acquired when selecting an agent to run a stage. Continua will select the agent with the largest available quota of each shared resource. Server shared resources can also be acquired when selecting an agent, or while on the build queue after evaluating configuration conditions. We hope you find that shared resources can provide many different ways to control the build process. Requeue Build Sometimes a build may fail due to an offline network resource, or some logical error in the stage workflow. Up until now, your only option was to re-run a build for the same branch heads. If any new changesets had been committed to the branch since that build, then you are out of luck. The new Requeue Build button on the Build View page allows you to requeue an existing build using the same changesets, variables and queue options. Any changes to the configuration such as stage actions or repositories are taken into account and used for the new build. The new Requeue Build button on the Build View page allows you to requeue an existing build using the same changesets, variables and queue options. Any changes to the configuration such as stage actions or repositories are taken into account and used for the new build. You can also change the priority, comment and variables before requeuing the build. Clicking on the \"Build requeue options\" menu item will open the Queue Options dialog. Persist Build Variables Another common request has been to persist variable values from one build to another build. This may be to keep a count of builds on a particular branch or to flag that some actions have been completed in one build and do not need to be repeated. Continua CI takes a copy of configuration and project variables at the start of each build. These copies are referred to as build variables. Any changes to build variables are normally discarded when the build finishes and cannot be used by other builds. Continua CI takes a copy of configuration and project variables at the start of each build. These copies are referred to as build variables. Any changes to build variables are normally discarded when the build finishes and cannot be used by other builds. The new Persist Build Variable build event handler allows you to save the value of the build variables when specific events happen in the build timeline. This is stored as the value of the configuration variable. Subsequent builds will then pick up this revised value and use it as the initial value of the build variable. As Continua CI allows multiple builds to run concurrently, it is important to control when the variables are overwritten. A later build may run faster and finish before a build which started earlier, causing unexpected results. You can optionally state that a variable should not be persisted if the configuration variable has been modified (e.g. by another build) since a specified build event, such the build start. You can also prevent concurrency issues by using this feature in conjunction with shared resource locks. Other New Features You can now set the Variables display order of variable prompts on the Queue Options dialog. We have provided buttons for cloning Triggers, Repositories and Build Event Handlers. Configuration Conditions can now be disabled. All actions which run external processes now have a Timeout (in seconds) setting. We have also added a new Cake build runner action and a new VSTest unit testing action."},{"title":"DevOps Assessment Tool","tags":"devops","url":"http://ciandcd.github.io/devops-assessment-tool.html","text":"From: http://devops.linuxjournal.com/devops-assessment-tool Are you ready to identify a path for growing competitive advantage through software-driven innovation? Taking this self-assessment will help you transform your application delivery by evaluating your current DevOps environment, practices and strategies and then help you identify choices for improvement. The IBM DevOps assessment will help you and your organization: Get a baseline of your current practices. Explore and outline your improvement options. Gain team agreement on your proficiency descriptions."},{"title":"Automating for Digital Transformation","tags":"devops","url":"http://ciandcd.github.io/automating-for-digital-transformation.html","text":"From: http://devops.linuxjournal.com/urban-code/automating-digital-transformation Automating for Digital Transformation: Tools-driven DevOps and Continuous Software Delivery in the Enterprise. Learn why companies are adopting DevOps to speed software delivery and innovation - Understand why \"speed with quality\" is the real challenge of Continuous Delivery and the importance of automation - Explore DevOps best practices and automation tools in place at high-performing companies."},{"title":"Automise 5 Released!","tags":"ciandcd","url":"http://ciandcd.github.io/automise-5-released.html","text":"From: https://www.finalbuilder.com/resources/blogs/postid/744/automise-5-released Today we are delighted to release the Automise 5, which now includes a gift. Automise Run-time is now free. As explained in the Beta announcement, our continuous delivery cycle has worked well for Automise 4. Producing constant stream of updates and fixes. Now, with significant updates to the internal workings for Automise today we are releasing Automise 5. What's new in Automise 5 Stepping Engine We have undertaken a major rewrite of the internal stepping engine for Automise 5. This has reduced the moving parts, while also enabling extra features to be implemented. In the end this will mean your projects will run faster, consume less resources, and also providing some extra tools for debugging projects. Action List Dependencies Action Lists can now list other action lists that they depend on. For example this allows specifying a UploadAndClean Action List that depends on the Clean and Upload Action Lists. When UploadAndClean is run, if the Clean and Upload Action Lists have not been run they will be. Step into included projects Previously, debugging include project actions meant running the entire included project when stepping over the action. In Automise 5, debugging allows for stepping into the included project. This will open the included project, if is not already open, and start to debug that project. Breakpoint Conditions We have also added to the debugging experience with breakpoint conditions. These work like break point conditions in Visual Studio. They present options for stopping the executing of a script when variables have certain values, or a condition specified becomes true. Conditions can also be the number of passes over the breakpoint. IDE Themes (Light and Dark) After five years we thought it was time Automise got a new coat of paint. We have implemented two new themes, a light and dark theme (defaulting to the dark on first run up). Choose your side wisely… Action List Out Parameters Action Lists parameters can now be defined as an out parameter. These parameters can have their value set inside the action list. On exiting the action list the calling Run Action List action will set the variable assigned as the out variable with that value. This effectively adds the ability to have action lists work as functions and return values calculated in the action list. Project Formats Let's face it, xml files are difficult to diff. To make diffs easier Automise 5 has introduce two major updates to the Automise project file structure. 1. A new DSL project file format (the new default format) (atp5) 2. A new XML project file format (atx5) New Actions We have done some major work to bring new Azure and Amazon S3 actions to you. Some of these being: Amazon S3 - Added bucket delete object, bucket list objects, bucket list, upload directory, and download folder actions. Azure Actions - Added Login, Logout, Config Mode actions. Azure Group Actions - Added Group Create, Group Delete, Group List, Group Log Show, Group Set, Group Show actions. Azure VM - Added VM Capture, VM Create, VM Deallocate, VM Delete, VM List, VM List, VM Quick Create, VM Restart, VM Start, VM Stop actions. Azure Storage - Added Storage File Copy Show, Storage File Copy Start, Storage File Copy Stop, Storage File Delete, Storage File Download, Storage File List, Storage File Upload actions. New License Manager Automise 5 introduces a new license manager that allows you to download licenses directly from the store. It will be presented to you on first load, or when no valid license has been found. You can also get to it from the Help menu. All that you will require to download a license from the store is your store credentials. It will then log in for you, and download a list of licenses that will work for the current version of Automise. If you had a current Automise 4 subscription as of 28 March 2016, you will already have an Automise 5 license waiting for you in your store account. In addition there is a simpler way to get Trial license, all that you require is a valid email address to receive a verification code on."},{"title":"Introducing Continua CI Version 1.8 Beta","tags":"ciandcd","url":"http://ciandcd.github.io/introducing-continua-ci-version-18-beta.html","text":"From: https://www.finalbuilder.com/resources/blogs/postid/743/introducing-continua-ci-version-18-beta This version adds several new features which build upon all the improvements and fixes made to version 1.7.1. Dashboard Filtering This version adds several new features which build upon all the improvements and fixes made to version 1.7.1. We've added a new filter box to the dashboard so you can quickly find the configuration (or project) that you are looking for as you type. Use the shortcut key F on the dashboard pages to focus on the filter box and start typing. Shared Resources Many of you have requested more control over the number of builds which can run concurrently for some configurations. This may be to restrict the number of times a particular tool is run due to a license, memory or processor limit, or to prevent concurrency issues with multiple build stages simultaneously writing to the same file, folder or network resource. You can now allocate quotas to named Shared Resources and specify that builds and stages must acquire a lock on the Shared Resource before running. If all locks are allocated, then the build or stage will wait on the queue until a lock is released. Shared resources can be associated with the server or a particular agent in the Administration pages. Agent shared resources are acquired when selecting an agent to run a stage. Continua will select the agent with the largest available quota of each shared resource. Server shared resources can also be acquired when selecting an agent, or while on the build queue after evaluating configuration conditions. We hope you find that shared resources can provide many different ways to control the build process. Requeue Build Sometimes a build may fail due to an offline network resource, or some logical error in the stage workflow. Up until now, your only option was to re-run a build for the same branch heads. If any new changesets had been committed to the branch since that build, then you are out of luck. The new Requeue Build button on the Build View page allows you to requeue an existing build using the same changesets, variables and queue options. Any changes to the configuration such as stage actions or repositories are taken into account and used for the new build. The new Requeue Build button on the Build View page allows you to requeue an existing build using the same changesets, variables and queue options. Any changes to the configuration such as stage actions or repositories are taken into account and used for the new build. You can also change the priority, comment and variables before requeuing the build. Clicking on the \"Build requeue options\" menu item will open the Queue Options dialog. Persist Build Variables Another common request has been to persist variable values from one build to another build. This may be to keep a count of builds on a particular branch or to flag that some actions have been completed in one build and do not need to be repeated. Continua CI takes a copy of configuration and project variables at the start of each build. These copies are referred to as build variables. Any changes to build variables are normally discarded when the build finishes and cannot be used by other builds. Continua CI takes a copy of configuration and project variables at the start of each build. These copies are referred to as build variables. Any changes to build variables are normally discarded when the build finishes and cannot be used by other builds. The new Persist Build Variable build event handler allows you to save the value of the build variables when specific events happen in the build timeline. This is stored as the value of the configuration variable. Subsequent builds will then pick up this revised value and use it as the initial value of the build variable. As Continua CI allows multiple builds to run concurrently, it is important to control when the variables are overwritten. A later build may run faster and finish before a build which started earlier, causing unexpected results. You can optionally state that a variable should not be persisted if the configuration variable has been modified (e.g. by another build) since a specified build event, such the build start. You can also prevent concurrency issues by using this feature in conjunction with shared resource locks. Other New Features You can now set the Variables display order of variable prompts on the Queue Options dialog. We have provided buttons for cloning Triggers, Repositories and Build Event Handlers. Configuration Conditions can now be disabled. We have also added a new Cake build runner action"},{"title":"Automise 5 Beta","tags":"ciandcd","url":"http://ciandcd.github.io/automise-5-beta.html","text":"From: https://www.finalbuilder.com/resources/blogs/postid/741/automise-5-beta Today we are delighted to release the Automise 5 BETA, which contains our new Stepping Engine and Action List Dependencies as the headline features. For five years we have updated and improved Automise 4 through our continuous delivery cycle. This has worked well. Allowing for improvements to actions (like FTP\\SFTP\\FTPS suite) to come out gradually and consistently. Allowing everyone to pick and choose at which point to take feature updates. The Automise 5 signals a major \"tick\" to this regular flow of updates. The majority of these updates are at the core of what Automise does to solve your automation challenges. What's new in Automise 5 Stepping Engine We have undertaken a major rewrite of the internal stepping engine for Automise 5. This has reduced the moving parts, while also enabled extra features to be implemented. In the end this will mean your projects will run faster, consume less resources, while also providing some extra tools for debugging projects. Action List Dependencies Action Lists now allow for listing of other Actions Lists they are dependent on. Dependencies are always run before the action lists which depend on them. For example this allows specifying a UploadAndClean Action List that depends on the Clean and Upload Action List. When UploadAndClean is run, if the Clean and Upload Action Lists have not been run they will be. Step into included projects Due to the previous version of the stepping engine stepping into included projects was not possible. Instead the user had to wait for the included project to complete before continuing with debugging. Stepping into included projects with Automise 5 will now open the included project, and continue stepping from inside the included project. Breakpoint Conditions Another addition to the debugging experience is breakpoint conditions. These allow stopping the executing of a script at a certain point in time. Conditions can be a number of passes over the breakpoint, or when a variable equals a certain value. IDE Themes (Light and Dark) After five years we thought it was time Automise got a new coat of paint. We have implemented two new themes, a light and dark theme (defaulting to the dark on first run up). Action List Out Parameters Action Lists now allow for retrieving any number of values from them. A variable assigned to the out parameter on the Action List will be given the value of that parameter when the Action List has completed. This will allow for more Action Lists that generate values for use else where in the Automise Project. Project Formats Since the start of Automise the project files have used XML for their structure. As Automise has grown, so too have the elements in the projects XML file. This has placed more strain on those left to diff versions of Automise projects. To aleavate this challenge Automise 5 has introduce two major updates to the Automise project file structure. 1. A new DSL project file format (the new default format) 2. A new XML project file format The new Automise DSL structure is concise, and very simple to diff. project begin projectid = {04710B72-066E-46E7-84C7-C04A0D8BFE18} Action List begin name = Default Action Listid = {E6DE94D6-5484-45E9-965A-DB69885AA5E2} rootaction begin action.group begin id = {D860420B-DE46-4806-959F-8A92A0C86429} end end end end The new Automise XML structure is a great deal less verbose than the older format. <?xml version=\"1.0\" encoding=\"UTF-8\"?> <Automise> <project> <projectid>{6A717C24-D00F-4983-9FD0-148B2C609634}</projectid> <Action List> <name>Default</name> <Action Listid>{E6DE94D6-5484-45E9-965A-DB69885AA5E2}</Action Listid> <rootaction> <action.group> <id>{D860420B-DE46-4806-959F-8A92A0C86429}</id> </action.group> </rootaction> </Action List> </project> </Automise> The new Automise XML structure is a great deal less verbose than the older format. New Actions Not much to report here, most of the focus has been on the Stepping engine and the IDE. We do have some updates to AWS EC2 and Azure in progress, they will most likely be added in an update when they are ready. How do I get the Beta? Links to the beta downloads will be published to the Automise Downloads page. What if I find a bug? Email support (please added Beta to the subject). When reporting an issue, be sure to include the beta build number and details about your environment. Please test with the latest beta build before reporting bugs. We are particularly keen for people to load up their existing projects from older (ie 4 or earlier) versions of Automise, save them in AT5 format, and load them again and confirm that everything loaded ok. When will it be released? When it's ready ;) Seriously, though, we expect the release to happen in the next few weeks. Automise 5 is based on FinalBuilder 8, which has been out for several months now and is quite stable."},{"title":"Code Signing Changes for 2016","tags":"ciandcd","url":"http://ciandcd.github.io/code-signing-changes-for-2016.html","text":"From: https://www.finalbuilder.com/resources/blogs/postid/742/code-signing-changes-for-2016 What do I need to do? First things first, check your code signing certificate. If it's current, and uses SHA1, contact your certificate issuer for a replacement SHA256 certificate. Most issuers have a formal process for this since this is something they have known about for a while and should not charge extra (KSoftware/Comodo do not charge). In our case, our certificate was renewed in Nov 2014 and was already an SHA256 certificate. What if I need to support Windows XP/Server 2003? Windows XP & Server 2003 do not support SHA256, so this deprecation of SHA1 does not apply to those versions of windows. If you sign with your SHA256 certificate using the SHA256 digest algorithm, you will find you code is not trusted on those versions of windows. The trick is to use the SHA1 digest algorithm. So do I need separate installers for XP and Windows 7+ ? Well that's one way to do it, but you can support XP and windows 7+ with a single installer or exe, by signing twice, with SHA1 and SHA256. NOTE: If you are a long time FinalBuilder user and still using the Authenticode action, then don't, it's been deprecated for some time as it uses the Recent versions of In my experiments, I found that you need to sign with SHA1 first, then SHA256. The reason for this is that WinXP only looks at the first signature and would not recognise the timestamps from any RFC3161 timestamp servers that I tried. The signtool options that allow adding additional signatures (/as for signing, /tp for timestamping) only work with RFC3161 compliant timestamp servers, so the SHA1 signature and timestamp must be done first since we can't use /as or /tp with a non RFC3161 timestamp server. Sign, then TimeStamp Whilst signtool can sign and timestamp in a single operation (and the By doing the timestamp operation separately, we can retry if timestamping fails. Often, just a few seconds between retries is enough (unless your internet connection is down), and there is always the option of using a different timestamp server. So the order of events is : Sign SHA1. Sign SHA256 (with append signature /as). Timestamp SHA1 - using older style authenticode timestamp server. Timestamp SHA256 (/tp with index 1 ) - using an RFC3161 compliant timestamp server. Show me how! I have created an examples repository on github : References https://www.comodo.com/e-commerce/SHA-2-transition.php http://social.technet.microsoft.com/wiki/contents/articles/32288.windows-enforcement-of-authenticode-code-signing-and-timestamping.aspx \\ Microsoft announced some time ago that windows 7 & higher would no longer trust anything that is code signed with an SHA1 ( https://en.wikipedia.org/wiki/SHA-1 ) certificate as of 1st Jan 2016. The reason for this is well documented, SHA1 has become increasingly vulnerable and is no longer secure enough to be trusted.First things first, check your code signing certificate. If it's current, and uses SHA1, contact your certificate issuer for a replacement SHA256 certificate. Most issuers have a formal process for this since this is something they have known about for a while and should not charge extra (KSoftware/Comodo do not charge). In our case, our certificate was renewed in Nov 2014 and was already an SHA256 certificate.Windows XP & Server 2003 do not support SHA256, so this deprecation of SHA1 does not apply to those versions of windows. If you sign with your SHA256 certificate using the SHA256 digest algorithm, you will find you code is not trusted on those versions of windows. The trick is to use the SHA1 digest algorithm.Well that's one way to do it, but you can support XP and windows 7+ with a single installer or exe, by signing twice, with SHA1 and SHA256.NOTE: If you are a long time FinalBuilder user and still using the Authenticode action, then don't, it's been deprecated for some time as it uses the deprecated capicom.dll api . The only reason we haven't removed it is to avoid errors when you load your old projects. The correct actions to use for code signing are the SignTool actions.Recent versions of Signtool.exe include a switch (/as) to append a signature ( the default operation is to replace the primary signature). I believe the windows 8.1 sdk was the first version to include this option (and other related options).In my experiments, I found that you need to sign with SHA1 first, then SHA256. The reason for this is that WinXP only looks at the first signature and would not recognise the timestamps from any RFC3161 timestamp servers that I tried. The signtool options that allow adding additional signatures (/as for signing, /tp for timestamping) only work with RFC3161 compliant timestamp servers, so the SHA1 signature and timestamp must be done first since we can't use /as or /tp with a non RFC3161 timestamp server.Whilst signtool can sign and timestamp in a single operation (and the SignTool Sign action in FinalBuilder can too), I prefer to do the timestamp step separately. The reason for this is that signing rarely fails (typically only when the certificate has expired or you get the password wrong!), but timestamping fails often, because the timestamp server may be unreachable or it just has some issue and doesn't respond correctly.By doing the timestamp operation separately, we can retry if timestamping fails. Often, just a few seconds between retries is enough (unless your internet connection is down), and there is always the option of using a different timestamp server.So the order of events is :Sign SHA1.Sign SHA256 (with append signature /as).Timestamp SHA1 - using older style authenticode timestamp server.Timestamp SHA256 (/tp) - using an RFC3161 compliant timestamp server.I have created an examples repository on github : FinalBuilder Examples - you can find a nice example there showing how to double (optionally) sign and then timestamp (with retries). This example requires FinalBuilder 8.0.0.1490 or later (added support for /tp option on timestamp action)."},{"title":"VSoft.CommandLineParser for Delphi","tags":"ciandcd","url":"http://ciandcd.github.io/vsoftcommandlineparser-for-delphi.html","text":"From: https://www.finalbuilder.com/resources/blogs/postid/740/vsoftcommandlineparser-for-delphi-updated A while back I published the VSoft.CommandLineParser library on github, which makes it simple to handle command line options in delphi applications. The first version only did enough to satisfy the needs I had in DUnitX. In another project I'm working on, I needed a command mode, where each command had a unique set of options, but keeping the ability to have global options. I have tried to implement the command mode in a backwards compatable manner, and so far the only change I had to make to an existing project was adding a const to a parameter. Adding Commands Adding commands is quite simple, using TOptionsRegistry.RegisterCommand. cmd := TOptionsRegistry.RegisterCommand('help','h','get some help','','commandsample help [command]'); option := cmd.RegisterUnNamedOption<string>('The command you need help for', procedure(const value : string) begin THelpOptions.HelpCommand := value; end); Note: this method returns a TCommandDefinition record that you can add options to. The reason for using a record rather than an interface here, is because delphi interfaces do not suport generic methods. Records do, so we use the record type as a wrapper around the ICommandDefinition interface. The helpstring parameter allows you to specify a longer help message that can be displayed when showing command usage. Handling Commands The ICommandLineParseResult interface has a new Command property (string) which is used to determine the selected command. It's up to you how to actually run the commands. Showing Usage The PrintUsage method now has some overloads and has some formatting improvements, and TOptionsRegistry also has new EnumerateCommands and EmumerateCommandOptions methods which make it relatively simple to handle showing usage etc yourself if you want to. Where is it? The source with samples is available on GitHub - https://github.com/VSoftTechnologies/VSoft.CommandLineParser"},{"title":"Modifying XML Manifest files with FinalBuilder","tags":"ciandcd","url":"http://ciandcd.github.io/modifying-xml-manifest-files-with-finalbuilder.html","text":"From: https://www.finalbuilder.com/resources/blogs/postid/739/modifying-xml-manifest-files-with-finalbuilder So lets define our XML Document by adding an This topic is something that I pulled from our support system, it's something we get asked about more that once a year, that is, how do I modify the xml manifest file using FinalBuilder. Typically its the assembly version attribute that users want to modify, so that's what I'll show here, but you can use the same technique to edit other parts of the manifest file.So lets define our XML Document by adding an XML Document Define Action , and point it at our manifest file. If you open your manifest file in notpad, you will notice assembly element looks something like this : <assembly xmlns=\"urn:schemas-microsoft-com:asm.v1\" manifestVersion=\"1.0\" > Note the xmlns attribute, this is what causes users problems with the xml actions in FinalBuilder, XML Namespaces. The MXSML Parser is very strict when it comes to namespaces, and it requires that we make use of them when using XPath to select nodes. On the XML Document action, switch to the MSXML Parser tab and in the Extra Namespaces grid, add the following. If you open your manifest file in notpad, you will notice assembly element looks something like this :Note the xmlns attribute, this is what causes users problems with the xml actions in FinalBuilder, XML Namespaces. The MXSML Parser is very strict when it comes to namespaces, and it requires that we make use of them when using XPath to select nodes. On the XML Document action, switch to the MSXML Parser tab and in the Extra Namespaces grid, add the following. What we are doing here is assigning a prefix (x in this case) to the namespace. This prefix will be used in our XPath statements. Add an What we are doing here is assigning a prefix (x in this case) to the namespace. This prefix will be used in our XPath statements.Add an Edit XML File action - set the XML File to use an XML Document and select the document we defined with the previous action. Now we need to define the XPath statement to the version attribute that we are going to modify. And finally, add a Save XML Document Action to save our changes to the file. Note that if you are editing other parts of the manifest file, make sure you add the namespaces and different prefixes, and use this prefixes appropriately in your XPath statements. Note, all of this could be done in a single Edit XML File action, however, if you want to make more than one modification to the manifest file then it's more efficient to use the xml document define action to avoid loading/parsing/saving the document for each edit. And finally, add a Save XML Document Action to save our changes to the file. Note that if you are editing other parts of the manifest file, make sure you add the namespaces and different prefixes, and use this prefixes appropriately in your XPath statements.Note, all of this could be done in a single Edit XML File action, however, if you want to make more than one modification to the manifest file then it's more efficient to use the xml document define action to avoid loading/parsing/saving the document for each edit."},{"title":"Delphi Code Coverage with Continua CI","tags":"ciandcd","url":"http://ciandcd.github.io/delphi-code-coverage-with-continua-ci.html","text":"From: https://www.finalbuilder.com/resources/blogs/postid/738/delphi-code-coverage-with-continua-ci Testing code is something we all do. Whether it be manual usability testing, unit testing, or integration testing, knowing how much of the application is covered by the tests is important. Without knowing what parts of the application are covered, there is no way to know if key features are tested. When performing unit testing there is an analytical way to determine what parts of the source code are covered by the tests. This is typically call source code coverage. Working with Delphi, one of the tools that performs this task is called DelphiCodeCoverage (open source). It can be located on GitHub (more recent fork) and SourceForge . Under the hood this tool simply marks each line of source code as \"hit\" when the application calls it at least once. From there it can generate a detailed report giving the overall coverage statistics for the project, as well as the individual lines not hit in the testing. Code Coverage What I will go through below is how to setup code coverage on a unit test project, and hook that into a continuous integration process using Continua CI. I will assume that if you require knowledge on how to setup a project on Continua CI you will refer to the The code that I would like to get a code coverage report on is the An extra consideration to have with a unit testing project is to make sure it can run under continuous integration. This means that it should run to completion and produce an output file that is able to be imported into the build summary. With this in mind I have created a \"CI\" All this code and other related scripts are located in the DelphiCodeCoverage To generate a code coverage report I decided to use DelphiCodeCoverage. The tool has a number of command line options, all of which are spelt out on the GitHub page. Some of the options are a little overwhelming in the effort they require. An example of this is passing a file that contains all the source directories to search for classes to include in the code coverage report. Thankfully there is a wizard supplied with DelphiCodeCoverage that will help generate a batch file containing the correct parameters to pass to DelphiCodeCoverage. In my project I have placed DelphiCodeCoverage into a sub-folder call \"CodeCoverage\" and included it into source control. There are two reasons I am doing this; 1. The code coverage executable is now available everywhere the source is pulled to. 2. It simplifies the script I will need for the CI Server. If your uncomfortable with placing binaries into your source control, this can be altered without affecting the produced report. Running the code coverage wizard your presented with a page to enter the executable, map file, source, and output directory locations. Below are the settings I have used: What I will go through below is how to setup code coverage on a unit test project, and hook that into a continuous integration process using Continua CI. I will assume that if you require knowledge on how to setup a project on Continua CI you will refer to the Create your First Project wiki page.The code that I would like to get a code coverage report on is the Core.Card.pas unit. The unit tests for this class are located in the tests folder and have a corresponding name of Core.CardTests.pas . You may have noticed that some of the code paths are not fully covered in my unit tests. This is intentional, and something that we will come to a little later on.An extra consideration to have with a unit testing project is to make sure it can run under continuous integration. This means that it should run to completion and produce an output file that is able to be imported into the build summary. With this in mind I have created a \"CI\" configuration on my unit testing project. This conditionally compiles the unit testing project so that it does not wait for user input (something my debug configuration does) and generates an XML output file.All this code and other related scripts are located in the VSoftTechnologies/DelphiCodeCoverageExample GitHub repository. Feel free to clone it to get a better sense of code coverage and the project structure I am using.To generate a code coverage report I decided to use DelphiCodeCoverage. The tool has a number of command line options, all of which are spelt out on the GitHub page. Some of the options are a little overwhelming in the effort they require. An example of this is passing a file that contains all the source directories to search for classes to include in the code coverage report. Thankfully there is a wizard supplied with DelphiCodeCoverage that will help generate a batch file containing the correct parameters to pass to DelphiCodeCoverage.In my project I have placed DelphiCodeCoverage into a sub-folder call \"CodeCoverage\" and included it into source control. There are two reasons I am doing this;1. The code coverage executable is now available everywhere the source is pulled to.2. It simplifies the script I will need for the CI Server.If your uncomfortable with placing binaries into your source control, this can be altered without affecting the produced report.Running the code coverage wizard your presented with a page to enter the executable, map file, source, and output directory locations. Below are the settings I have used: The last option for the wizard allows for making all paths relative. This is exactly what we require to have our generated batch file run on any system, however at the time of writing it does not work correctly. This meant that I had to manually change all paths to a version that was relative to the folder in which DelphiCodeCoverage was located. dcov_execute.bat before \"I:\\Examples\\DUnitX_CodeCoverage\\CodeCoverage\\CodeCoverage.exe\" -e \"I:\\Examples\\DUnitX_CodeCoverage\\Win32\\Debug\\DUnitX_And_CodeCoverage.exe\" -m \"I:\\Examples\\DUnitX_CodeCoverage\\Win32\\Debug\\DUnitX_And_CodeCoverage.map\" -uf dcov_units.lst -spf dcov_paths.lst -od \"I:\\Examples\\DUnitX_CodeCoverage\\CodeCoverage\\Output\\\" -lt -html dcov_execute.bat after \"CodeCoverage.exe\" -e \"..\\Win32\\CI\\DUnitX_And_CodeCoverage.exe\" -m \"..\\Win32\\CI\\DUnitX_And_CodeCoverage.map\" -ife -uf dcov_units.lst -spf dcov_paths.lst -od \".\\Output\\\" -lt -html Note: Line breaks are only included above for readability. There are none in the resulting batch files. Another alteration that I have made to the batch file is to include the \"-ife\" option. The option will include file extensions. This means that it will stop a unit like \"Common.Encoding\" being 'converted' to \"Common\". As in my project I have unit called \"Core.Cards.pas\" this option is required to have it included in generated code coverage report. Next the relative path change should be applied to the two generated list files Now that the batch file and list files have been corrected running the dcov_executable.bat should produce summary out similar to that below. Note that the unit test project needs to be compiled as DelphiCodeCoverage runs the unit test executable. ********************************************************************** * DUnitX - (c) 2013 Vincent Parrett * * vincent@finalbuilder.com * * * * License - http://www.apache.org/licenses/LICENSE-2.0 * ********************************************************************** Fixture : Core ------------------------------------------------- Fixture : Core.CardTests ------------------------------------------------- Fixture : Core.CardTests.TCardTest ------------------------------------------------- Test : Core.CardTests.TCardTest.A_Card_FacingUp_Once_Flipped_Is_Facing_Down ------------------------------------------------- Executing Test : A_Card_FacingUp_Once_Flipped_Is_Facing_Down Success. Running Fixture Teardown Method : Destroy Done testing. Tests Found : 1 Tests Ignored : 0 Tests Passed : 1 Tests Leaked : 0 Tests Failed : 0 Tests Errored : 0 Summary: +-----------+-----------+-----------+ | Lines | Covered | Covered % | +-----------+-----------+-----------+ | 15 | 11 | 73 % | +-----------+-----------+-----------+ Continuous Integration With the code coverage batch file we are now able to run code coverage on any system, include on a continuous integration system. Our goal with the continuous integration is to have the unit tests built and run each time a set of code is checked into source control. This will allow us to then track if any unit tests fail, and changes in the code coverage. To achieve this I have create a The last option for the wizard allows for making all paths relative. This is exactly what we require to have our generated batch file run on any system, however at the time of writing it does not work correctly. This meant that I had to manually change all paths to a version that was relative to the folder in which DelphiCodeCoverage was located.Note: Line breaks are only included above for readability. There are none in the resulting batch files.Another alteration that I have made to the batch file is to include the \"-ife\" option. The option will include file extensions. This means that it will stop a unit like \"Common.Encoding\" being 'converted' to \"Common\". As in my project I have unit called \"Core.Cards.pas\" this option is required to have it included in generated code coverage report.Next the relative path change should be applied to the two generated list files dcov_paths.lst and dcov_units.lst . The paths file should be the only one that has path in need of altering to be relative. Both however need to be checked to make sure they contain everything to be covered in the report. If there are source folders missing they need to be added to the dcov_paths.lst file. If there are unit names missing they need to be added to the dcov_units.lst file.Now that the batch file and list files have been corrected running the dcov_executable.bat should produce summary out similar to that below. Note that the unit test project needs to be compiled as DelphiCodeCoverage runs the unit test executable.With the code coverage batch file we are now able to run code coverage on any system, include on a continuous integration system. Our goal with the continuous integration is to have the unit tests built and run each time a set of code is checked into source control. This will allow us to then track if any unit tests fail, and changes in the code coverage.To achieve this I have create a Continua CI configuration that builds my unit test project, runs the unit tests under code coverage, and then import the unit test results into the build summary. The FinalBuilder action calls the FinalBuilder project responsible for compiling the DUnitX unit test project. It uses the CI configuration so that the unit tests executable will run to completion, and will produce an NUnit XML results file in the same directory as the executable. It is important to build the unit tests each time as the source code for our project would have changed each time we run the continuous integration. Note that you do not have to use FinalBuilder, you can also use MSBuild to build your DUnitX Project - see The The FinalBuilder action calls the FinalBuilder project responsible for compiling the DUnitX unit test project. It uses the CI configuration so that the unit tests executable will run to completion, and will produce an NUnit XML results file in the same directory as the executable. It is important to build the unit tests each time as the source code for our project would have changed each time we run the continuous integration. Note that you do not have to use FinalBuilder, you can also use MSBuild to build your DUnitX Project - see Integrating DUnitX Unit Testing with Continua CI The execute program action simply runs the code coverage batch file generated above. This batch file will run the unit test project we compiled and log code coverage information as it does. The result will be a summary written out to our build log while also html files written to the report folder we specified in the batch file. It is these html files which we will attach to the continuous build report a little later. Lastly we want to import the actual unit test results. These are written out by DUnitX as a NUnit compatible XML file which we can import with the \"Import NUnit Tests\" action. The results from the XML file will be attached to the build report presented by Continua CI. Lastly we want to import the actual unit test results. These are written out by DUnitX as a NUnit compatible XML file which we can import with the \"Import NUnit Tests\" action. The results from the XML file will be attached to the build report presented by Continua CI. As all builds for Continua CI are run on agents, and all build reports come from the server, we need to transfer the code coverage report back to the server. This is done through \\Output\\CodeCoverage\\\" the report should appear in \"Source\\Output\\CodeCoverage\\Output\" (Note that $Source.DelphiCodeCoverage.Path$ was mapped to the \\Source\\ folder on the agent). Workspace rules use the greater than symbol to signal the files should be copied from the server to the agent, and the less than symbol to copy from the agent to the server. This therefore leaves use the workspace rule of \"\"/Output/CodeCoverage/ < \\Source\\CodeCoverage\\Output\\*.html\" to get all code coverage report files back to the server. As all builds for Continua CI are run on agents, and all build reports come from the server, we need to transfer the code coverage report back to the server. This is done through workspace rules on the build stage. In this example DelphiCodeCoverage writes out all html report files to a relative directory of \".\\Output\\\". This means if we run the DelphiCodeCoverage batch file from \"Source Now that the html reports are on the server, we need to show them against the Continua CI build. To achieve this we use the Now that the html reports are on the server, we need to show them against the Continua CI build. To achieve this we use the reports section of our Continua CI configuration. The reports section allows us to specify a file to attach to the build as a report to be displayed or offered as a download. In this case we want to display the report summary html file. All reports work from the server point of view, and each build has it own workspace on the server. To this end the report we want to be display would have been copied to \"$Workspace$\\Output\\CodeCoverage\\CodeCoverage_summary.html\". The Code Coverage Report The end report appearing in the report section of the Continua CI build summary. The end report appearing in the report section of the Continua CI build summary. As shown in the report the example project has some code that is not covered during unit testing. This reduces the overall coverage to 73%. If I had more than one unit each would have their own code coverage summary. In addition I could click on each file and get a line by line report to see what section of the unit is not covered. As shown in the report the example project has some code that is not covered during unit testing. This reduces the overall coverage to 73%. If I had more than one unit each would have their own code coverage summary. In addition I could click on each file and get a line by line report to see what section of the unit is not covered. Final Notes It is worth mentioning that code coverage is only one arrow in a software testing quiver. In my example I purposely chose to include code that was not covered. This showed the power of code coverage in picking up where unit testing should potentially be directed to next. I also included code where the unit tests cover the code, however not fully. The code testing Core.Card.Flip only tests one path through the code, not all the possible paths. Currently the test sees if the code works when going from face up to face down, not from face down to face up. Although in this example it might be benign, it shows that other tools are needed to help cover this gap. It is worth mentioning that code coverage is only one arrow in a software testing quiver. In my example I purposely chose to include code that was not covered. This showed the power of code coverage in picking up where unit testing should potentially be directed to next. I also included code where the unit tests cover the code, however not fully. The code testing Core.Card.Flip only tests one path through the code, not all the possible paths. Currently the test sees if the code works when going from face up to face down, not from face down to face up. Although in this example it might be benign, it shows that other tools are needed to help cover this gap."},{"title":"DevOps For Dummies - New Edition with SAFe®","tags":"devops","url":"http://ciandcd.github.io/devops-for-dummies-new-edition-with-safer.html","text":"From: http://devops.linuxjournal.com/devops/devops-dummies-new-edition-safe In this NEW 2nd edition, learn why DevOps is essential for any business aspiring to be lean, agile, and capable of responding rapidly to changing customer and marketplace Download the E-book to learn about: • The business need and value of DevOps. • DevOps capabilities and adoption paths. • How cloud accelerates DevOps. • The Ten DevOps myths • And more Complete the short form to the right to download the eBook!"},{"title":"ABCs of Release and Deploy","tags":"devops","url":"http://ciandcd.github.io/abcs-of-release-and-deploy.html","text":"From: http://devops.linuxjournal.com/devops/abcs-release-and-deploy The bottlenecks in the release and deployment of software applications often start in the handoff from development to operations. This paper discusses how these bottlenecks can be reduced or eliminated by applying a DevOps approach to your release and deployment practices."},{"title":"DevOps For Dummies - New Edition with SAFe® - French","tags":"devops","url":"http://ciandcd.github.io/devops-for-dummies-new-edition-with-safer-french.html","text":"From: http://devops.linuxjournal.com/devops/devops-dummies-new-edition-safe-fr In this NEW 2nd edition, learn why DevOps is essential for any business aspiring to be lean, agile, and capable of responding rapidly to changing customer and marketplace Download the E-book to learn about: • The business need and value of DevOps. • DevOps capabilities and adoption paths. • How cloud accelerates DevOps. • The Ten DevOps myths • And more Complete the short form to the right to download the eBook!"},{"title":"Delphi-Mocks Parameter Matchers","tags":"ciandcd","url":"http://ciandcd.github.io/delphi-mocks-parameter-matchers.html","text":"From: https://www.finalbuilder.com/resources/blogs/postid/737/delphi-mocks-parameter-matchers We recently updated Delphi Mocks to allow for better parameter matching on Expectations registered with the Mock. This allows the developer to place tighter controls on verifying that a mocked interface/object method is called. Below is a simple example of when the parameter matchers can be used. procedure TExample_InterfaceImplementTests.Implement_Multiple_Interfaces; var sutProjectSaver : IProjectSaveCheck; mockProject : TMock<IProject>; begin //Test that when we check and save a project, and its dirty, we save. //CREATE - The project saver under test. sutProjectSaver := TProjectSaveCheck.Create; //CREATE - Mock project to control our testing. mockProject := TMock<IProject>.Create; //SETUP - Mock project will show as dirty and will expect to be saved. mockProject.Setup.WillReturn(true).When.IsDirty; //NEW! - Add expectation that the save will be called as dirty is returning true. // As we don't care about the filename value passed to us we // allow any string to be passed to report this expectation as met. mockProject.Setup.Expect.Once.When.Save(It(0).IsAny<string>()); //TEST - Visit the mock element to see if our test works. sutProjectSaver.Execute(mockProject); //VERIFY - Make sure that save was indeed called. mockProject.VerifyAll; end; Previously the developer writing this test would have to provide the exact filename to be passed to the mocked Save method. As we don't know what the projects filename is going to be (in our example case), we would either have to; 1. Forgo doing this test. 2. Implement a project object to test with. Both of these options are not ideal. Parameter matchers resolve this situation. It is now simple to either restrict or broaden the parameters passed to mocked methods that will satisfy the expectation defined. To achieve this Delphi-Mocks offers eleven new functions; function It(const AParamIndx : Integer) : ItRec; function It0 : ItRec; function It1 : ItRec; function It2 : ItRec; function It3 : ItRec; function It4 : ItRec; function It5 : ItRec; function It6 : ItRec; function It7 : ItRec; function It8 : ItRec; function It9 : ItRec; The first \"function It(const AParamIndx : Integer) : ItRec;\" allows the developer to specify the index of the parameter they wish to set for the next expectation setup of a mock method. It(0) will refer to the first parameter, It(1) the second and so forth. Note that the reason for specifying the parameter index is that Delphi's parameter evaluation order is not defined, so we could not rely on the parameters being evaluated in order (which is what we did when we initially wrote this feature). Interestingly, with the 64 bit Delphi compiler, parameter evaluation does appear to happen in order, but we could not be certain this will always be the case. The other ten functions It0 through to It9 are simply wrappers of the index call passing the index in their name. All these functions return an ItRec. The ItRec has the function structure; ItRec = record var ParamIndex : cardinal; constructor Create(const AParamIndex : Integer); function IsAny<T>() : T ; function Matches<T>(const predicate: TPredicate<T>) : T; function IsNotNil<T> : T; function IsEqualTo<T>(const value : T) : T; function IsInRange<T>(const fromValue : T; const toValue : T) : T; function IsIn<T>(const values : TArray<T>) : T; overload; function IsIn<T>(const values : IEnumerable<T>) : T; overload; function IsNotIn<T>(const values : TArray<T>) : T; overload; function IsNotIn<T>(const values : IEnumerable<T>) : T; overload; {$IFDEF SUPPORTS_REGEX} //XE2 or later function IsRegex(const regex : string; const options : TRegExOptions = []) : string; {$ENDIF} end; Each of the functions creates a different matcher. For example the IsAny<T> will cause the expectation to be met when the parameter passed to the mock is of any value that has the type T. In the example above this type would be a string. You will also notice that each function returns the type T. This is so that each call can be placed within the mock methods call directly. Doing so helps with making sure parameter types match the testing value. IsEqualTo<T> requires that the parameter matches exactly to the value passed into the IsEqualTo<T>. This could be used to restrict the expectation to a tighter test of the functionality under test. //Match on the filename being \"temp.txt\" only. mockProject.Setup.Expect.Once.When.Save(It(0).IsEqualTo<string>('temp.txt')); //VERIFY - Make sure that save was indeed called. mockProject.VerifyAll; In the future we are looking to provide \"And\"\\\"Or\" operators. These operators might also live on the ItRec and allow combining with as many other matchers using the same type. //Match on the filename being \"temp.txt\" or \"temp.doc\" only. mockProject.Setup.Expect.Once.When.Save( It(0).Or(It(0).IsEqualTo<string>('temp.txt'), It(0).IsEqualTo<string>('temp.doc')); //VERIFY - Make sure that save was indeed called. mockProject.VerifyAll; There might be a better way to make the resulting code a bit cleaner. It would make the tests easier to read, instead of using regex which is also possible in this case. As a result we believe this would be a good edition to the library. Feel free to clone the repository from GitHub . If you have some time to spare submit a pull requests or two with your ideas/improvements. We believe this is a great little project worthy of some attention. Let us know what you think of the changes so far."},{"title":"VSO Task for Running FinalBuilder Project","tags":"ciandcd","url":"http://ciandcd.github.io/vso-task-for-running-finalbuilder-project.html","text":"From: https://www.finalbuilder.com/resources/blogs/postid/736/finalbuilder-vso-task Those who use TFS on-prem will be very familiar with our XAML build activity already. This activity took a great deal of confusion out of the XAML build process. Changing a build progress from a complex workflow into a simple to maintain FinalBuilder project. The time and effort saved is huge, especially considering the \"default\" XAML workflow looks like this: Thankfully Microsoft have improved on their build system with the release of Team Foundation Build 2015. You can read more about this at The FinalBuilder task is our custom task for TFS Build 2015. It offers TFS script builders the ability to still have a simplified overview of their build process while still gaining the power of FinalBuilder and all its supported actions. With FinalBuilder TFS build script creators are able to perform a wide number of tasks that would otherwise require breaking out powershell and diving into the TFS agent environment variables. When installed, the FinalBuilder Task gives users the following UI. All of the properties present in the UI are easily accessible from within any FinalBuilder script run by the task. FinalBuilder also gives simple access to a list of files that triggered the build. Installation and Usage The steps to adding custom build activities to your TFS and VSO instances are quick and easy. We have created a Repository Clone To clone this repository use the following command line. You will require git to be installed and available on your path. > mkdir VSoft > cd VSoft > git clone https://github.com/VSoftTechnologies/FinalBuilder-VSO.git Cloning into 'FinalBuilder-VSO'... remote: Counting objects: X, done. remote: Compressing objects: 100% (X/X), done. remote: Total X (delta 0), reused 0 (delta 0), pack-reused 0 Unpacking objects: 100% (X/X), done. Checking connectivity... done. With the repository cloned we require the TFS Extensions Command Line Utility (tfx-cli). It comes as a Node Package Manager (npm) package. Npm comes with both the node.js and io.js installer. Download the installer for your Windows platform and run it. To check that NPM is working correctly you can use the npm version command > npm -v 2.10.1 Now your able to install the tfx-cli package using npm. Install this globally so that its accessable on the command line. The command line for this is as follows; > npm install -g tfx-cli tfx-cli@0.1.11 C:\\Users\\<username>\\AppData\\Roaming\\npm\\node_modules\\tfx-cli ├── os-homedir@1.0.1 ├── async@1.4.2 ├── colors@1.1.2 ├── minimist@1.2.0 ├── node-uuid@1.4.3 ├── q@1.4.1 ├── read@1.0.7 (mute-stream@0.0.5) ├── validator@3.43.0 ├── shelljs@0.5.3 ├── vso-node-api@0.3.4 └── archiver@0.14.4 (buffer-crc32@0.2.5, lazystream@0.1.0, async@0.9.2, readable-stream@1.0.33, tar-stream@1.1.5, glob@4.3.5, lodash@3.2.0, zip-stream@0.5.2) To test that tfx-cli is working correctly and is on the path use the tfx command. > tfx Copyright Microsoft Corporation tfx <command> [<subcommand(s)> ...] [<args>] [--version] [--help] [--json] fTfs fSSSSSSSs fSSSSSSSSSS TSSf fSSSSSSSSSSSS SSSSSF fSSSSSSST SSSSS SSfSSSSSsfSSSSSSSt SSSSS SS tSSSSSSSSSs SSSSS SS fSSSSSSST SSSSS SS fSSSSSFSSSSSSf SSSSS SSSSSST FSSSSSSFt SSSSS SSSSt FSSSSSSSSSSSS FSSSSSSSSSS FSSSSSSs FSFs (TM) commands: build manage task extensions and builds help command help login login and cache credentials. types: pat (default), basic login <collection url> [--authtype <authtype>] [options] parse parse json by piping json result from another tfx command parse <jsonfilter> [options] version output the version version [options] Options: --help : get help on a command --json : output in json format. useful for scripting For tfx-cli to upload a task to TFS it needs to be logged in. We can do this once so that all following commands will use the some credentials. The method used depends on whether your using VSO or an On Prem installation. On Premises Login For on premises TFS basic authentication will need to be enabled. The tfx-cli project has a great guide on how to achieve this Using tfx against Team Foundation Server (TFS) 2015 using Basic Authentication. Once TFS has been configured to use basic authentication use the tfx-cli login command to connect to TFS. You will be prompted for the TFS collection URL to connect to, and the username and password for accessing that collection. > tfx login --authType basic Copyright Microsoft Corporation Enter collection url > http://<server>:<port>/tfs/<collection> Enter username > <user>@<domain> Enter password > <password> logged in successfully With a successful login subsequent commands will not require us to provide the credentials again. Visual Studio Online (VSO) Login For VSO login you need a personal access token setup under your account. There is a great article to configure an access token located at Using Personal Access Tokens to access Visual Studio Online. With the personal access token configured use the tfx-cli login command to connect to VSO. You will be prompted for the TFS collection URL to connect to, and access token for accessing that collection. > tfx login Copyright Microsoft Corporation Enter collection url > https://<vsoname>.visualstudio.com/<collection> Enter personal access token > <access token> logged in successfully With a successful login subsequent commands will not require us to provide the credentials again. Uploading Task Once logged into TFS we are able to upload the FinalBuilder task to the server. Tasks are uploaded to the server, the server will then pass them onto agents requried to run those tasks. To upload the task use the tfx-cli tasks upload command. Each command shown below is a sub-command of the previous, so order does matter here. The overwrite option is included so that any previously installed version is overwritten. Note however the highest version number of the task will win when running builds. Note: This command is run under the directory in which this repositry was cloned to (i.e. FinalBuilderTFS). > tfx build tasks upload ./FinalBuilder --overwrite Copyright Microsoft Corporation task at: ./FinalBuilder uploaded successfully! To test that the FinalBuilder task is now installed on the builds page for teh collection the task was uploaded to. Create a new empty Team Foundation Build definition. After clicking \"Add build step\" a FinalBuilder task should appear in the \"Build\" category. Further Steps For more information on the following subjects please follow the links; How to configure the build task refer to How to install FinalBuilder on an agent refer to How to create a FinalBuilder VSO agent refer to Creating a Today we are announcing the new build step for Team Foundation Build 2015. This task will allow users of TFS on-prem and VSO to run FinalBuilder projects on Team Foundation Build agents. The task itself is open source and can be found on GitHub Those who use TFS on-prem will be very familiar with our XAML build activity already. This activity took a great deal of confusion out of the XAML build process. Changing a build progress from a complex workflow into a simple to maintain FinalBuilder project. The time and effort saved is huge, especially considering the \"default\" XAML workflow looks like this:Thankfully Microsoft have improved on their build system with the release of Team Foundation Build 2015. You can read more about this at Team Foundation Build 2015 . In summary, the new build system greatly simplifies the build process into a list of tasks to perform.The FinalBuilder task is our custom task for TFS Build 2015. It offers TFS script builders the ability to still have a simplified overview of their build process while still gaining the power of FinalBuilder and all its supported actions. With FinalBuilder TFS build script creators are able to perform a wide number of tasks that would otherwise require breaking out powershell and diving into the TFS agent environment variables.When installed, the FinalBuilder Task gives users the following UI. All of the properties present in the UI are easily accessible from within any FinalBuilder script run by the task. FinalBuilder also gives simple access to a list of files that triggered the build.The steps to adding custom build activities to your TFS and VSO instances are quick and easy. We have created a GitHub Repository for explaining how to install, and use our FinalBuilder VSO task.To clone this repository use the following command line. You will require git to be installed and available on your path.With the repository cloned we require the TFS Extensions Command Line Utility (tfx-cli). It comes as a Node Package Manager (npm) package. Npm comes with both the node.js and io.js installer. Download the installer for your Windows platform and run it.To check that NPM is working correctly you can use the npm version commandNow your able to install the tfx-cli package using npm. Install this globally so that its accessable on the command line. The command line for this is as follows;To test that tfx-cli is working correctly and is on the path use the tfx command.For tfx-cli to upload a task to TFS it needs to be logged in. We can do this once so that all following commands will use the some credentials. The method used depends on whether your using VSO or an On Prem installation.For on premises TFS basic authentication will need to be enabled. The tfx-cli project has a great guide on how to achieve this Using tfx against Team Foundation Server (TFS) 2015 using Basic Authentication.Once TFS has been configured to use basic authentication use the tfx-cli login command to connect to TFS. You will be prompted for the TFS collection URL to connect to, and the username and password for accessing that collection.With a successful login subsequent commands will not require us to provide the credentials again.For VSO login you need a personal access token setup under your account. There is a great article to configure an access token located at Using Personal Access Tokens to access Visual Studio Online.With the personal access token configured use the tfx-cli login command to connect to VSO. You will be prompted for the TFS collection URL to connect to, and access token for accessing that collection.With a successful login subsequent commands will not require us to provide the credentials again.Once logged into TFS we are able to upload the FinalBuilder task to the server. Tasks are uploaded to the server, the server will then pass them onto agents requried to run those tasks.To upload the task use the tfx-cli tasks upload command. Each command shown below is a sub-command of the previous, so order does matter here. The overwrite option is included so that any previously installed version is overwritten. Note however the highest version number of the task will win when running builds.Note: This command is run under the directory in which this repositry was cloned to (i.e. FinalBuilderTFS).To test that the FinalBuilder task is now installed on the builds page for teh collection the task was uploaded to. Create a new empty Team Foundation Build definition. After clicking \"Add build step\" a FinalBuilder task should appear in the \"Build\" category.For more information on the following subjects please follow the links;How to configure the build task refer to Task UI How to install FinalBuilder on an agent refer to Installing FinalBuilder How to create a FinalBuilder VSO agent refer to Creating a VSO FinalBuilder Agent"},{"title":"Team Foundation Server XAML Builds","tags":"ciandcd","url":"http://ciandcd.github.io/team-foundation-server-xaml-builds.html","text":"From: https://www.finalbuilder.com/resources/blogs/postid/735/team-foundation-server-xaml-builds Team Foundation Server XAML Builds Today we have released an update for Team Foundation Server XAML activities for FinalBuilder 7 and 8. These updates are to deal with conflicts caused by GAC installing these activities. Since 2010 Team Foundation Server has allowed custom activities to allow provide extra functionality in XAML build workflows. For those that have not had the (dis)pleasure, XAML workflows can be difficult to work with. To alleviate this pain point we implemented XAML build templates and custom activities to allow people to run any FinalBuilder script within the build workflow without the need to edit XAMAL. Customers we have talked to say this simplifies their TFS XAML build workflows a great deal. In previous releases, FinalBuilder's installer automatically installed the TFS XAML activity into the GAC. The version of the activity installed was based on the TFS Agent detected on the machine in question. Installing into the GAC was done to simplify the process. This way the developer would simply use the activities in their build workflow and it would be picked up through the GAC. With the introduction of the new TFS build in TFS 2015 and TFS-Git in TFS 2013 this is no longer advisable. In the case of TFS-Git Workflows, assembly conflicts can cause assembly load issues. If the activity requires a different assembly version to those already loaded by the TFS agent, you would see assembly load errors with lib2gitsharp. With TFS 2013 having five updates this is increasingly possible . The way to avoid any assembly loading issue for custom activities is to use \"version control paths for custom XAML activities\". To be clear, we have left the GAC installation option in both FinalBuilder 7 and 8. We however do recommend switching to using custom XAML activity paths, especially if you're using TFS-Git. To this end, the FinalBuilder installers now give the option as to whether to install the XAML build activities into the GAC or not. Only FinalBuilder 7 will automatically install TFS activities into the GAC for TFS 2012 and earlier agents, with FinalBuilder 8 you much chose whether to GAC install the assemblies. Note that if activities were previously installed in the GAC restarting the TFS Build Controller is required. This refreshes the build controller and releases any assemblies that it may have previously loaded. Creating a build definition The creation of the build definition is exactly the same as before. If you're interested in how to setup a build definition from scratch using the FinalBuilder XAML templates please review the \" FinalBuilder and Team Foundation Server \" article. Custom XAML activities Version control paths for custom XAML activities is a feature in TFS XAML build controllers. This feature allows the build controller to source all assemblies required for an activity from a known location. If a required assembly is missing from this location the standard .Net assembly lookup methodology is used. Using version control paths for custom XAML activities requires the activity assemblies to be added to a repository on the TFS system. This repository can be shared with other code, or can be a repository just for the assemblies. As the custom folder does not change based on the build being performed, we suggest a separate TFS repository for the custom activities. The repository can also be either a TFS or Git-TFS repository. Both will work the same. To add the custom activity assemblies to a repository connect to the repository through team explorer in visual studio. In the source control view create a new folder that will hold the custom activity assemblies. In the GAC sub-folder in your FinalBuilder installation (typically \"%ProgramFiles(x86)%\\FinalBuilder 8\\GAC\\\") there are folders for each version of TFS custom activities are provided for. From the folder relating the TFS version copy all assemblies contained within into the newly created repository folder. From the source control explorer add these files to source control. Updating the XAML Build Controller Next the build controller needs to be configured use the repository location. In the builds tab of team explorer click on the \"Actions\" link. A drop down will appear with the option to \"Manage Build Controllers…\". Click the \"Manage Build Controllers…\" menu item. From the build controllers window that opens select the build controller responsible for the FinalBuilder builds. If there is more than one controller simply follow these steps for each controller. Next click the \"Properties\" button. This will present the build properties dialogue in which the \"Version control path to custom assemblies\" can be set. Select the folder that was created in the repository that is responsible for the custom activity assemblies. Now confirm this change by clicking OK on the build controller properties dialogue. This has now setup the build controller to source custom assemblies from the configured repository folder. The most recent checked in assemblies will always be sourced. Therefore keeping this repository folder in sync with the custom activities used in build workflows is very important. Once the custom path is set the build controller can run builds using the custom activities. To test simply queue a build using the FinalBuilder custom activities."},{"title":"Continua CI Version 1.7 released","tags":"ciandcd","url":"http://ciandcd.github.io/continua-ci-version-17-released.html","text":"From: https://www.finalbuilder.com/resources/blogs/postid/734/continua-ci-version-17-released Version 1.7 of Continua is now released. A big thank you to all those who downloaded the beta and especially those of you who reported issues and bugs. This version introduces several new features, many of which have been requested by users over the past few months. These features are built upon the various improvements and bug fixes applied in revisions to version 1.6. Please don't dismay if your requested feature is not included yet, it is still high on our to-do list. Indeed we have several other features specced out, and some partially developed in the background. Version 1.7 Features New Builds View dashboard This view is useful for project administrators and shows a list of active builds across all viewable configurations. This includes running builds, queued builds and builds awaiting promotion. New panel of indicators Some important numbers including the total count of queued and running builds, as well as available agents and concurrent build licenses. New Repositories tab This is accessed via the Configurations view and shows status of each repository. We've also included \"Check Now\" buttons for immediately polling each repository. You can also initiate repository checking from all existing repository pages Project-wide and configuration versioning options. We've added some new options in the details section of the project and configuration wizards Project-wide versioning: The build version number can now be incremented across many configurations within a project. Build number re-use: A new option at the project or configuration level to decrement the version counter when a build is discarded while initialising. e.g. due to configuration conditions. Please note that the build number will be decremented only if no other build has started in the mean time and is using a later build number. Improvements to Build Completed triggers. Variable expressions: You can now use expressions when defining variables allowing you to pass information from triggering to triggered build. New conditions tab: This allows you to use expressions to control whether a build is triggered Improvements to Repository triggers. Trigger on specific file change types: Triggers can now be set to start only when the changeset contains certain types of file changes e.g. additions, modifications and deletions. Trigger file pattern: You can now specify a file pattern for repository triggers to restrict triggering only to changesets containing matching files. Trigger comment pattern: You can also limit triggering to changesets with specific text in the comment. Other build features New force repository check option in queue build dialog allowing control over whether to recheck repository when building. There is also a default setting for each configuration Improvements to Stop Build buttons on dashboard view to ensure that the build stopped is always the latest build at the time when the button was clicked. Stop build dialogs also now display the build number of the build being stopped. Actions and event handlers New node.js actions Package management with Npm and Bower Grunt and Gulp build runners Unit testing with Mocha New build event handler for posting status updates to a Stash server Log Entry action now allows you to add the message as a build comment . This can be useful for showing additional build details on the build view page. New comments field on all actions – displayed as a tooltip in Stages editor. New ContinuaCI.* system environment variables are now available to all executable actions. ContinuaCI.AgentProperty.* ContinuaCI.Variable.* ContinuaCI.Project.Name ContinuaCI.Configuration.Name ContinuaCI.Build.Id ContinuaCI.Build.BuildNumber ContinuaCI.Build.ChangesetCount ContinuaCI.Build.ChangesetRevisions ContinuaCI.Build.ChangesetTagNames ContinuaCI.Build.ChangesetUserNames ContinuaCI.Build.Elapsed ContinuaCI.Build.HasNewChanges ContinuaCI.Build.IsFeatureBranchBuild ContinuaCI.Build.IssueCount ContinuaCI.Build.LatestChangeset.Created ContinuaCI.Build.LatestChangeset.IssueCount ContinuaCI.Build.LatestChangeset.RepositoryName ContinuaCI.Build.LatestChangeset.Revision ContinuaCI.Build.LatestChangeset.TagCount ContinuaCI.Build.LatestChangeset.UserName ContinuaCI.Build.Started ContinuaCI.Build.StartedBy ContinuaCI.Build.TimeOnQueue ContinuaCI.Build.UsesDefaultBranch ContinuaCI.Build.Version Execute Program, DOS Command and PowerShell actions now include an option to generate a context XML file . This file contains details of the build including repositories, changesets and files for you to parse with your own script or program. Git repositories Case-only renames are now recorded in the repository cache. New option to list author instead of committer as changeset username Version 1.7 is ready for you to download and install. All feedback is welcome!"},{"title":"FinalBuilder 8 Released!","tags":"ciandcd","url":"http://ciandcd.github.io/finalbuilder-8-released.html","text":"From: https://www.finalbuilder.com/resources/blogs/postid/733/finalbuilder-8-released It's very nearly 5 years since FinalBuilder 7 was released. Since it's release we have shipped many official updates , nearly every update including new features or improvements. This program of continuous improvement has worked well, with customers not having to wait for major new versions to arrive to get support for new versions of Visual Studio or Delphi etc, but it has limited our ability to make major changes. So it's time for a new major version of FinalBuilder. What's new in FinalBuilder 8 IDE Themes The IDE has two new themes, Dark and Light (yes, imaginatively named!). The IDE defaults to Dark on first run, however you can change the theme in the options quite easily. Debugger One of the most asked for features now available in FinalBuilder 8, stepping into included projects . In FinalBuilder 7 and earlier, you could only step over included projects, and wait for them to return. In FinalBuilder 8, you can step into the included project, if it is not already opened the IDE will open the project and switch to it automatically. To make this possible, there are now \"Step Into\" and \"Step Over\" functions. The Step into/over now also applies to targets (see below). Debugger breakpoints now have conditions : Actionlists renamed to Targets ActionLists have been renamed to Targets. Targets can now also define dependencies, so you can for example define Clean, Build, Test, and have Test depend on Build. If you execute the Test target, and Build has not already been executed, it will be executed first before Test. Targets can be specified on the command line. In FinalBuilder 7 and earlier, projects had a Main and an OnFailure (global error handler) actionlist. In FinalBuilder 8, projects just have a Default Target. Older projects will be imported such that the Main and OnFailure Targets are called from the Default Target inside a try/catch block. Run Target Action You can now return values from Targets (ie out parameters) . New Help System The help has moved online in the form of a wiki. This enables us to do inline help updates without needing to ship new builds. The new help is still being worked on, lots of screenshots are missing etc.. Non Visible Changes Stepping Engine The stepping engine was rewritten to enable stepping into included projects, and to enable target dependencies. This, work, together with the new variables architecture is where the bulk of effort/time was spent in the FinalBuilder 8 development cycle. Variables Architecture The variables architecture and the expression evaluator were rewritten to resolve several corner case issues that we were not able to resolve in FinalBuilder 7. The expression evaulator has a new parser that will allow us to more easily extend the syntax in the future. The User variable namespace was removed, it caused too many problems with projects not running under other users, not running on the build server etc. Use Project variables instead. Core Messaging Changes to the messaging has allowed us to improve the performance of the stepping engine and logging, with much less thread switching. This also improved the IDE performance. CLR Hosting The minimum CLR version is now .NET 4.0 (ie FinalBuilder requires .net 4.0 to be installed). FinalBuilder 8 also requires Powershell 3.0 or later. Code Changes In addition to the architectural changes, we also spent a lot of time refactoring the code, running static analysis tools over the source, looking for memory leaks, potential bugs etc. One of the results of this is reduced memory usage during a build compared to FB7. The FB8 IDE does use slightly more memory than the FB7 IDE at startup (mostly due to the heavy use of delphi generics), however the runtime memory usage is much lower.A large part of the refactoring involved unit testing (we created a new unit test framework to suite our needs!) and creating a suite of integration tests. FBCmd The command line parameters have changed to be more consistent and easier to specify. You can also specify one or more targets to execute (when not specified, the default target is executed). New Project File Formats FinalBuilder has used an xml file format since version 1, however a common complaint over the years, has been that it is difficult to diff file versions. FinalBuilder 8 has tackled this in two ways. A new DSL style project file format (.fbp8) is now the default format, it is very easy to diff. project begin projectid = {04710B72-066E-46E7-84C7-C04A0D8BFE18} target begin name = Default targetid = {E6DE94D6-5484-45E9-965A-DB69885AA5E2} rootaction begin action.group begin id = {D860420B-DE46-4806-959F-8A92A0C86429} end end end end A new xml format (.fbx8), much less verbose than the old format. <?xml version=\"1.0\" encoding=\"UTF-8\"?> <finalbuilder> <project> <projectid>{6A717C24-D00F-4983-9FD0-148B2C609634}</projectid> <target> <name>Default</name> <targetid>{E6DE94D6-5484-45E9-965A-DB69885AA5E2}</targetid> <rootaction> <action.group> <id>{D860420B-DE46-4806-959F-8A92A0C86429}</id> </action.group> </rootaction> </target> </project> </finalbuilder> Compressed project files (.fbz8) use the dsl format internally (compressed projects are just a zip file with a project.fbp8 inside it). The default project file encoding is now UTF-8, which is more version control friendly (some version control systems treat utf-16 as binaries). New Actions FinalBuilder 8 includes new actions for Chocolatey, Bower, NPM, Gulp, Grunt, Rake, Fake, Mocha, along with Redgate SQL Compare and SQL Data Compare actions. TFS 2015 XAML builds are also supported, a VSO task will be available soon (will be published on github). License Key installation We implemented a new more reliable trial mechanism for FinalBuilder 8, and made it simpler to install license keys. You can log into your account on our website directly in the FinalBuilder IDE and download & install license keys The trial mechanism also uses license keys, which is more reliable than the mechansim used in earlier versions. Where's my license key? If you had an active Software Assurance Subscription for FinalBuilder 7 as of 20th August 2015, FinalBuilder 8 license keys were generated and added to your account today. You should have received an email notification. How do I purchase an upgrade? If you had a subscription for FinalBuilder 7, and it expired, you can upgrade by Renewing your subscription . If you have a license for FinalBuilder 6 or earlier, you can Purchase an upgrade here. Can I safely install FinalBuilder 8 on a machine with FinalBuilder x? Yes. FinalBuilder 8 installs into a separate folder (as did all older versions of FinalBuilder). Can FinalBuilder 8 open an run my old projects? Yes, FinalBuilder 8 can open and run any projects from any earlier version of FinalBuilder. When you save the projects in FinalBuilder 8, it will save it as a FinalBuilder 8 project (new file). Can older versions of FinalBuilder open FinalBuilder 8 projects? No, FinalBuilder 8 projects use a different file format. Does FinalBuilder 8 work with FinalBuilder Server 7? No. FinalBuilder 8 does not work with FB Server 7, nor can it be made to work. FinalBuilder Server was discontinued over 2 years ago, if you need Continuous Integration then consider migrating to Continua CI. I have questions! Feel free to contact support@finalbuilder.com with any questions you might have."},{"title":"Introducing Continua CI Version 1.7 beta","tags":"ciandcd","url":"http://ciandcd.github.io/introducing-continua-ci-version-17-beta.html","text":"From: https://www.finalbuilder.com/resources/blogs/postid/732/introducing-continua-ci-version-17-beta This version introduces several new features, many of which have been requested by users over the past few months. These features are built upon the various improvements and bug fixes applied in revisions to version 1.6. Please don't dismay if your requested feature is not included yet, it is still high on our to-do list. Indeed we have several other features specced out, and some partially developed in the background. Version 1.7 Features New Builds View dashboard This view is useful for project administrators and shows a list of active builds across all viewable configurations. This includes running builds, queued builds and builds awaiting promotion. New panel of indicators Some important numbers including the total count of queued and running builds, as well as available agents and concurrent build licenses. New Repositories tab This is accessed via the Configurations view and shows status of each repository. We've also included \"Check Now\" buttons for immediately polling each repository. You can also initiate repository checking from all existing repository pages Project-wide and configuration versioning options. We've added some new options in the details section of the project and configuration wizards Project-wide versioning: The build version number can now be incremented across many configurations within a project. Build number re-use: A new option at the project or configuration level to decrement the version counter when a build is discarded while initialising. e.g. due to configuration conditions. Please note that the build number will be decremented only if no other build has started in the mean time and is using a later build number. Improvements to Build Completed triggers. Variable expressions: You can now use expressions when defining variables allowing you to pass information from triggering to triggered build. New conditions tab: This allows you to use expressions to control whether a build is triggered Improvements to Repository triggers. Trigger on specific file change types: Triggers can now be set to start only when the changeset contains certain types of file changes e.g. additions, modifications and deletions. Trigger file pattern: You can now specify a file pattern for repository triggers to restrict triggering only to changesets containing matching files. Trigger comment pattern: You can also limit triggering to changesets with specific text in the comment. Other build features New force repository check option in queue build dialog allowing control over whether to recheck repository when building. There is also a default setting for each configuration Improvements to Stop Build buttons on dashboard view to ensure that the build stopped is always the latest build at the time when the button was clicked. Stop build dialogs also now display the build number of the build being stopped. Actions and event handlers New node.js actions Package management with Npm and Bower Grunt and Gulp build runners Unit testing with Mocha New build event handler for posting status updates to a Stash server Log Entry action now allows you to add the message as a build comment . This can be useful for showing additional build details on the build view page. New comments field on all actions – displayed as a tooltip in Stages editor. New ContinuaCI.* system environment variables are now available to all executable actions. ContinuaCI.AgentProperty.* ContinuaCI.Variable.* ContinuaCI.Project.Name ContinuaCI.Configuration.Name ContinuaCI.Build.Id ContinuaCI.Build.BuildNumber ContinuaCI.Build.ChangesetCount ContinuaCI.Build.ChangesetRevisions ContinuaCI.Build.ChangesetTagNames ContinuaCI.Build.ChangesetUserNames ContinuaCI.Build.Elapsed ContinuaCI.Build.HasNewChanges ContinuaCI.Build.IsFeatureBranchBuild ContinuaCI.Build.IssueCount ContinuaCI.Build.LatestChangeset.Created ContinuaCI.Build.LatestChangeset.IssueCount ContinuaCI.Build.LatestChangeset.RepositoryName ContinuaCI.Build.LatestChangeset.Revision ContinuaCI.Build.LatestChangeset.TagCount ContinuaCI.Build.LatestChangeset.UserName ContinuaCI.Build.Started ContinuaCI.Build.StartedBy ContinuaCI.Build.TimeOnQueue ContinuaCI.Build.UsesDefaultBranch ContinuaCI.Build.Version Execute Program, DOS Command and PowerShell actions now include an option to generate a context XML file . This file contains details of the build including repositories, changesets and files for you to parse with your own script or program. Git repositories Case-only renames are now recorded in the repository cache. New option to list author instead of committer as changeset username Version 1.7 beta is ready for you to download and install. All feedback is welcome!"},{"title":"Delivering DevOps Value","tags":"devops","url":"http://ciandcd.github.io/delivering-devops-value.html","text":"From: http://devops.com/2015/07/02/delivering-devops-value-show-money/ HM Heath Solutions delivers new business applications in the healthcare industry to 35 million customers. In order to embark on their DevOps journey to improve software delivery, Valerie Scott, their Manager of Shared Services Division, had to show senior management an ROI on a successful project, before they would invest fully. On June 2, Val Scott along with Rick Slade of IBM joined Alan Shimel of DevOps.com in a webcast to discuss this interesting client case study. During their discussion, they illustrated in vivid detail how HM Health Solutions showed success in DevOps to their senior executives, for them to buy in and sponsor the project. Val and Rick's webinar parallels many similar discussions I've had with senior executives. To net it out, the bottom line is, as it was so clearly and unequivocally articulated in this amusing clip from the movie Jerry Maguire: \"Show Me The Money!\" When I meet with senior LOB, Finance or IT executives — they shout a similar refrain: \"All I want to do is buy a business outcome. I want to invest in an IT project, and have it pay back. Unfortunately for many application development (or other types of enterprise IT) projects, that pay back is illusory, and ROI difficult to measure.\" — Jane or John Executive, representing nearly every IT executive or CFO I have ever met Unfortunately, many struggle to quantify, monetize and demonstrate the financial return on investing in IT projects. Yet as described in this blog , it is the primary responsibility and outcome of management; leading the economic performance of their department or unit. Show me the money! Fortunately, many clients experience positive returns, both operationally and economically. Ten (with links to more) client case studies with returns are highlighted in this blog, and summarized on this page . As you undoubtedly have experienced yourself, senior executives respond to clear business value and benefits articulated as financial returns, as ROI. But how do you do that for DevOps, a set of technical practices and associated culture change? As Val and Rick described during their informative webcast , comparing productivity, time-to-market, speed of delivery, and quality, both before and after a DevOps pilot, are crucial. These operational metrics will be improved by adopting DevOps. However, to win the hearts and minds of senior executives, you have to monetize the results, in other words – show them the money! Technical techniques and practices such as those Val and Rick described to Alan, shifting left, test driven development, and automating application build, deployment, and test are absolutely necessary, but they are insufficient to win the argument with senior executives. The gains they produce are necessary, and impressive. The operational results have been stellar thus far at HM Health Solutions, per Val: a 67% reduction in resource demands on the release team & process, a 75% reduction in relative hours to repair defects per environment, an 82% reduction in application deployment time. Yet as impressive as these operational results are, they are insufficient. As Val described, arguments about business value, technical value, or improvements to developer or operations staff productivity, alone did not persuade the management. The magic word is an economic and financial one: \"investment\". Making a strategic investment to improve delivery speed and quality of business applications was the key to winning the hearts and minds of the senior executives. In addition to the technical practices described above, Alan asked Val to describe the tools HM Health Solutions adopted. A continuous delivery solution with application release automation software from IBM UrbanCode was implemented, including application build, deploy, and release. As a way of summarizing the webcast for the audience, Alan asked Rick and Val for their two key takeaways; the two most important messages and things to remember. Rick's: Knowing which metrics are critical to your organization. There are many benefits to adopting DevOps, and many potential hard numbers that are straightforward to capture and measure. Deciding which metrics have the most impact and are most persuasive to your stakeholders is critical to making the case to invest in DevOps. Implementing tools and automation: automated testing, and linking that to build and deployment processes and automation. Val's: Rick's two takeaways of course! And one more… DevOps is new in the industry, and is not yet well understood or common practice in enterprise IT or among its leaders. Understand your audience, and become good at explaining it in straightforward and clear business and financial terms to senior executives. Especially persuasive are other client proof points, and the results of pilots in your own enterprise IT environment. Make sure pilot results show that the solution works and delivers timely business value, and that the related facts and metrics clearly quantify the results. Read this blog to know more and Check out other customer proof points/ case studies This was a very informative, fast paced, and information rich webcast. I hope that you find the replay useful in continuing your DevOps adoption journey. If you'd like to explore more about management responsibilities and know-how for establishing an ROI for DevOps and app release & deployment automation, see this ROI web page, and read this ROI blog for additional resources on method, approach, making the case, and calculating the ROI. And please don't hesitate to reach out to me on twitter: @paspung . I look forward to hearing your success stories in making the ROI case to senior executives, and delivering DevOps value to your organization! Peter Spung, DevOps Deploy and UrbanCode Integration Executuve, IBM Cloud, @paspung"},{"title":"SaaS: Building Tools for the DevOps-MindedDevOps.com","tags":"devops","url":"http://ciandcd.github.io/saas-building-tools-for-the-devops-mindeddevopscom.html","text":"From: http://devops.com/2015/07/01/saas-building-tools-for-the-devops-minded/ DevOps, Continuous Delivery, Continuous Integration and Continuous Testing are no longer just methodologies and development strategies — they're all a part of a growing economy within the tech industry. Great Software-as-a-Service (SaaS) companies such as Chef , Docker and New Relic have done an amazing job of spotting the trends early and building their tools and services around a new wave of development processes. Chef and Docker alone have reportedly raised nearly $250 million, and New Relic went public in 2014. In fact, this very website is another example of how impactful these new development practices are — as several specialized news outlets have been born out of these revolutionary development strategies. So, not only has the DevOps methodology helped developers create better products, faster; but the process has opened opportunities for technical experts to share their experiences on sites like this, e-books, webinars, conferences and any other way people communicate. On DevOps.com, you'll see the best articles on the web discussing how DevOps is defined in different organizations, why Fortune-500 companies are shifting old practices toward CD and the best practices of CI from industry experts. In the race to be the most DevOps-friendly solution — no matter the niche — hundreds of startups have been founded and funded, which has only enhanced the toolboxes of thousands of developers. You'd be hard-pressed to find a good SaaS company that doesn't offer integration with Jenkins CI, TeamCity or Chef, and that's because not only should a SaaS product solve an important problem, but it should have minimum disruption of your current development process. With all that in mind, Continuous Testing is one of the most exciting spaces in technology today. Continuous Testing is a process that aims to synchronize testing and QA with dev and ops, which ultimately yields better products more quickly. There are several factors that make Continuous Testing successful in a team's software development lifecycle — such as open-source collaboration solutions, test automation APIs and tools to track the historic performance of applications . With the release of Load Impact Version 3.0 , the global performance and load testing SaaS company has announced its latest feature that fits right in with the DevOps mindset: Performance trending . Performance trending allows users to plot runs of the same test over time. That gives companies a clear view into data that locates patterns of performance degradation or improvement in order to easily validate code and infrastructure changes. In the spirit of Continuous Testing, DevOps, CD and CI — performance trending improves users' ability to gain both a low-level and high-level view into the performance of their systems. And with the increased number of tests run by DevOps professionals, performance trending yields valuable information for companies concerned with their application performance — which should be everyone. Between DevOps.com and plenty of great conferences and meetups around the world, it's safe to say this economy of DevOps-minded tools will only continue to grow in the coming years."},{"title":"Stop whining about shadow IT and do something about it","tags":"devops","url":"http://ciandcd.github.io/stop-whining-about-shadow-it-and-do-something-about-it.html","text":"From: http://devops.com/2015/07/01/stop-whining-about-shadow-it-and-do-something-about-it/ Much has been written about shadow IT at the business unit and employee level where the proliferation of cloud applications has all but eliminated IT's control over what applications are used in their organization. The explosion of public cloud technologies has caused an identical revolt amongst development teams who no longer want to—or need to—wait for IT to spin up the IT infrastructure needed to fuel their development efforts. The reality is public cloud has drastically changed the expectations of developers – even if they aren't using it. Gone are the days where development teams are satisfied with putting in a help desk request for a new server and waiting to access it. Developers now expect IT to provide services similar to what they can get from a public cloud provider. And if you can't provide it? Then you risk them pulling out a credit card and bypassing everything you've built. But just as shadow IT can wreak havoc on data security and IT budgets, there's a whole new set of problems that can arise when shadow IT hits the infrastructure level. So now that the developers' expectations for immediate resources have been changed irrevocably by public cloud, what can IT do to bridge the gap? Calling a truce: provide self-service provisioning and management First and foremost, the obvious one—give developers the immediate access to IT resources that they need to do their jobs. IT needs to leverage its existing investment in virtualization technologies by providing self-service provisioning and virtual machine management capabilities to developers. In other words, get out of the way and give them what they want. At the minimum, IT should be looking to setup an internal cloud management system which provides IT an on-premises solution with: Integration to existing authentication/authorization systems (e.g. Active Directory) Quotas to control usage by employee/group Chargeback/showback to provide usage tracking and accounting Giving back control is the best way to ease what can be a tenuous relationship between IT and developers. Developers will be happier because they will be provisioning new servers in minutes rather than days and IT can focus on more interesting challenges. Abstract away the complexity Self-service provisioning is only step one. IT needs to get out of the business of provisioning servers and let developers manage their virtual machines themselves. However, that doesn't mean that IT should leave developers to manage every aspect of their cloud usage on their own. On the contrary, IT's job should be to create the tools and infrastructure to ensure best practices within your organization and track IT infrastructure resources to avoid unnecessary infrastructure costs. But how? The first step is using a centralized automation and orchestration technology. I won't get into the religious battle between the various flavors—Puppet, Chef and Ansible are the three most common that we see and they all have their strengths and weaknesses. However, they all provide the type of framework IT needs to empower self-service provisioning capabilities. IT should own the base OS images which developers can use for their virtual machines and use an automation framework to provide the means so that developers can easily set up a common set of framework applications (e.g. web servers, databases, monitoring tools, etc.). IT can also use cloud management tools to effectively track resources. We've seen many instances where customers have discovered numerous unused VMs. By tracking resources and enforcing expiration dates on certain resources, IT can deliver infrastructure more cost effectively. Give users choice Frankly not all clouds are created equal. And certainly not for all tasks. An internal VMWare virtualization may provide a great cloud—once IT has provided the services mentioned above—for most developer's needs—but don't stop there. For example, public clouds, like AWS, Azure, Google, have servers located all around the world. That makes them ideal for web applications which are going to have geographically dispersed users. Most IT shops certainly shouldn't be trying to duplicate that capability or prevent its usage. Embrace it instead, where appropriate! That doesn't mean that IT shouldn't have visibility and control into that usage. IT should strive to create a system which supports a collection of private and public clouds, has unified chargeback/showback and quotas across ALL of the private and public clouds that IT wants to support. Abstract Away the Complexity to Support Teams Beyond Developers These best practices shouldn't be limited to developers! One of my favorite use cases that I have seen was an IT shop which provided their sales team with the capability to spin up VMs for product demos. They encapsulated the complexity of 10+ product variations and multiple operating system varieties into a single dialog box. When the sales manager wanted to do a demo, they checked a couple of boxes in a dialog box, hit submit and a VM was spun up on their internal VMWare cloud which automatically expired 8 hours later. This was a perfect example of allowing a non-technical user to control of their own virtual machines while IT maintained complete control of the underlying technologies. Maintain flexibility! One final word of caution: the world of private and hybrid cloud is changing very rapidly. IT needs to be able to utilize their existing virtualization infrastructure, typically VMware, today while exploring new private cloud options like OpenStack and preparing for the future of containerization and micro-services. It's important to avoid solutions which are locked into a particular technology—which may leave IT behind in this rapidly changing environment. There is no doubt that public cloud has led developers to have drastically increased expectations for private IT. By putting in place a cloud management platform backed by DevOps server automation, IT can meet these expectations and stop the growth of shadow IT at the infrastructure level. About the Author/Jon Mittelhauser Jon Mittelhauser is the CEO of CloudBolt Software. A 20-year Silicon Valley veteran, Jon Mittelhauser has a proven track record of building highly regarded technology organizations from the ground up and helping to architect pioneering technologies. He is considered one of the founding fathers of the World Wide Web. He was co-author of the first widely-used Web browser, NCSA Mosaic, and was a founding engineer of Netscape Communications Corporation. Mittelhauser was an early investor in Tesla Motors and one of the first Tesla Roadster owner."},{"title":"Apply DevOps automation to APIs with Akana","tags":"devops","url":"http://ciandcd.github.io/apply-devops-automation-to-apis-with-akana.html","text":"From: http://devops.com/2015/06/30/apply-devops-automation-to-apis-with-akana/ One of the core values of DevOps is automation. The ability to automate routine tasks and have triggers that automatically initiate other tasks takes care of all of the routine functions so developers and IT personnel can focus on bigger issues. One thing that has also emerged as driving force for DevOps—and IT in general—is APIs. Organizations have different platforms and tools from different vendors and they need everything to integrate smoothly together. The APIs enable that synchronicity but add another layer of complexity to develop and manage the APIs as well. That's where Akana comes in. Akana enables organizatiions to quickly deliver scalable applications, connect and integrate applications, and share data as APIs. The Akana API Management Platform was created to give developers and IT personnel the tools necessary to effectively develop and manage APIs. Akana recently introduced new capabilities in its API Management Platform to streamline API management and reduce the time required to update APIs. One problem organizations face is that DevOps disrupts the traditional workflow and creates new bottlenecks. DevOps is sort of a natural evolution of Agile development—taking the principles of Agile beyond developers to the IT infrastructure and the company as a whole so that everyone can keep up with the faster pace of development. The rapid deployment of apps, however, can have a cascade effect that impacts the API lifecycle. As new apps are deployed the API definitions can change. Those changes have to be propagated to the API portal. The API owner has to update associated documentation. In most cases this process is a tedious manual exercise. The new Akana API Management Platform seeks to change that by bringing DevOps-style automation to the API lifecycle management process. The new API Management Platform integrates with continuous integration platforms like Jenkins and TeamCity . It has provisions for tracking necessary approvals to push updates to the API portal or API gateway. It also includes support for standard API descriptor document types like Swagger, RAML and WADL. \"APIs have fast become the de-facto standard for digital enterprises to connect applications and services with digital end-points like mobile apps and internet-of-things,\" said Brent Carlson, Senior Vice President, Technology, at Akana. \"With this new capability, enterprises can deliver continuous innovation with speed and agility, ensuring that new updates and capabilities are automatically, efficiently and securely delivered to their developers and partners, in a timely fashion and without manual intervention.\" No matter how streamlined you get you'll eventually hit a bottleneck. As more of the development and deployment process gets automated that bottleneck gets moved farther down the chain, but then new platforms and tools emerge to address evolving issues and fix the new bottlenecks. For some organizations that have already embraced DevOps APIs may be the new bottleneck, but a tool like the Akana API Management Platform may alleviate that problem. It will be interesting to see where the next bottleneck strikes. At some point as DevOps evolves and every element is streamlined and automated the only bottlenecks will be organic—meaning the human beings involved in the process."},{"title":"What does the future of IT Operations look like (in a #DevOps world)?","tags":"devops","url":"http://ciandcd.github.io/what-does-the-future-of-it-operations-look-like-in-a-devops-world.html","text":"From: http://blog.devopsguys.com/2015/06/30/what-does-the-future-of-it-operations-look-like-in-a-devops-world/ What does the future of IT Operations look like? As more businesses rely on virtualisation, containers, cloud, Infrastructure as a Service and Microservices is there still a role for IT Operations? How do these teams change to continue to deliver value when supporting Agile Operations techniques? Is there still a role for IT Operations? Absolutely 100% (we believe that so much we started a company to offer application-centric cloud operations!). We blogged about this back in 2013 when we said that \" Devops Does Not Equal \"Developers Managing Production \". We said then: \"Operations is a discipline, with its own patterns & practices, methodologies, models, tools, technology etc. Just because modern cloud hosting makes it easier to deploy servers without having to know one end of a SCSI cable from another doesn't mean you \"know\" how to do Operations (just like my knowledge of SQL is enough to find out the information I need to know monitor and manage the environment but a long way from what's required to develop a complex, high-performing website).\" – @TheOpsMgr This still holds true today. That said, the role of Operations is changing – Ops has to become more \"Application-Centric\" and understand the applications that are running on the platforms they provide. It's not enough anymore to take a narrow view that says \"my servers are OK, it's not my fault the application doesn't work\". Well, it might not be your \"fault\" but you share the responsibility for making sure the application is available for your customers. Stop passing the buck! Operations people almost certainly need to learn to code, since we are heading towards a code-driven, API enabled world. If you can't code (or at least have solid scripting skills) you risk being left behind will be left behind. More importantly, the Operations Engineer/Developer of the future will be filling a role more akin to that of a \" process engineer \" in a physical factory or logistical supply chain. A process engineer designs a process and production line that transforms raw materials into a finished product. The Operations Engineer/Developer of the future will be building Digital Supply Chains and Digital Production Lines. These Digital Supply Chains will transform raw materials (source code) via continuous integration, test automation, packaging, release automation, infrastructure-as-code etc into applications running in cloud-hosted environments. The rate of changes flowing along the Digital Supply Chain will far exceed \"old school\" Change and Release methodologies – you can't have a weekly CAB (Change Advisory Board) meeting if you're doing multiple deployments per day (or every 11.6 seconds à la Amazon). So, just like a physical production line includes statistical sampling, automated testing etc., so will the Digital Supply Chain of the future. We already do this with TDD/BDD, automated testing with tools like Selenium etc but it will become the Operations Engineer/Developer job to ensure that the digital production line delivers release packages of sufficient quality to ensure the stability of the application (and the organisation's revenue that depends on it!). Modern supply chains are complex and have many interdependencies on 3rd parties, particularly if you're operating a \"Just-In-Time\" (JIT) model. Modern software applications have the ultimate in JIT dependencies due to their integrations with 3rd party SaaS API's like payment gateways, recommendation engines, authentication gateways, cloud providers etc. Modern Operations Engineers will need to ensure that they design the digital supply chain that can cope with failures in these interdependencies, or at least ensure that they select the right 3rd party partners who can offer the right levels of performance and availability needed for their applications. In summary, will the Operations Engineer/Developer of the future be \"just managing (virtual) servers\"? No, almost certainly not. What they will be doing is designing and building complex digital supply chains with complex interdependencies both internally and externally to the organisation, digital supply chains designed to meet the needs of applications that are designed to meet the needs of their customers, safely, securely and cost-effectively. The Q&A above is part of material prepared as our contribution to an CA ebook on \"Agile Operations\". We wrote our thoughts on 6 questions, of which 4 will be used in the ebook, scheduled to come out in August 2015. You can read the earlier Q&A here – http://blog.devopsguys.com/2015/06/23/what-is-important-for-an-it-ops-to-team-more-effectively-with-preproduction-teams-devops/"},{"title":"What Blocks Your Developers","tags":"devops","url":"http://ciandcd.github.io/what-blocks-your-developers.html","text":"From: http://java.dzone.com/articles/what-blocks-your-developers-0 Developer productivity is not an easy topic. In many managers' points of view the product is often not developed fast enough, presented estimates too big and developers are focusing too much on robustness of solutions for which customers just would not pay. On the other side, developers frequently perceive managers as people pushing forward without looking at the current situation, ignoring some technical consequences of their decisions and forcing programmers to create rubbish and cheap solutions. The fact is that there is always not enough time to create a product in the technical way with which we would like to. Many developers are hopping from job to job in a search of a holy grail project. To meet both sides expectations, some standard can be set and developer productivity measured. Unfortunately, here even bigger obstacles emerge. How the heck do you measure developer productivity? I will not even mention lines of code metrics as it simply tells you almost nothing. The number of bugs or issues solved is also not the best one. It is very hard to assure that each task is equal in terms of required amount of time to finish. Most of the currently used metrics are only half measures. Very often the biggest impact have a more or less biased leader or senior developers opinion about a specific programmer. While it is very hard to measure productivity, we can more easily identify what things have a negative impact on it and try to remove them. By the way, it would be great to have good measuring methods to check how removing each of the problems listed below would affect performance: Lack of fast and good unit tests - under such circumstances introducing any change is rather troublesome. The developer does not know what exact behaviour the code should provide and if their change has broken anything. Diligent developers might spend enormous amount of time verifying correctness of their commits. Not being diligent enough might introduce major bugs. Professional ones would spend quite a lot of time allocated to issue for unit tests creation (which is IMHO the right way), Project compilation and local deployment takes quite a lot of time - while what amount of time is too long might be disputable, several minutes is quite a lot for sure. Imagine that after each change a developer has to wait 10 minutes to check application behaviour. It is quite distracting and demotivating and can significantly lower the performance. To countermeasure it you can not only spend some time on tuning the build but also use one of the frameworks for reloading code changes instantly (like for example JRebel ), Developers' Continuous Integration feedback loop takes a lot of time - after the commit is pushed to repository, your CI server should present results as fast as possible. If a developer needs to wait for more than 1 hour for their commit results they can simply forget about it or check it another day. Under such circumstances it is difficult to create a feeling of common responsibility for the project. It might happen that one dedicated person would need to be a CI watch and will have to monitor builds, spend time on failure analysis, notify responsible programmers and ensure that they have fixed what was broken, Badly structured code (code name: spaghetti code) - the bigger the amount of spaghetti the more amount of time is required to introduce a change. It is harder to track dependencies. A lot of code needs to be checked and deeply investigated to fully understand what is going on and avoid regression. Each change might have an impact on many modules that look like they are not related. The chances to introduce bugs are big, Requirements are not clear - quite a common case in software development. In such a situation developers often spend more time on research what in fact needs to be implemented than on the implementation itself. Additionally, as a result it frequently happens that implementation is not exactly what the customer wanted and additional work is required to handle the requirement. What's worth mentioning is that depending on staff characteristics, it might be a really demotivating factor, Lack of motivation - the last but not least. It is one of the biggest performance and productivity problems. While of course some people are more or less motivated by their nature, an employee has the biggest impact on this factor. It is strictly the responsibility of leaders, managers and organizational culture if developers will be motivated or not. Usually it is mainly about the atmosphere and relationships. Most of the above issues are not easy to address. Some require aside projects that do not bring immediate value (no customer paying for them), some need major organizational changes. What I find the most difficult here is to create business cases for them. How to exactly measure developer productivity and how to estimate how each change would affect it. The lack of clear indicators is usually a blocker for many organizations to start changes or projects solving the above problems. Very strong conviction and developer feelings are often not enough."},{"title":"How Codeship Deals with Service Interruptions","tags":"devops","url":"http://ciandcd.github.io/how-codeship-deals-with-service-interruptions.html","text":"From: http://architects.dzone.com/articles/how-codeship-deals-service As a continuous delivery service, we form an integral part of our customers' workflow. Any interruption to our service impacts our customers a lot, so we need to make sure we're resolving any service interruption immediately. By building our system to be very resilient with immutable infrastructure at the core, we're able to react to most interruptions quickly. Additionally we've implemented automatic health checks to remove systems from production as soon as the load or network latency becomes too high. This is very important when deploying into cloud systems because the cloud behaves more unpredictably than your own hardware. You want to take this into account when building your system so your team can be focused on building your product at all times. A recent interview I did with Statuspage prompted me to write in more detail about how we respond to incidents. In this post, I'll outline our processes, tools, and the document templates we use to make sure we have a repeatable workflow. Downtime Recognition Downtime recognition must always be based on metrics in your system. If downtimes are only recognized when your team or customers see it in the production application a negative impact has already happened. To make sure we've got great insights into our infrastructure, we use Librato Metrics , Pingdom , and New Relic for collecting various application and server metrics. Every system has various alerts set that trigger Pagerduty incidents. Our on-call hours We use Pagerduty to manage on-call times and notify our team of any service interruption. We're spread out between Vienna and Boston and a few other remote locations, so we can cover almost 24 hours without getting into somebody's night time. A person woken up in the middle of the night will not perform at their best during the outage. Plus, it ruins their productivity for the next day. Our local coverage times are as follows: Europe: 4 a.m. – 4 p.m. CEST Boston: 10 a.m. – 10 p.m. EST Both are reasonable times for each city. In the worst case, somebody might be woken up in the morning in Vienna, though that's at a time where we get much less traffic, so service interruptions are less likely. We can cover the whole US workday, from East to West Coast, from Boston so nobody has to be woken up during this most crucial time of the day for us. We've recently switched to weekly schedules, so you're on call for a week in your timezone. Weekends are covered completely by one person, so it's rare that you have to be on call on a weekend. To make sure there's always somebody covering even if somebody misses a pager call, we've set up a secondary schedule where even non-technical people are on call. Our primary versus secondary schedules The primary schedule is always comprised of people who can fix our infrastructure; the secondary is made up of most of the rest of the team. This has several advantages. First of all, we share on-call duty between most people in the team. Even if one of the developers doesn't see the page, somebody else picks up and can ping other developers and talk to customers. Having only the development team on a schedule would put an unfair burden on the dev team. It would also make it more difficult to resolve the issue and communicate with customers at the same time. Through this secondary schedule system, we always have somebody we can pull in to take over customer communication. That can mean calling the person on secondary support at night as well if that's necessary. It usually isn't necessary to pull in the secondary, but it's definitely a great fallback to have. In case a pager call falls through the first and second layer, Jim, our VPE, is on a third layer, and I am on a fourth layer. This makes sure the escalation goes all through the technical team in Codeship if necessary (thankfully it hasn't been for a long time). While growing our team, we're constantly iterating on our coverage hours to make sure we have great coverage at all times. Downtime Remediation and Follow Up As soon as the issue is discovered, the development team will decide who will look into it. Our customer success team will decide who will take over communication for this specific issue and will work with the assigned developer to communicate regularly with our customers. By defining those two people, we have a clear line of communication between the developers who know the current status of the issue and customer success who can answer support requests and send updates through Twitter or our Statuspage. Steps while fixing the issue The first and most important step is to update our Statuspage . We want to make sure we're always communicating immediately with our customers if we have a service interruption. We're always trying to ask ourselves, \"How would we expect important services that we use to communicate with us in a downtime?\" and \"How can we build more long-term trust through our downtime communication?\" The way you communicate to your customers during a downtime will decide if they trust you more or less in the future. A single interruption doesn't necessarily erode trust, but a downtime with bad communication definitely will. Customers expect full transparency when issues are coming up, including a detailed description of what the issue is and how it will be resolved. While fixing the issue, we will regularly update our customers through various channels, and the development team and customer success are in constant contact with each other. As we're currently growing our engineering team, we will probably move to a setup where the roles are even more clearly defined. As soon as you have several teams working on different services where downtime might impact several services at once, you need to have a higher level management role in place to manage the downtime effort. Heroku released a very interesting post last year about their incident response with the Incident Command System. It's definitely an interesting approach that we're thinking about for the future. Debriefings As soon as an issue is fixed, we start work on a debriefing about the service interruption. We've found structured debriefings to be an incredibly helpful tool to get an overview of the whole issue and determine future steps we need to take. Because we're spread out between Europe and the US, compiling this debrief is typically done asynchronously on GitHub with people commenting or committing and pushing changes directly onto the debriefing branch. Meetings or calls are done if necessary, but typically don't have to be part of a debriefing. We have a GitHub repository called Operations that contains our debriefings as well as a template for them. I've copied the template into a GitHub Gist . The template contains an executive summary so anyone can get a quick overview on what happened. We can share this summary internally. The debriefing template then dives into different aspects of the issue, from detailed technical descriptions to customer communication. We also capture any steps we can take to improve our resolution efforts in the future. We want to make sure this document accurately reflects what has happened and digs deep while at the same time doesn't push blame on anyone. Only when we analyze exactly what happened in our technology and process are we able to improve. But we won't be able to dig deep and make sure everyone opens up about their mistakes if we then turn around and immediately use it to punish people. One very important lesson we've learned about our template is to leave irrelevant sections of the template empty instead of removing them for a particular debriefing. Removing irrelevant sections makes it difficult to compare debriefings to one another. It can also cause somebody not directly involved with the downtime to wonder if something is missing or getting hidden. Debriefings should be as transparent as possible to maintain your team's trust in the process. The debriefings also capture To Dos, so it's easy for anyone to follow up and see if we followed through with steps we needed to improve. These debriefings also form the core data we use to write any public follow-up blog post or any other customer communication we're doing after the resolution. Conclusions By having a well-defined process that goes from detection of an issue to customer communication to solving the issue to debriefing, we can focus all of our energy on resolving an issue once it comes up instead of scrambling to find an ad hoc process. In fact, we want this process to be so second nature to everyone that we wrote down a document outlining those steps and hung them up in some of our restrooms. You can download the document here . We also include the document as part of our onboarding activities. This ensures that everyone who joins knows exactly how to resolve an issue once they become part of the Pagerduty schedule. While we're definitely going to change and formalize our process as we grow, we feel we've got the right balance of process and flexibility for the current size of our team. Let us know if you have a different process for finding or resolving issues in your system in the comments."},{"title":"Launching Missiles With Haskell","tags":"devops","url":"http://ciandcd.github.io/launching-missiles-with-haskell.html","text":"From: http://java.dzone.com/articles/launching-missiles-haskell Haskell advocates are fond of saying that a Haskell function cannot launch missiles without you knowing it. Pure functions have no side effects, so they can only do what they purport to do. In a language that does not enforce functional purity, calling a function could have arbitrary side effects, including launching missiles . But this cannot happen in Haskell. The difference between pure functional languages and traditional imperative languages is not quite that simple in practice. Programming with pure functions is conceptually easy but can be awkward in practice. You could just pass each function the state of the world before the call, and it returns the state of the world after the call. It's unrealistic to pass a program's entire state as an argument each time, so you'd like to pass just that state that you need to, and have a convenient way of doing so. You'd also like the compiler to verify that you're only passing around a limited slice of the world. That's where monads come in. Suppose you want a function to compute square roots and log its calls. Your square root function would have to take two arguments: the number to find the root of, and the state of the log before the function call. It would also return two arguments: the square root, and the updated log. This is a pain, and it makes function composition difficult. Monads provide a sort of side-band for passing state around, things like our function call log. You're still passing around the log, but you can do it implicitly using monads. This makes it easier to call and compose two functions that do logging. It also lets the compiler check that you're passing around a log but not arbitrary state. A function that updates a log, for example, can effect the state of the log, but it can't do anything else. It can't launch missiles. Once monads get large and complicated, it's hard to know what side effects they hide. Maybe they can launch missiles after all . You can only be sure by studying the source code. Now how do you know that calling a C function, for example, doesn't launch missiles? You study the source code. In that sense Haskell and C aren't entirely different. The Haskell compiler does give you assurances that a C compiler does not. But ultimately you have to study source code to know what a function does and does not do."},{"title":"Fixing Your Org by Continually Breaking It - DevOps Days Austin","tags":"devops","url":"http://ciandcd.github.io/fixing-your-org-by-continually-breaking-it-devops-days-austin.html","text":"From: http://java.dzone.com/articles/fixing-your-org-continually Here is the third post in a six part series focusing on DevOps Days Austin . Following Cote's presentation , next in the line up of speakers on day one was Paul Read of Release Engineering Approaches. Paul's talk tackles the \"importance of continually breaking your organization with experiments, some surprising examples, and how to do it in such a way that you aren't left with a broken organization.\" Take a listen, and check out his awesome tie! (also see his slides below) Some of the ground Paul covers: Learning from Toyota Kata The culture of continuous learning and experimentation/improvement Making \"the pain visible\" Where Paul sees DevOps going in the future Still to come Cameron Haight – Gartner John Willis – Docker Barton George (Sputnik ignite talk) – Dell Extra credit reading The coming donkey apocalypse — Cote Opening Keynote – Damon Edwards Pau for now…"},{"title":"How Much For This Software?","tags":"devops","url":"http://ciandcd.github.io/how-much-for-this-software.html","text":"From: http://java.dzone.com/articles/how-much-software \"Here is the specification; how much will it cost to create this software?\" I hear this almost every day from clients of Teamed.io and prospects that are planning to become our clients and outsource the software development to us. My best answer is \"I don't know; it depends.\" Sounds like a strange response for someone who claims he knows what he is doing, isn't it? \"Here is the 20-page specification that explains all the features of the product; how come you can't estimate the cost?\" I can, but I won't. Here is why. Let me ask you something else: Why do you need an estimate? Yes, I mean it — why do you ask me how much it will cost to develop the software for you? I can tell you why. Because you don't trust me. And obviously you have good reasons for that, simply because we both know that a software product is something that can stay in development forever and never be finished. Look at YouTube, for example. How much do you think it would take to create a website like this, where users are able to upload their videos and then stream them? A few days for a good web developer. Will it stream video? Yes, it will. Will it be ready to compete with YouTube? No, it won't. Add a few hundred developers to the team, a few years, and a few million dollars, and even then you will be behind YouTube. Simply because it's a never-ending process. Any software, no matter how big or good it is, always needs more and more improvements and bug fixes. Thus, when you ask me how much it will cost to create a system similar to YouTube, according to your specifications, my honest and accurate answer should be: \"All of your money, and it won't be enough.\" Will you sign a contract and outsource the project to me after that answer? No, you won't. That's why I have to lie and promise something like \"three months and $40,000\". Why would you trust me? If you're smart enough, you won't. My point is that no matter what I promise you, I will be wrong. Terribly wrong. What is the solution? What do you do? I totally understand that you do need a number to make your plans and secure the money. You need to choose the right software outsourcing partner, and you need to know what to expect and when, but ... You're asking the wrong question ! This question has only one valid answer, and it's the answer nobody will ever give you — the development will take forever and will consume all your money. All other answers are simply a lie. I'm sorry if I've delivered bad news to you. But let's get back to the original problem: Why are you asking me how much it will take to develop the software if you know it's a never-ending process and there is basically no limit? Because you want to make sure your $40,000 will be used the right way and will produce the maximum it can produce. To get this assurance from me, you're asking for an estimate. I'm telling you that your software will be ready for $40K, and you sleep well. Until the moment you realize you've been fooled. By your own self. Your concern is perfectly valid. You want to spend no more than $40K, and you want to get a product that will help you achieve your business goals. For example, you want to get into the market and acquire your first few thousand users. In other words, your biggest worry is that your dollars will be turned into the right amount of the right software. Any software team can consume your $40K, but each team will produce a different amount of software with different quality. My point is that instead of asking how much a software project will cost, you should ask how much software we can create for each dollar you give us and what quality will it be. Basically, don't ask us to estimate how much gas it will take to get to the finish line, because there is no finish line. Instead, ask us how much we charge per gallon and how many gallons we consume per mile. Different teams use different metrics to measure their results (to be honest, most of them don't use any). We, at Teamed.io , use hits of code , bugs, pull requests, test coverage, and a few other metrics as measurable indicators of quantity and quality. We know exactly how much software we can produce for each $100 you pay us. Collect those numbers from other teams and compare them. Also, make sure you can control these numbers during the course of the project. That's the guarantee you're looking for. Now you know what you're buying and how much you're paying for it. In other words, like I said above, having these numbers in front of you will assure you that your money is producing the maximum amount of software it can produce, at the highest quality. The only question left is how you can know you're buying the right software. In other words, you know how much we charge per gallon and how many gallons we use per mile, but how do you know we're driving in the right direction and not making too many circles or detours? There is only one mechanism to guarantee that: regular checkpoints. You should ask us whether we deliver the software in small and regular increments, and whether we allow you to conduct regular independent reviews of our progress. Also, make sure we prioritize our technical goals, use milestones, versionalize releases, publish release notes, etc. Make sure that in the course of our journey, you are able to control the progress and make corrections. To summarize, this is wrong (because there is no \"there\"): Hey driver, how much will it cost to get there? And this is right: Hey driver, how much do you charge per mile, and do you have a map? Hope I've made my point clear :)"},{"title":"Problems with Cobertura and Sonar 5.1","tags":"devops","url":"http://ciandcd.github.io/problems-with-cobertura-and-sonar-51.html","text":"From: http://java.dzone.com/articles/problems-cobertura-and-sonar Recently, I was having some bother trying to use Sonar 5.1 with my Grails 2.4.4 project. I was using the usual Groovy stuff: Gmetrics, Codenarc and Cobertura . For the Sonar database I was using Postgres 9.4 . The logfile for the Sonar runner just gave me this: build 22-Jun-2015 07:44:30 INFO: ------------------------------------------------------------------------ build 22-Jun-2015 07:44:30 INFO: EXECUTION FAILURE build 22-Jun-2015 07:44:30 INFO: ------------------------------------------------------------------------ build 22-Jun-2015 07:44:30 Total time: 9.153s build 22-Jun-2015 07:44:30 Final Memory: 30M/1039M build 22-Jun-2015 07:44:30 INFO: ------------------------------------------------------------------------ error 22-Jun-2015 07:44:30 ERROR: Error during Sonar runner execution error 22-Jun-2015 07:44:30 ERROR: Unable to execute Sonar error 22-Jun-2015 07:44:30 ERROR: Caused by: Unable to save file sources error 22-Jun-2015 07:44:30 ERROR: Caused by: -1 Not much use! I thought there was some permission problem, since \"Unable to save file sources\" usually means that! But there were no permission issues. I then disabled the Cobertura part of the analysis and things were ok, so it was something wrong with the Cobertura part. I then: enabled verbose logging -- sonar.verbose=true enabled full stack trace logging -- using the -e switch enabled full debug logging with the -- using the -X switch this provided a few more clues. ? error 22-Jun-2015 11:09:06 ERROR: Error during Sonar runner execution build 22-Jun-2015 11:09:06 INFO: ------------------------------------------------------------------------ error 22-Jun-2015 11:09:06 org.sonar.runner.impl.RunnerException: Unable to execute Sonar error 22-Jun-2015 11:09:06 at org.sonar.runner.impl.BatchLauncher$1.delegateExecution(BatchLauncher.java:91) error 22-Jun-2015 11:09:06 at org.sonar.runner.impl.BatchLauncher$1.run(BatchLauncher.java:75) error 22-Jun-2015 11:09:06 at java.security.AccessController.doPrivileged(Native Method) error 22-Jun-2015 11:09:06 at org.sonar.runner.impl.BatchLauncher.doExecute(BatchLauncher.java:69) error 22-Jun-2015 11:09:06 at org.sonar.runner.impl.BatchLauncher.execute(BatchLauncher.java:50) error 22-Jun-2015 11:09:06 at org.sonar.runner.api.EmbeddedRunner.doExecute(EmbeddedRunner.java:102) error 22-Jun-2015 11:09:06 at org.sonar.runner.api.Runner.execute(Runner.java:100) error 22-Jun-2015 11:09:06 at org.sonar.runner.Main.executeTask(Main.java:70) error 22-Jun-2015 11:09:06 at org.sonar.runner.Main.execute(Main.java:59) error 22-Jun-2015 11:09:06 at org.sonar.runner.Main.main(Main.java:53) error 22-Jun-2015 11:09:06 Caused by: java.lang.IllegalStateException: Unable to save file sources error 22-Jun-2015 11:09:06 at org.sonar.batch.index.SourcePersister.persist(SourcePersister.java:84) error 22-Jun-2015 11:09:06 at org.sonar.batch.phases.DatabaseModePhaseExecutor.executePersisters(DatabaseModePhaseExecutor.java:165) error 22-Jun-2015 11:09:06 at org.sonar.batch.phases.DatabaseModePhaseExecutor.execute(DatabaseModePhaseExecutor.java:133) error 22-Jun-2015 11:09:06 at org.sonar.batch.scan.ModuleScanContainer.doAfterStart(ModuleScanContainer.java:264) error 22-Jun-2015 11:09:06 at org.sonar.api.platform.ComponentContainer.startComponents(ComponentContainer.java:92) error 22-Jun-2015 11:09:06 at org.sonar.api.platform.ComponentContainer.execute(ComponentContainer.java:77) error 22-Jun-2015 11:09:06 at org.sonar.batch.scan.ProjectScanContainer.scan(ProjectScanContainer.java:235) error 22-Jun-2015 11:09:06 at org.sonar.batch.scan.ProjectScanContainer.scanRecursively(ProjectScanContainer.java:230) error 22-Jun-2015 11:09:06 at org.sonar.batch.scan.ProjectScanContainer.doAfterStart(ProjectScanContainer.java:220) error 22-Jun-2015 11:09:06 at org.sonar.api.platform.ComponentContainer.startComponents(ComponentContainer.java:92) error 22-Jun-2015 11:09:06 at org.sonar.api.platform.ComponentContainer.execute(ComponentContainer.java:77) error 22-Jun-2015 11:09:06 at org.sonar.batch.scan.ScanTask.scan(ScanTask.java:57) error 22-Jun-2015 11:09:06 at org.sonar.batch.scan.ScanTask.execute(ScanTask.java:45) error 22-Jun-2015 11:09:06 at org.sonar.batch.bootstrap.TaskContainer.doAfterStart(TaskContainer.java:135) error 22-Jun-2015 11:09:06 at org.sonar.api.platform.ComponentContainer.startComponents(ComponentContainer.java:92) error 22-Jun-2015 11:09:06 at org.sonar.api.platform.ComponentContainer.execute(ComponentContainer.java:77) error 22-Jun-2015 11:09:06 at org.sonar.batch.bootstrap.GlobalContainer.executeTask(GlobalContainer.java:158) error 22-Jun-2015 11:09:06 at org.sonar.batch.bootstrapper.Batch.executeTask(Batch.java:95) error 22-Jun-2015 11:09:06 at org.sonar.batch.bootstrapper.Batch.execute(Batch.java:67) error 22-Jun-2015 11:09:06 at org.sonar.runner.batch.IsolatedLauncher.execute(IsolatedLauncher.java:48) error 22-Jun-2015 11:09:06 at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) error 22-Jun-2015 11:09:06 at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) error 22-Jun-2015 11:09:06 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) error 22-Jun-2015 11:09:06 at java.lang.reflect.Method.invoke(Method.java:606) error 22-Jun-2015 11:09:06 at org.sonar.runner.impl.BatchLauncher$1.delegateExecution(BatchLauncher.java:87) error 22-Jun-2015 11:09:06 ... 9 more error 22-Jun-2015 11:09:06 Caused by: java.lang.ArrayIndexOutOfBoundsException: -1 error 22-Jun-2015 11:09:06 at java.util.ArrayList.elementData(ArrayList.java:371) error 22-Jun-2015 11:09:06 at java.util.ArrayList.get(ArrayList.java:384) error 22-Jun-2015 11:09:06 at com.google.protobuf.RepeatedFieldBuilder.getBuilder(RepeatedFieldBuilder.java:245) error 22-Jun-2015 11:09:06 at org.sonar.server.source.db.FileSourceDb$Data$Builder.getLinesBuilder(FileSourceDb.java:2911) error 22-Jun-2015 11:09:06 at org.sonar.batch.index.SourceDataFactory. Now, I could see earlier in the log, that the Cobertura analysis had finished. I could also see that the Cobertura coverage.xml generated ok (this is the file which collates the code coverage info). The next step after creating the coverage.xml file was for the sonar runner to parse it and send a request to Postgres, something had to going wrong at the parsing stage since connecting to Postgres was definitely not an issue (remember everything fine when Cobertura disabled). I knew there was no problem sending the request to Postgres, so thought there must be something odd in the coverage.xml file which meant Sonar runner failed to parse it. As stated, the coverage.xml file details what line number for each class has and hasn't been covered. Sample: ? <class name=\"com.dublintech.me.ApiLogFilters\" filename=\"com/dublintech/me/ApiLogFilters.groovy\" line-rate=\"0.0\" branch-rate=\"0.0\" complexity=\"0.0\"> <methods> <method name=\"<clinit>\" signature=\"()V\" line-rate=\"0.0\" branch-rate=\"1.0\"> <lines> <line number=\"25\" hits=\"0\" branch=\"false\"> </line></lines> </method> ... </methods></class> ... So what kind of things could make the parsing barf? What about if there was some odd line number in the coverage.xml file? hmmm... To check this, I ran the following grep: ? > grep \"line number\" coverage.xml This gave too much. What about any negative line numbers? >grep \"line number=\\\"\\-\" coverage.xml ? Nope, none. Ok go back to exception, look at this line: ? java.lang.ArrayIndexOutOfBoundsException: -1 hmmm... If a line number was 0, I wonder could it make some array parsing in the sonar runner throw index out of bounds? ? >grep \"line number=\\\"0\" coverage.xml Hit! Time to grep lines before and after and get more info about this file. ? >grep -C20 \"line number=\\\"0\" coverage.xml This gave me the culprit. It made no sense to me why Cobertura was saying that linenumber 0 had 0 hits. It was still possible to open the Cobertura html report and view the analysis. Sonar was just barfing when it was parsing it. So I removed this file from Cobertura analysis by adding the following to my build config. ? 123456 coverage { xml = true exclusions = [ \"**/com/dublintech/me/MyOddFile*\" ] } I then re-ran and hey presto, everything working. The file wasn't in the coverage.xml file. This meant the Sonar runner could parse the file and everything was ok. I like sonar, I like a stable build and I like rapid feedback so yeah I was a happy person when it was working again!"},{"title":"What is important for an IT Ops to team more effectively with preproduction teams? #DevOps","tags":"devops","url":"http://ciandcd.github.io/what-is-important-for-an-it-ops-to-team-more-effectively-with-preproduction-teams-devops.html","text":"From: http://blog.devopsguys.com/2015/06/23/what-is-important-for-an-it-ops-to-team-more-effectively-with-preproduction-teams-devops/ \"DevOps can present IT Operations teams with new ‘customers' in development and test. What traditional or new tools and technologies are most likely to be important for IT Ops to team more effectively with preproduction teams? What information does IT Ops need to pass right to left and which tools are most likely to aid in that?\" The short answer is \"A whiteboard marker, a pad of Post-It notes and a couple of pizzas\" :-) That answer is a bit tongue-in-cheek, but there is a serious side to it; whilst new tools can be an important part of DevOps (particularly in A utomation) you can get started in changing your C ulture and improving your S haring with very simple tools i.e. the aforementioned whiteboard marker, Post-It notes and pizza. Start to break down the silos by getting key people in a room with some blank walls and whiteboards and start sharing information, mapping out your value stream and trying to find out, collaboratively, where the bottlenecks in your existing processes are. Once you've identified your key constraints then fire up Google and start searching for the tools to solve your problems (or visit a site like DevOpsBookmarks ). DevOpsGuys, like most organisations, have our own \"Opinionated Stack\" – we like the Atlassian Toolset for managing our Agile workflow, TeamCity or Jenkins as our CI tool of choice, Ansible as our configuration management tool for Linux, Powershell DSC for Windows, AppDynamics as our APM tool, Redgate for our Database Lifecycle Management (DLM) and so on. We partner with many of these companies now because we've \"dogfooded\" the products internally and with our customers and they've worked well for our use cases. We always \"try before we buy\" and we \"try before we partner\" too because, as they say, \"your mileage may vary\" (YMMV). This comes back to fostering a culture of experimentation – give something a try and see what works for you. We started off using Atlassian HipChat as our chat tool and we really liked it. Then we tried Slack and we liked that one more, so we switched. YMMV. One additional point worth mentioning – the premise of the question is flawed! They aren't customers they're colleagues. There isn't a silo of \"Us\" (IT Ops=supplier) versus \"Them\" (Everyone Else=customer). We are supposed to be breaking down these silos to create cross-functional, multi-disciplinary, product-based teams. Development, Test, IT Security, Networks shouldn't be silos any more – they are people in our team, sitting over the desk from us, attending our daily standups, eating our pizza :-) The Q&A above is part of material prepared as our contribution to an CA ebook on \"Agile Operations\". We wrote our thoughts on 6 questions, of which 4 will be used in the ebook, scheduled to come out in August 2015. We'll post the remaining 2 questions with our answers onto the blog over the next 2 weeks."},{"title":"Get Back Up and Try Again: Retrying in Python","tags":"devops","url":"http://ciandcd.github.io/get-back-up-and-try-again-retrying-in-python.html","text":"from:http://python.dzone.com/articles/get-back-and-try-again I don't often write about tools I use when for my daily software development tasks. I recently realized that I really should start to share more often my workflows and weapons of choice. One thing that I have a hard time enduring while doing Python code reviews, is people writing utility code that is not directly tied to the core of their business. This looks to me as wasted time maintaining code that should be reused from elsewhere. So today I'd like to start with retrying , a Python package that you can use to… retry anything. It's OK to fail Often in computing, you have to deal with external resources. That means accessing resources you don't control. Resources that can fail, become flapping, unreachable or unavailable. Most applications don't deal with that at all, and explode in flight, leaving a skeptical user in front of the computer. A lot of software engineers refuse to deal with failure, and don't bother handling this kind of scenario in their code. In the best case, applications usually handle simply the case where the external reached system is out of order. They log something, and inform the user that it should try again later. In this cloud computing area, we tend to design software components with service-oriented architecture in mind. That means having a lot of different services talking to each others over the network. And we all know that networks tend to fail, and distributed systems too. Writing software with failing being part of normal operation is a terrific idea. Retrying In order to help applications with the handling of these potential failures, you need a plan. Leaving to the user the burden to \"try again later\" is rarely a good choice. Therefore, most of the time you want your application to retry. Retrying an action is a full strategy on its own, with a lot of options. You can retry only on certain condition, and with the number of tries based on time (e.g. every second), based on a number of tentative (e.g. retry 3 times and abort), based on the problem encountered, or even on all of those. For all of that, I use the retrying library that you can retrieve easily on PyPI . retrying provides a decorator called retry that you can use on top of any function or method in Python to make it retry in case of failure. By default, retry calls your function endlessly until it returns rather than raising an error. import random from retrying import retry @retry def pick_one(): if random.randint(0, 10) != 1: raise Exception(\"1 was not picked\") This will execute the function pick_one until 1 is returned by random.randint . retry accepts a few arguments, such as the minimum and maximum delays to use, which also can be randomized. Randomizing delay is a good strategy to avoid detectable pattern or congestion. But more over, it supports exponential delay, which can be used to implement exponential backoff , a good solution for retrying tasks while really avoiding congestion. It's especially handy for background tasks. @retry(wait_exponential_multiplier=1000, wait_exponential_max=10000) def wait_exponential_1000(): print \"Wait 2&#94;x * 1000 milliseconds between each retry, up to 10 seconds, then 10 seconds afterwards\" raise Exception(\"Retry!\") You can mix that with a maximum delay, which can give you a good strategy to retry for a while, and then fail anyway: # Stop retrying after 30 seconds anyway >>> @retry(wait_exponential_multiplier=1000, wait_exponential_max=10000, stop_max_delay=30000) ... def wait_exponential_1000(): ... print \"Wait 2&#94;x * 1000 milliseconds between each retry, up to 10 seconds, then 10 seconds afterwards\" ... raise Exception(\"Retry!\") ... >>> wait_exponential_1000() Wait 2&#94;x * 1000 milliseconds between each retry, up to 10 seconds, then 10 seconds afterwards Wait 2&#94;x * 1000 milliseconds between each retry, up to 10 seconds, then 10 seconds afterwards Wait 2&#94;x * 1000 milliseconds between each retry, up to 10 seconds, then 10 seconds afterwards Wait 2&#94;x * 1000 milliseconds between each retry, up to 10 seconds, then 10 seconds afterwards Wait 2&#94;x * 1000 milliseconds between each retry, up to 10 seconds, then 10 seconds afterwards Wait 2&#94;x * 1000 milliseconds between each retry, up to 10 seconds, then 10 seconds afterwards Traceback (most recent call last): File \"<stdin>\", line 1, in <module> File \"/usr/local/lib/python2.7/site-packages/retrying.py\", line 49, in wrapped_f return Retrying(*dargs, **dkw).call(f, *args, **kw) File \"/usr/local/lib/python2.7/site-packages/retrying.py\", line 212, in call raise attempt.get() File \"/usr/local/lib/python2.7/site-packages/retrying.py\", line 247, in get six.reraise(self.value[0], self.value[1], self.value[2]) File \"/usr/local/lib/python2.7/site-packages/retrying.py\", line 200, in call attempt = Attempt(fn(*args, **kwargs), attempt_number, False) File \"<stdin>\", line 4, in wait_exponential_1000 Exception: Retry! A pattern I use very often, is the ability to retry only based on some exception type. You can specify a function to filter out exception you want to ignore or the one you want to use to retry. def retry_on_ioerror(exc): return isinstance(exc, IOError) @retry(retry_on_exception=retry_on_ioerror) def read_file(): with open(\"myfile\", \"r\") as f: return f.read() retry will call the function passed as retry_on_exception with the exception raised as first argument. It's up to the function to then return a boolean indicating if a retry should be performed or not. In the example above, this will only retry to read the file if an IOError occurs; if any other exception type is raised, no retry will be performed. The same pattern can be implemented using the keyword argument retry_on_result , where you can provide a function that analyses the result and retry based on it. def retry_if_file_empty(result): return len(result) <= 0 @retry(retry_on_result=retry_if_file_empty) def read_file(): with open(\"myfile\", \"r\") as f: return f.read() This example will read the file until it stops being empty. If the file does not exist, an IOError is raised, and the default behavior which triggers retry on all exceptions kicks-in – the retry is therefore performed. That's it! retry is really a good and small library that you should leverage rather than implementing your own half-baked solution!"},{"title":"Continuous Integration and Delivery with Docker","tags":"devops","url":"http://ciandcd.github.io/continuous-integration-and-delivery-with-docker.html","text":"from:http://java.dzone.com/articles/continuous-integration-and-0 Written by Jaroslav Holub for The Codeship Blog . Continuous delivery is all about reducing risk and delivering value faster by producing reliable software in short iterations. As Martin Fowler says , you actually do continuous delivery if: Your software is deployable throughout its lifecycle. Your team prioritizes keeping the software deployable over working on new features. Anybody can get fast, automated feedback on the production readiness of their systems any time somebody makes a change to them. You can perform push-button deployments of any version of the software to any environment on demand. Containerization of software allows us to further improve on this process. The biggest improvements are in speed and in the level of abstraction used as a cornerstone for further innovations in this field. In this post, I'll show you how to set up a continuous delivery pipeline using Docker. We'll see how using this tool for Linux containers as part of the continuous delivery pipeline lets us nicely encapsulate the build process of a service. It also lets us deploy any revision with a few simple steps. I'll mainly use the term continuous delivery in this article, because it stands for the full circle of steps leading to our ultimate goal. However, continuous integration is the most substantial part of continuous delivery. Continuous Integration with Docker Let's take a Hello World web server written in Go as an example service. You can find all the code used in this example here: https://github.com/ContainerSolutions/cd-with-docker-tutorial The continuous integration setup consists of: running unit tests building the Docker image that we use to build our service running the build container and compiling our service building the Docker image that we run and deploy pushing the final image to a Docker registry Automated testing Running tests in this example is as trivial as it should be: go test Building Docker image The core of a single service integration is making the end artifact — Docker image in our case. Because I've deliberately chosen the compiled language Go in this example, we need to build an executable file as part of our integration process. We'll eventually place the executable file inside this Docker image. Now one might think that we would build our web server executable file using build tools installed on the host dedicated to continuous integration and then somehow copy the binary to the Docker image. But this is a no-no in the containerized world. Let's do it all in containers. That way, we won't rely on any build tools installed on hosts, and it'll make the whole setup easily reproducible and encapsulated. Building an executable file can be part of a single Docker image build process together with runtime environment setup. Or we can separate the two. Having everything in a single build process, we would end up with extra content (build process leftovers) in our Docker image filesystem, even if we clean it afterwards in separate RUN commands within the Dockerfile. Some people use tricks to create, manipulate, and remove unwanted stuff in a single RUN command. Although it's sometimes handy, I can't generally recommend it; in my opinion this adds to Dockerfile complexity. Of course, there are situations where you might want to retain your sources and all in the end artifact. The approach I recommend, however, is to create separate \"build\" and \"distribution\" Dockerfiles. Use Dockerfile.build to do the heavy lifting during building the software, and use Dockerfile.dist to create the distributable Docker image, as light and clean as possible. The following is Dockerfile.build . As you can see, once we run the build file, we create the container from a golang image, compile our example service, and output the binary. FROM golang:1.4 RUN mkdir -p /tmp/build ADD hello-world.go /tmp/build/ WORKDIR /tmp/build RUN go build hello-world.go CMD tar -czf - hello-world In Dockerfile.dist , we only use this binary and run it on runtime: FROM debian:jessie RUN mkdir /app ADD build.tar.gz /app/ ENTRYPOINT /app/hello-world Our build.sh script — the essential part of our continuous integration pipeline — then looks like this: # !/bin/sh docker build -t hello-world-build -f Dockerfile.build . docker run hello-world-build > build.tar.gz docker build -t hello-world -f Dockerfile.dist . As you can see, these three simple Docker commands get us a clean, small Hello-World Docker image that's ready to be deployed and run on demand. Once both images used in the FROM clauses are pulled and cached locally, our build process will be a matter of milliseconds or at most a few seconds, with a very small resources footprint. Storing Docker images Once our build process artifact is created, we want to push it to Docker Registry, where it will be available for deployments. Please note that tagging images properly is very important. Docker ecosystems suffer from the usage of \"latest\" tag. If you use a unique tag for every new image, then all your image versions will be easily accessible for deployment in the future. We can choose whether we want to use our own Docker Registry or rely on Docker Hub . On Docker Hub, you can store public or private repositories of images. It's also the first place people would look for your images (if you want anyone to look for them). Your own Docker Registry on the other hand gives you full control over your images storage, performance, and security. More advanced setups might combine both approaches. This way you can tag the new image with an appropriate tag and push it to a public hub (replace your_username and your_tag with actual values): # !/bin/sh docker tag hello-world:latest your_username/hello-world:your_tag docker push your_username/hello-world:your_tag Continuously Delivered Containers Once we have our Docker images building pipeline working and images nicely stashed in a repository, we definitely want to get our service deployed. How you deploy your applications depends on your infrastructure or cloud provider. A few cloud providers support Docker images in their APIs these days (e.g., Amazon EC2 Container Service , Digital Ocean , or Giant Swarm ). You can further leverage the power of containerized applications with resource abstraction tools like Apache Mesos (read more about running containers on Mesos ) or Google Kubernetes that let you deploy and manage containers in their own ways. In case of our Hello World example, deploying remotely means running the following command remotely on a target machine with Docker installed on it: # !/bin/sh docker stop hello-production docker run --rm -p 8000:80 --name hello-production hello-world Beyond Continuous Delivery with Docker Using containerized software does not inherently mean one is implementing microservices. However, containers enable this architectural pattern because they encourage developers to split their monoliths based on separation of concerns. Microservices also promote communication between containerized components over a plain network using standardized and easily replaceable tubes. To learn more about microservices and why they might be a good architectural pattern for your software project, I recommend Building Microservices by Sam Newman. A continuous delivery pipeline with containerized software also allows you to set up a new kind of testing environment; subsets of (micro)services are deployed in small clusters that represent the system under test running with some parts intentionally disabled or disconnected. Creation of such a matrix of deployments and programming against it has little to no additional costs in terms of a continuous integration time. It does have a dramatic impact on the stability and resilience of software in production. Such a testing system allows teams to get ready to deal with any kind of Chaos Monkey ."},{"title":"Enabling DataOps with Easy Log Analytics","tags":"devops","url":"http://ciandcd.github.io/enabling-dataops-with-easy-log-analytics.html","text":"from:http://java.dzone.com/articles/enabling-dataops-easy-log DataOps is becoming an important consideration for organizations. Why? Well, DataOps is about making sure data is collected, analyzed, and available across the company – i.e. Ops insight for your decision-making systems like Hubspot, Tableau, Salesforce and more. Such systems are key to day-to-day operations and in many cases are as important as keeping your customer facing systems up and running. If you think about it, today every online business is a data driven business! Everyone is accountable to have up to the minute answers on what is happening across their systems. You can't do this reliably without having DataOps in place. We have seen this trend across our own customer base at Logentries where more and more customers using log data to implement DataOps across their organization . Using log data for DataOps allows you to perform the following: Troubleshoot your systems managing your data by identifying errors and correlating data sources Get notified when one of these systems is experiencing issues via real time alerts or anomaly detection Analyze how these systems are used by the organization Logentries has always been great at 1 and 2 above, and this week we have enhanced Logentries to now allow you to perform easier and more powerful analytics with our n ew easy-to-use SQL like query language – Logentries QL (LEQL) . LEQL is designed to make analyzing your log data dead simple. There are too many log management tools that are built around complex query languages and require data scientists to operate. Logentries is all about making log data accessible to anyone. With LEQL you are going to be able to use analytical functions like CountUnique, Min, Max, GroupBy, Sort…A number of our users have already been testing these out via our beta program. One great example is how Pluralsight has been using Logentries to manage and understand the usage of their Tableau environment . For example: Calculating the rate of errors over the the past 24 hours e.g. using LEQL Count function Understanding user usage patterns e.g. using GroupBy to understand queries performed grouped by different users Sorting the data to find the most popular queries and how long they are taking Being able to answer these types of questions enables DataOps teams to understand where they need to invest time going forward. For example, do I need to add capacity to improve query performance? Are internal teams having a good user experience or are they getting a lot of errors when they try to access data? At Logentries we are all about making the power of log data accessible to everyone and as we do this we are constantly seeing cool new use cases when using logs. If you have some cool use cases do let us know!"},{"title":"How to Make Sure Your Mobile App is Secure","tags":"devops","url":"http://ciandcd.github.io/how-to-make-sure-your-mobile-app-is-secure.html","text":"from:http://java.dzone.com/articles/how-make-sure-your-mobile-app Mobile app development has become vital for enterprises as they look to support new devices (phones, tablets, wearables, etc.) for internal use while also reaching out to their increasingly mobile customers. This approach makes sense: According to a comScore report , the number of mobile Internet users outnumbered desktop ones for the first time at some point in late 2013, and has since achieved significant separation. Many companies have responded to this change by implementing bring-your-own-device policies and building mobile apps that complement their full websites, mobile Web presence and/or desktop applications. Watch out for pitfalls in mobile apps: General risks and the recent Starbucks example However, both BYOD policies and mobile app development require due diligence around cybersecurity if they are to be worthwhile. Safety starts with well-designed applications that are strongly authenticated, do not leak sensitive data and are safe from popular attack vectors like brute-force password guessing. Unfortunately, many apps still have a long way to go on these fronts. An early 2014 study from MetaIntell discovered that 92 percent of the top 500 most popular Android apps at the time created privacy risks due to data leakage . Wary of leaky apps as well as what kinds of information users put into them, enterprises have understandably been concerned about the impact of mobile apps on their operations and BYOD initiatives. Security is often the biggest barrier to effective BYOD, and justifiably so considering that barely more than 40 percent of employees are required to have a security tool installed, according to Webroot. To get a sense of what could go wrong with today's mobile apps, consider what recently happened to Starbucks. The company's app is a mainstay on many phones, and at one time it accounted for the bulk of all mobile payments made in North America. The issue that arose over the last few months involved unauthorized card reloads and apparent account hijackings. The causes may have been mixed, with poor password management on the part of users possibly exacerbated by exploitation of the app's auto-reload feature and an April 2015 outage of the coffee chain's point-of-sale systems. At the end of the day, Starbucks implemented additional security questions and has been urged to add two-factor authentication into the app to prevent erroneous transactions. Catching mobile app security issues with a test management solution As we can see, mobile app security is multifactorial, requiring best efforts on the parts of end users, developers and infrastructure/network providers. For enterprises, the best approach to ensuring long-term security is to catch potential vulnerabilities early and often with a test management system. A test management solution supports both automated and manual testing , and receiving updates in real-time offers you the ability to make important decisions once issues arise. Regardless of how many tests, sprints and projects your company is running, all of them should be conveniently viewed from a lone interface, enabling a single source of truth that keeps your mobile app development initiatives on track."},{"title":"Ode to a Workstation","tags":"devops","url":"http://ciandcd.github.io/ode-to-a-workstation.html","text":"from:http://java.dzone.com/articles/ode-workstation Every now and then I get work done in the home office. I've written previously about my setup, but after churning out some solution design today, I sat back and really took some time to appreciate the workspace. I'm really pleased with the configuration, it's probably the best setup I've had in years. The desk is a former QLD police desk from the 1940s, so it wasn't built for modern computers – not a problem, the cables run down the back which is just a minor annoyance. The keyboard and mouse are gaming varieties so that they perform well – the old Sennheiser (RF) wireless headset has been with me since 2006 and still works very well. The wooden clock ( recently reviewed ) acts as external speakers, a Bluetooth receiver and has a built in microphone so it can be used as a hands-free option for conference calls. It also features Qi wireless charging capability and also features a thermostat. Under the second monitor is a HDD caddy which supports USB3, and features 4 bays which can be used in parallel. I try to keep the desk reasonably neat, and there's plenty of space so it doesn't get too cluttered. I have a nice view out the window to a small courtyard which gets early morning sun."},{"title":"Ensure Software Security by Understanding the Attack Surface","tags":"devops","url":"http://ciandcd.github.io/ensure-software-security-by-understanding-the-attack-surface.html","text":"from:http://java.dzone.com/articles/ensure-software-security For many organizations, it seems like cyberattacks can come from anywhere, at any time. This sense is heightened by the number of endpoints in play that could be vulnerable to threats. Quality assurance teams must ensure that they have the data on hand to keep these risks at bay. By gathering information on current dangers, companies can better understand the attack surface and establish safeguards. Breaking down elements in play The attack surface contains all possible vulnerabilities - known and unknown - that may exist across your infrastructure, and sums up your risk of exposure. While the attack surface may seem like one big scary entity, it's actually made up of several parts. Tripwire broke considerations down into software, network and human attack surfaces to make this large picture easier to manage. QA professionals should approach the attack surface this way in order to ensure that all aspects are accommodated for rather than being overwhelmed by the big picture. Everything from coding to devices and human error must be considered when gathering information and preparing for potential threats. Analyze data and act on it Testing results can be a critical indicator of what types of vulnerabilities may be present within a program. The Open Web Application Security Project noted that an attack surface analysis will help QA and developers better understand what they're up against and build in security accordingly. During this evaluation, they must determine high risk areas of code, what functions should be reviewed for defects and when the attack surface has changed. This last consideration will be especially critical as further tests and adjustments will be needed to secure the software. Anything that an organization does could affect the attack surface, which means that it will have to be constantly monitored. QA teams need to ask what's changed, how it's different from before and what potential holes were opened in the process. This will help keep the attack surface visibly mapped out, making it easy to strategize how to protect the business, its employees and customers. Reduce the noise While a breach is certainly possible, that doesn't mean it should be easy for attackers to gain entry into business systems. Organizations can reduce their attack surface by decreasing the amount of noise within their infrastructure. Accuvant pointed out that doing this will reduce an attack's operating surface , minimizing the likelihood of malicious access. QA teams can use tactics like configuration management, exploit analysis, patching, sandboxing and secure application development to effectively reduce or eliminate the impact of a vulnerability. \"Integrating these strategies into your security program make it much harder for exploits to attack your organization's systems,\" Accuvant stated. \"By reducing your adversaries' operating surface, you are effectively limiting their attack surface.\" The threat of a vulnerability is a very real concern for businesses. By gathering information on what types of attacks are becoming prevalent and understanding how they can affect company software, QA teams can prepare for these risks and protect their users from the growing attack surface."},{"title":"Reducing Risk Through Security Qa Automation","tags":"devops","url":"http://ciandcd.github.io/reducing-risk-through-security-qa-automation.html","text":"from:http://java.dzone.com/articles/reducing-risk-through-security Organizations are under constant pressure to protect their critical assets from cyberattacks that have plagued a wide variety of industries. However, there is currently no set method of how to ensure that company applications will be safe from these threats. Quality assurance teams have implemented a wide range of approaches to ensure security, but manually executing all of these cases can be time-consuming and lead to potential vulnerabilities. For this reason, QA should look into security automation to reduce risks and improve overall program capabilities . Have realistic expectations When building security into the software development life cycle, there are numerous benefits businesses can see, including seamless protection integration and awareness of team members. An AT&T white paper noted that automated vulnerability scanning can be a great first step for QA teams to implement as it can easily and quickly identify commonly occurring issues . At the same time, however, it's not foolproof, since it cannot detect more sophisticated defects like authentication issues or business logic vulnerabilities. That being said, security QA automation can be a major asset to development efforts and can reduce overall risk, but will still require other tools like manual testing to fully evaluate the threat landscape. After the app has been released, automation can often be essential for finding threats, while enabling QA teams to focus on current projects that are still underway. This helps lower the potential risk across the board while still ensuring that each program gets the attention it needs, no matter where it is in its life cycle. Tools for the job There are a number of resources that QA teams can utilize to test the security of their projects. TechTarget contributor Michael Cobb noted that automated QA verification is often executed through code analysis and vulnerability testing . Both of these assets can quickly find errors that may be easily missed during manual evaluations. This alone helps significantly reduce risks to app functionality and security capabilities while ensuring that QA teams are eliminating common vulnerabilities. These tools paired with human testers can effectively find issues and better protect their projects for the future. \"Despite advances in computer automation, humans are still superior at ensuring applications are developed securely, probably because the best challenge is posed by humans, notably those who can think as an attacker would,\" Cobb wrote. \"However, human work is often more effective if a framework guides it.\" Relying on QA for better security Even if QA teams leverage automated tools for security needs, they must still have an understanding of how these tests work and be able to execute them. Chiron Professional Journal noted that while QA professionals may not often be security experts, having the tools on hand can help them perform the necessary processes and mitigate critical risks. \"Let's be clear here – we're not expecting a QA analyst to be able to cobble together a complicated script to evade an anti-cross-site scripting library … but we should reasonably expect that the analyst can either effectively use a tool, or follow a well-documented process that has varying tests and permutations allowing the analyst to think for themselves and flag questionable results for review by the security experts,\" the Chiron Professional Journal stated."},{"title":"Why We Need Continuous Integration","tags":"devops","url":"http://ciandcd.github.io/why-we-need-continuous-integration.html","text":"from:http://java.dzone.com/articles/why-we-need-continuous Introduction Continuous integration is a practice that helps developers deliver better software in a more reliable and predictable manner. This article deals with the problems developers face while writing, testing and delivering software to end users. Through exploring continuous integration, we will cover how we can overcome these issues. The Problem First, we will take a look at the source of the problem, which lies in the software development cycle. Next, we will cover some of the change conflicts that can take place during that process, and finally we will explore the main factors that can make these problems escalate, followed by an explanation of how continuous integration solves these issues. The Source of the Problem Let's take a look at what a traditional software development cycle looks like. Each developer gets a copy of the code from the central repository. The starting point is usually the latest stable version of the application. All developers begin at the same starting point, and work on adding a new feature or fixing a bug. Each developer makes progress by working on their own or in a team. They add or change classes, methods and functions, shaping the code to meet their needs, and eventually they complete the task they were assigned to do. Meanwhile, the other developers and teams continue working on their own tasks, changing the code or adding new code, solving the problems they have been assigned. If we take a step back and look at the big picture, i.e. the entire project, we can see that all developers working on a project are changing the context for the other developers as they are working on the source code. As teams finish their tasks, they copy their code to the central repository. There are two scenarios that can take place at this point. The code in the central repository is unchanged The code is the same as the initial copy. If this is the case, things are simple, because the system is unchanged. All the ideas we had about the system still stand. This is always the case if you are the only developer working on the application and if you have finished your work before the other members of your team. Either way, things are looking good for you. The system you have created and tested can be delivered to users without additional changes. The code in the central repository has changed The second scenario is that the application you have been working on has changed, and you discover this at the point when you try to copy your code over to the central repository. Changes in the code may or may not be in conflict with the ones you've made. If there are conflicts, you need to resolve them in order to be able to successfully deliver your code to the users. In this case, things could get complicated. Next, we'll explore the types of conflicts that can happen and what you may need to do to resolve them. Change Conflicts There are several types of change conflicts that can occur when integrating code. Here are some of the most common ones. We'll start with the simplest scenarios, and gradually explore the more complex ones. The implementation details have changed - You refactored a method, but so did the developer that has already integrated their code into the central repository. The behavior of the method is the same in all three implementations. You will need to pick the version that will stay, and remove the other implementations. You can even come up with a fourth implementation. This is a simple type of conflict, which you can usually resolve within a few minutes. The APIs you have been relying on have changed - For instance, the behavior of a certain method has changed. This could affect your code in a number of ways — from minor changes that you might need to make, to major structural changes. There is no silver bullet in such cases. You will need to carefully study the changes and make all the fixes. An entire subsystem of the application behaves in a different way - in such cases you will almost certainly be facing a partial, if not a full rewrite of your solution. If this is the case, you will probably need to speak with all the developers working on the application, because such a significant change should not happen without letting the rest of the team know about it. These and a number of other issues could come up, caused by various factors. Different versions of frameworks, libraries, databases are another potential source of conflicts. Once you have updated your code so it can be compiled or interpreted, you also need to remember to repeat all the tests that you have previously ran. These examples show that the amount of work needed to solve a problem that was initially assigned to a developer can easily double. Escalating Factors Here are some of the main factors that can make these problems escalate. The size of the team working on the project. The number of changes that are being pushed back into the main repository is proportional to the number of people on the project. This makes the process of integrating code into the main repository significantly harder. The amount of time passed since the developer got the latest version of the code from the central repository. As time passes, other people working on the same project are integrating more and more of their work, and changing the context in which your code needs to run. Sometimes the changes in the main repository are so big that it's easier to do a complete rewrite of your solution. A large number of changes in the system make integration events more complex and can have a huge effect on the productivity of the team. Such situations are even referred to as \"integration hell\". This process has a number of other negative consequences for your business. Testing and fixing bugs can take forever. Your releases are running late. Teams are stressed out because of long and unpredictable release cycles, and morale deteriorates. Solution: Integrate Continuously The solution to the problem of managing a large number of changes in big integration events is conceptually simple. We need to split these big integration events into much smaller integration events. This way, developers need to deal with a much smaller number of changes, which are easier to understand and manage. To keep integration events small and easily manageable, we need them to happen often. A couple of times a day is ideal. The practice of doing small integrations often is called Continuous Integration . The idea is simple, but at the same time it often appears to be impossible to implement in practice. This is because changing the process requires us to change some of our own habits, and changing habits is difficult. The Practice of Continuous Integration In order to avoid the previously described issues, developers need to integrate their partially complete work back into the main repository on a daily basis, or even a couple of times a day. To accomplish this, they first need to pull in all the changes added to the main repository while they were working on the code. They also must make sure that their code will work once it is integrated into the main repository. The only way to ensure this is to test every feature of the application. What first comes into mind when we start considering continuous integration is that the developers would need to spend half of their time every day testing the code in order not to break the code in the main repository for everyone else. This is why the prerequisite for continuous integration is having an automated test suite. Automated tests take away the burden of the manual, repetitive, and error-prone testing process from the developers. They also make the entire testing process much quicker. A computer can replace hours of manual testing with just minutes of automated testing. Behavior-driven and test-driven development are techniques that help developers write clean, maintainable code while writing tests at the same time. Testing techniques are out of the scope of this article, and you can read more about them in other articles on Semaphore Community . Tests make sense only if they are executed every time the source code changes, without exception. A continuous integration service such as Semaphore CI is a tool which can automate this process by monitoring the central code repository and running tests on every change in the source code. Apart from running tests, they also collect test results and communicate those results to the entire team working on the project. The result of continuous integration is so important that many teams have a rule to stop working on their current task if the version in the central repository is broken. They join the team which is working on fixing the code until tests are passing again. The role of a continuous integration service is to improve the communication between developers by communicating the status of a project's source code. How to Adopt Continuous Integration Continuous integration as a practice makes a big contribution to improving the development process, but also calls for essential changes in the everyday development routine. Adopting it comes with challenges that are easy to overcome if the process is introduced gradually. One of the biggest challenges teams face is the lack of an automated testing suite. A good recipe for overcoming this situation is to start adding automated tests for all new features as they are being developed. At the same time, the developer working on a bug fix should also work to cover the related code with tests. Whenever a bug is reported, the team should first write a failing test to demonstrate the existence of bug. Once the fix is created, the tests should pass. Over time, the automated tests suite gradually becomes more comprehensive, and the developers begin relying on it more and more. Adopting a continuous integration service to communicate the status of the tests to the entire team in the early stages of a project is also important, because it raises awareness of the project status among team members. Conclusion Introducing continuous integration and automated testing into the development process changes the way software is developed from the ground up. It requires effort from all team members, and a cultural shift in the organization. Big changes in the workflow are not easy to pull off quickly. Changes have to be introduced gradually, and all team members and stakeholders need to be on board with the idea. Educating team members about the practice of continuous integration practice and building the automated tests suite needs to be done systematically. Once the first steps have been taken, the process usually continues on its own, as both developers and stakeholders begin seeing the benefits of automated testing suites and the peace of mind that this practice brings to the entire team. Article originally posted on the Semaphore Community ."},{"title":"How to Monitor a Java EE DataSource","tags":"devops","url":"http://ciandcd.github.io/how-to-monitor-a-java-ee-datasource.html","text":"from:http://java.dzone.com/articles/how-monitor-java-ee-datasource Introduction FlexyPool is an open-source framework that can monitor a DataSource connection usage. This tool come out of necessity, since we previously lacked support for provisioning connection pools. FlexyPool was initially designed for stand-alone environments and the DataSource proxy configuration was done programmatically. Using Spring bean aliases , we could even substitute an already configured DataSource with the FlexyPool Metrics-aware proxy alternative. Java EE support Recently, I've been asked about supporting Java EE environments and in the true open-source spirit, I accepted the challenge. Supporting a managed environment is tricky because the DataSource is totally decoupled from the application-logic and made available through a JNDI lookup. One drawback is that we can't use automatic pool sizing strategies, since most Application Servers return a custom DataSource implementation (which is closely integrated with their in-house JTA transaction manager solution), that doesn't offer access to reading/writing the connection pool size. While the DataSource might not be adjustable, we can at least monitor the connection usage and that's enough reason to support Java EE environments too. Adding declarative configuration Because we operate in a managed environment, we can no longer configure the DataSource programmatically, so we need to use the declarative configuration support. By default, FlexyPool looks for the flexy-pool.properties file in the current Class-path. The location can be customized using the flexy.pool.properties.pathSystem property , which can be a: URL (e.g. file:/D:/wrk/vladmihalcea/flexy-pool/flexy-pool-core/target/test-classes/flexy-pool.properties) File system path (e.g. D:\\wrk\\vladmihalcea\\flexy-pool\\flexy-pool-core\\target\\test-classes\\flexy-pool.properties) Class-path nested path (e.g. nested/fp.properties) The properties file may contain the following configuration options: Parameter nameDescription flexy.pool.data.source.unique.name Each FlexyPool instance requires a unique name so that JMX domains won't clash flexy.pool.data.source.jndi.name The JNDI DataSource location flexy.pool.data.source.jndi.lazy.lookup Whether to lookup the DataSource lazily (useful when the target DataSource is not available when the FlexyPoolDataSource is instantiated) flexy.pool.data.source.class.name The DataSource can be instantiated at Runtime using this Class name flexy.pool.data.source.property.* If the DataSource is instantiated at Runtime, each flexy.pool.data.source.property.${java-bean-property} will set the java-bean-property of the newly instantiated DataSource (e.g. flexy.pool.data.source.property.user=sa) flexy.pool.adapter.factory Specifies the PoolAdaptorFactory, in case the DataSource supports dynamic sizing. By default it uses the generic DataSourcePoolAdapter which doesn't support auto-scaling flexy.pool.metrics.factory Specifies the MetricsFactory used for creating Metrics flexy.pool.metrics.reporter.log.millis Specifies the metrics log reported interval flexy.pool.metrics.reporter.jmx.enable Specifies if the jmx reporting should be enabled flexy.pool.metrics.reporter.jmx.auto.start Specifies if the jmx service should be auto-started (set this to true in Java EE environments) flexy.pool.strategies.factory.resolver Specifies a ConnectionAcquiringStrategyFactoryResolver class to be used for obtaining a list of ConnectionAcquiringStrategyFactory objects. This should be set only if the PoolAdaptor supports accessing the DataSource pool size. Hibernate ConnectionProvider Most Java EE applications already use JPA and for those who happen to be using Hibernate, we can make use of the hibernate.connection.provider_class configuration property for injecting our proxy DataSource. Hibernate provides many built-in extension points and the connection management is totally configurable. By providing a custom ConnectionProvider we can substitute the original DataSource with the FlexyPool proxy. All we have to do is adding the following property to our persistence.xml file: <property name=\"hibernate.connection.provider_class\" value=\"com.vladmihalcea.flexypool.adaptor.FlexyPoolHibernateConnectionProvider\"/> Behind the scenes, this provider will configure a FlexyPoolDataSource and use it whenever a new connection is requested: private FlexyPoolDataSource<DataSource> flexyPoolDataSource; @Override public void configure(Map props) { super.configure(props); LOGGER.debug( \"Hibernate switched to using FlexyPoolDataSource \"); flexyPoolDataSource = new FlexyPoolDataSource<DataSource>( getDataSource() ); } @Override public Connection getConnection() throws SQLException { return flexyPoolDataSource.getConnection(); } Instantiating the actual DataSource at runtime If you're not using Hibernate, you need to have the FlexyPoolDataSource ready before the EntityManagerFactory finishes bootstrapping: <?xml version=\"1.0\" encoding=\"UTF-8\"?> <persistence version=\"2.0\" xmlns=\"http://java.sun.com/xml/ns/persistence\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\" http://java.sun.com/xml/ns/persistence http://java.sun.com/xml/ns/persistence/persistence_2_0.xsd\"> <persistence-unit name=\"persistenceUnit\" transaction-type=\"JTA\"> <provider>org.hibernate.jpa.HibernatePersistenceProvider</provider> <jta-data-source>java:global/jdbc/flexypool</jta-data-source> <properties> <property name=\"hibernate.hbm2ddl.auto\" value=\"update\"/> <property name=\"hibernate.show_sql\" value=\"true\"/> <property name=\"hibernate.dialect\" value=\"org.hibernate.dialect.HSQLDialect\"/> <property name=\"hibernate.transaction.jta.platform\" value=\"org.hibernate.service.jta.platform.internal.SunOneJtaPlatform\"/> </properties> </persistence-unit> </persistence> While in a production Java EE environment we use an Application server specific DataSource configuration, for simplicity sake, I'm going to configure the FlexyPooldataSource using the DataSourceDefinition annotation: @DataSourceDefinition( name = \"java:global/jdbc/flexypool\", className = \"com.vladmihalcea.flexypool.FlexyPoolDataSource\") @Stateless public class FlexyPoolDataSourceConfiguration {} We now need to pass the actual DataSource properties to FlexyPool and this is done through the flexy-pool.properties configuration file: flexy.pool.data.source.unique.name=unique-name flexy.pool.data.source.class.name=org.hsqldb.jdbc.JDBCDataSource flexy.pool.data.source.property.user=sa flexy.pool.data.source.property.password= flexy.pool.data.source.property.url=jdbc:hsqldb:mem:test flexy.pool.metrics.reporter.jmx.auto.start=true The actual DataSource is going to be created by the FlexyPoolDataSource on start-up. Locating the actual DataSource from JNDI If the actual DataSource is already configured by the Application Server, we can instruct FlexyPool to fetch it from JNDI. Let's say we have the following DataSource configuration: @DataSourceDefinition( name = \"java:global/jdbc/default\", className = \"org.hsqldb.jdbc.JDBCDataSource\", url = \"jdbc:hsqldb:mem:test\", initialPoolSize = 3, maxPoolSize = 5 ) @Stateless public class DefaultDataSourceConfiguration {} To proxy the JNDI DataSource, we need to configure FlexyPool like this: flexy.pool.data.source.unique.name=unique-name flexy.pool.data.source.jndi.name=java:global/jdbc/default flexy.pool.metrics.reporter.jmx.auto.start=true The FlexyPoolDataSource is defined alongside the actual DataSource: @DataSourceDefinition( name = \"java:global/jdbc/flexypool\", className = \"com.vladmihalcea.flexypool.FlexyPoolDataSource\") @Stateless public class FlexyPoolDataSourceConfiguration {} The JPA will have to fetch the FlexyPoolDataSource instead of the actual one: <jta-data-source>java:global/jdbc/flexypool</jta-data-source> In TomEE , because the DataSourceDefinitions are not lazily instantiated, the actual DataSource might not be available in the JNDI registry when the FlexyPoolDataSource definition is processed. For this, we need to instruct FlexyPool to dely the JNDI lookup until the DataSource is actually requested: flexy.pool.data.source.jndi.lazy.lookup=true Conclusion The last time I used Java EE was in 2008, on a project that was using Java EE 1.4 with EJB 2.1. After 7 years of using Spring exclusively, I'm pleasantly surprised by the Java EE experience. Arquillian is definitely my favourite add-on, since integration testing is of paramount importance in enterprise applications. CDI is both easy and powerful and I'm glad the dependency injection got standardised. But the best asset of the Java EE platform is the community itself. Java EE has very strong community, willing to give you a hand when in need. I'd like to thank Steve Millidge (Founder of Payara and C2B2) for giving me some great tips on designing the FlexyPool Java EE integration, Alex Soto , Antonio Goncalves and all the other Java EE members whom I had some very interesting conversations on Twitter."},{"title":"Better and Fewer Suppliers (2015 Software Supply Chain Report)","tags":"devops","url":"http://ciandcd.github.io/better-and-fewer-suppliers-2015-software-supply-chain-report.html","text":"from:http://devops.com/2015/06/19/better-fewer-suppliers-2015-software-supply-chain-report/ That Supplier is Better For You Since releasing the 2015 State of the Software Supply Chain Report, there has been a lot of great discussion across the industry on best practices for managing the complexity introduced by the volume and velocity of the components used across your software supply chain. Today I want to focus on the huge ecosystem of open source projects (\"suppliers\") that feed a steady stream of innovative components into our software supply chains. In the Java ecosystem alone, there are now over 108,000 suppliers of open source components. Across all component types available to developers (e.g., RubyGems, NuGet, npm, Bower, PyPI, etc.), estimates now reach over 650,000 suppliers of open source projects. However, like in traditional manufacturing, not all suppliers deliver parts of comparable quality and integrity. My latest research, the 2015 State of the Software Supply Chain Report , shows that some open source projects use restrictive licenses and vulnerable sub-components, while other projects are far more diligent at updating the overall quality of their components. Choosing the best and fewest suppliers can improve the quality and integrity of the applications we deliver to our customers. While I am hosting a webinar next week to share many of the detailed report findings, I wanted to share a few of the more meaningful stats here. Your 7,600 Suppliers My research for the report revealed many new perspectives on \"suppliers\" across the software supply chains. First of all, I saw that the average large development organization consumed over 240,000 open source components last year — sourced from over 7,600 open source projects. On the surface, the huge reliance on open source projects is a great thing. Development teams have chosen to not write those pieces themselves, but have sourced the needed components from outside suppliers. This practice speeds development, enables more innovation, and ensures time-to-release goals are achieved. The use of open source is so prolific today, few of us could ever imagine reducing the use of those components and their suppliers in the future. At the same time that we benefit from open source, our high paced, high volume consumption practices don't allow us the time needed to do the due diligence on the suppliers or open source projects where we source our component parts from. For example, of the 240,000 average component downloads in 2014, the same businesses sourced an average of 15,000 components that included known security vulnerabilities. In many cases, developers were downloading vulnerable component versions, when safer versions of those same components were available from the open source projects. While no one intends to download components with known vulnerabilities, the problem is exacerbated due to the lack the visibility into a better recommended version. Fewer Suppliers, Less Context Switching Choosing an open source project supplier should be considered an important strategic decision in organizations because changing a supplier (\"open source project\") used is far more effort than swapping out a specific component. Like traditional suppliers, open source projects have good and bad practices impacting the overall quality of their component parts. Traditional manufacturing supply chains intentionally select specific parts from approved suppliers. They also rely on formalized sourcing and procurement practices. This practice also focuses the organization on using the best and fewest suppliers — an effort that improves quality, reduces context switching, and also accelerates mean time to repair when defects are discovered. One industry example from the report describes how Toyota manages 125 suppliers for their Prius to help sustain competitive advantages over GM who manages over 800 suppliers for the Chevy Volt. By contrast, development teams working with software supply chains often rely on an unchecked variety of supply, where each developer or development team can make their own sourcing and procurement decisions. The effort of managing over 7,600 suppliers introduces a drag on development and is contrary to their need to develop faster as part of agile, continuous delivery and devops practices. Coming to Terms When you come to terms with the volume of consumption and the massive ecosystem of suppliers you can source your components from, you quickly realize it is impossible to address this issue with a manual review process. Any organizations clutching to these outdated manual practices are will continue to be outgunned by the velocity by their software supply chains. Just as traditional manufacturing supply chains have turned to automation, software development teams need to take the same approach by further automating their software supply chains. Information about suppliers and the quality of their projects needs to be made available to developers at the time they are selecting components. Information about the latest versions, features, licenses, known vulnerabilities, popularity of versions being used, and the cadence of new releases should be made available to developers in an automated way. Automating the availability of this information about suppliers can lead to better and fewer suppliers being used. Be sure to read the full 2015 State of the Software Supply Chain Report for more information about open source suppliers and organizations sourcing practices. The report also highlights current and best practices being used in organizations that are managing their use of suppliers that feed their software supply chains. To hear more about the overall report findings and industry best practices, please join me on Wednesday, June 24th (1pm ET) for our webinar ."},{"title":"Internap's DevOps Culture: PrivateStack + CD = ? [Read On & Draw Your Own Conclusions]","tags":"devops","url":"http://ciandcd.github.io/internaps-devops-culture-privatestack-cd-read-on-draw-your-own-conclusions.html","text":"from:http://devops.com/2015/06/19/internaps-devops-culture-privatestack-cd-read-draw-conclusions/ Engineers at Internap , a hosting company and public cloud vendor designed a new OpenStack-based cloud platform and development environment. DevOps.com tells the story of Internap's DevOps cultural evolution, which grew virally, interwoven with the company's development of PrivateStack. Challenges in Internap's Common Development Environment Challenges that slowed Internap's cloud development process triggered a hunger for change and for a new / altered development scheme. \"The Internap public cloud product is essentially composed of micro services that make cloud resources available to our customers,\" says Mathieu Mitchell, Senior Software Developer, Internap. Internap engineering teams tested their cloud services simultaneously with each affecting the other and transferring adverse effects to production or pre-production. Internap needed to let the billing team test their services, integrated with a duplicate of the production software and environment, without affecting the virtualization team. These challenges were rooted in a commonly shared development environment for all the engineering teams and team members, which introduced tedium for engineers as they worked to build and deploy stable code. When sharing a common development environment, each engineer's tests depended on environment consistency and reliability. When engineers / developers tested two changes at the same time, there was no way to tell which change introduced a regression. The most reasonable way to address this prior to PrivateStack was to push all changes into the environment and dedicate specific engineers to identify and fix the issues that occurred. The challenge with dedicating specific engineers to troubleshoot these regressions was that they had a diminished context to work with when compared to the original developer's understanding. The team that introduced a change needed feedback from the environment and to shoulder responsibility for fixing the issue. This would result in a stable codebase and the ability to write more thorough, automated tests. \"This is why we created PrivateStack–to allow us to independently test a single change, integrated with other services, in an environment that is the equivalent of a private production setup,\" says Mitchell. PrivateStack enables development teams to innovate while ensuring that changes behave correctly in production. This in turn enables CD and speeds development. DevOps Culture Leads Teams to Success As Internap developed PrivateStack, engineering observed a DevOps belief system spreading across the project and the teams. \"At first, it was only a minority among us, mostly people with an Agile background, who advocated the Continuous Delivery approach while we worked on this project. We really believe in delivering value to our customers as quickly as possible. To achieve Continuous Delivery, we needed people to understand that automating everything was the way to go,\" says Mitchell. As more engineers realized that they were the solution, they became increasingly motivated to enhance processes, create the new development environment, and use CD to develop the Internap public cloud product. Results with PrivateStack PrivateStack enables Internap teams to share the same code and system configurations while individual developers have each their own private production environment to test their software without affecting others. \"We heavily leverage virtualization to be able to recreate our environments, since buying racks and racks of hardware to make this available to all of our developers would be cost prohibitive,\" says Mitchell. With PrivateStack, Internap isolates production issues during development, keeping R&D efficient and slashing pre-production troubleshooting time. \"PrivateStack also improves our time-to-market, is much less costly, and most importantly, reduces the chance of any issues reaching the customer,\" says Mitchell. DevOps Culture Wrap Up To finish the PrivateStack project and foster a DevOps culture at the same time, Internap's internal core of CD true believers lead by example. Mitchell and his colleagues adhered to consistent principles while hearing out other team members on their concerns. This helped them to encourage the larger engineering department including billing and virtualization teams to adopt a DevOps and CD frame of mind. \"We dedicated two people to drive this initiative, along with a few others who believed in the Continuous Delivery approach but were not involved directly,\" says Mitchell. That was enough to get the job done. Now the vast majority of the engineering department is targeting CD. \"We are eager to share our PrivateStack platform with the open source community to enable other developers to run production-like environments for development in their day-to-day operations,\" says Mitchell."},{"title":"Two paths to metal devops: cloud-like API driven & cluster building","tags":"devops","url":"http://ciandcd.github.io/two-paths-to-metal-devops-cloud-like-api-driven-cluster-building.html","text":"from:http://devops.com/2015/06/19/two-paths-metal-devops-cloud-like-api-driven-cluster-building/ I've been seeing a rising interest in metal DevOps fueled by containers and scale-out data center platforms (like Hadoop, Ceph & OpenStack) that run at the metal level. While I see this is a growing general trend ( Packet , Internap , RackSpace , OpenStack Ironic , MaaS ), I'm going to stay firmly within my wheelhouse and use OpenCrowbar as my reference here. Building on the API-driven metal features of OpenCrowbar, this has translated into two paths for workloads to run on metal: 1) \"Cloudify\" the metal using APIs from tools like Chef Provision , SaltStack Libcloud , Docker Machine , Cloud Foundry BOSH . These tools have clients that target cloud APIs like OpenStack and Amazon. These same clients work against cloud are easily ported to Crowbar's APIs. Five years ago, conventional wisdom was that we'd need a universal cloud API; however, practice has shown it's not very difficult to wrap APIs in a way that does not reduce every cloud to a least common denominator. 2) DevOps deploy the workload using hand-offs to tools like Chef, Saltstack, Puppet or Ansible. This approach leverages the community scripts (Cookbooks, Modules, Playbooks) for the workload with the critical ability to create a tuned environment and inject the needed parameters directly into the scripts. A critical lesson we learned going from Crowbar v1 to v2 was for our scripts to have crisp attribute input/output boundary to avoid embedding environmental knowledge into the code. While I'm casting this in Crowbar terms, I see this approach to metal as coming into the market by force fuels by a desire for containers-on-metal and devops-on-metal. Let's look at some of the unique and shared use-cases for each approach: Metal API Both Metal Cluster Easy Cloud to Metal Migration Minimal Tool Customization Portability of DevOps Scripts Take advantage of power cycling Enables constant refresh cycles Leverage Hardware features Advanced Network topologies In either case, you have to handle bespoke (hipster word for custom) steps in the provisioning flow that are unique to the your operational needs. Our experience is that each site (even each server!) is unique in some incremental way. For example, one site may require teamed networks with VLANs while another requires flat networks with an SDN layer. These differences are not mistakes or errors : the reality of physical ops and individual operational choices mean that there are a lot of valid configurations. Rather than attempt the Sisyphean task of enforced conformity, we work to abstract differences so that they can be ignored when they are not material. In the end, the choices are not mutually exclusive. Metal APIs are often faster but harder to optimize. You can use them to get started quickly and then invest time to optimize a cluster for long term operations. The underlying physical orchestration can support both. Are you looking at getting closer to metal? Which of the options above makes the most sense to you? I'd love to hear about your use-cases, architecture and configuration requirements."},{"title":"ClusterHQ and DevOps.com survey show Containers poised for mass adoption","tags":"devops","url":"http://ciandcd.github.io/clusterhq-and-devopscom-survey-show-containers-poised-for-mass-adoption.html","text":"from:http://devops.com/2015/06/18/clusterhq-and-devops-com-survey-show-containers-poised-for-mass-adoption/ DevOps.com and ClusterHQ, conducted a survey on Container usage that shows an overwhelming majority of users have either already using, testing or investigating Container usage. With 285 respondents representing a wide range of organizations, it shows that Containers will be part of many production environments in the very near future. Currently, only 38 percent of respondents reported using containers in production environments, but that number is projected to increase 69 percent over the next 12 months as organizations find new ways to address important barriers to adoption. It verified that Docker is overwhelmingly the container of choice, with 92% of respondents having used or investigated it, followed by LXC (32%) a distant second, but still far ahead of Rocket (21%). To access the complete survey and report visit https://clusterhq.com/assets/pdfs/state-of-container-usage-june-2015.pdf . Companies ranging in size from small organizations with 1 to 500 employees (69%), to mid-size companies with 501-2,500 personnel (12%), all the way up to large enterprises with over 2,500 employees (19%) are represented in the survey. This demonstrates that containers are being embraced by all businesses from the startup stage to Fortune 500 companies. Respondents came predominantly from Development, Operations and DevOps teams. QA and security teams were a smaller share. The survey revealed how container technologies are being used today, as well as research-based insights while providing clues as to where the industry is trending. From the ClusterHQ release on the survey: The survey also revealed insights into what is perceived to be the primary barriers to container adoption. Security seems to be emerging as a consistent concern throughout the DevOps community in these times of never ending breaches throughout the world: Security — 61% Data Management — 53% Networking — 51% Skills and Knowledge — 48% Persistent Storage — 48% Data Management capabilities also emerged as essential to the success of container strategies and that the vast majority of organizations want to run databases as well as additional services in containers. When asked to rate how important data management is to container strategies, 66 percent reported it as a critical or important gating factor, 29 percent ranked it as moderately important and only 5 percent reported that it carries no importance. Over 70% of respondents said they would like to run a database or other stateful service in their container environments. Respondents were also asked which specific features of container data management they considered to be most important, selecting the \"integration of data management capabilities into existing container workflows and tools\" as their first choice, with \"seamless movement of data between dev, test and production environments\" a close second. MySQL (53%), Redis (52%), PostgreSQL (50%), and Elasticsearch (43%) were reported as the top four most frequently used stateful services. Containers have become known for portability and flexibility, the survey reveals that organizations are using them in different infrastructures but most frequently in on-premises data centers (57%), followed by Amazon Web Services (52%). So I think it is safe to say Containers are rapidly becoming one of the staples of development and are here to stay to stay in the foreseeable future. DevOps.com along with ElasticBox are currently conducting another survey on \"What is DevOps to you?\" One in 50 respondents wins a $50 dollar Amazon gift card and one grand prize winner will win a new 3DR Drone. Take a few minutes to help us with this survey ."},{"title":"DOGs at Digital 2015","tags":"devops","url":"http://ciandcd.github.io/dogs-at-digital-2015.html","text":"From: http://blog.devopsguys.com/2015/06/18/dogs-at-digital-2015/ Last week the DevOpsGuys headed up to Newport's Celtic Manor to take part in Digital 2015 – the Welsh Government's initiative to bring digital innovators and business professionals together. The 2-day event saw more than 2,000 delegates and 140 speakers. DevOpsGuy co-founder Steve Thair says: \"These initiatives are invaluable to the digital sector because they expose the wide variety of digital and technological services that are available in South Wales to business professionals who can use them to take online business services to the next level. It's a relaxed environment where people can chat and form connections that will have a direct impact on the future of business and technology in Wales.\" The diverse range of speakers at the event included Microsoft, the WRU, Amazon and the DVLA. The opportunity to discuss the needs of businesses directly with those running them is invaluable. This dialogue can lead to collaborative projects and further development of the burgeoning tech industry in the area. The team are excited to see more events like this one springing up in the near future. Look out for us at the up-coming Agile Cymru in the Wales Millennium Centre on the 7th and 8th of July."},{"title":"DOGWalking in Brecon","tags":"devops","url":"http://ciandcd.github.io/dogwalking-in-brecon.html","text":"From: http://blog.devopsguys.com/2015/06/17/dogwalking-in-brecon/ So, aching, tired and happy the DOGs returned from TrekFest 2015 in the Welsh mountains having raised £1,143.00 for the Countess Mountbatten Hospice and SSNAP – two charities close to the heart of the team. That's 114% of our initial target, so a huge thank you to everyone who donated so generously. The weather could not have been better for a long, scenic ramble in one of the UKs most beautiful spots; beautifully sunny with a refreshing breeze kept the team going. We completed the trek in approximately 5 and a half hours, just in time for a piece of cake and a glass of celebratory champagne at the finish line. Everyone had a thoroughly enjoyable time and we're all looking forward to the next DOG adventure!"},{"title":"Git Simple Feature Branch Workflow","tags":"devops","url":"http://ciandcd.github.io/git-simple-feature-branch-workflow.html","text":"from:http://java.dzone.com/articles/git-simple-feature-branch In my previous post , I wrote about git work flows. Now I will going to try out simple ' Feature Branch Workflow '. 1. I pull down the latest changes from mastergit checkout mastergit pull origin master2. I make branch to make changes git checkout -b new-feature3. Now I am working on the feature4. I keep my feature branch fresh and up to date with the latest changes in master, using 'rebase'Every once in a while during the development update the feature branch with the latest changes in master.git fetch origingit rebase origin/masterIn the case where other devs are also working on the same shared remote feature branch, also rebase changes coming from it:git rebase origin/new-featureResolving conflicts during the rebase allows me to have always clean merges at the end of the feature development.5. When I am ready I commit my changesgit add -pgit commit -m \"my changes\"6. rebasing keeps my code working, merging easy, and history clean.git fetch origingit rebase origin/new-featuregit rebase origin/masterBelow two points are optional6.1 push my branch for discussion (pull-request)git push origin new-feature6.2 feel free to rebase within my feature branch, my team can handle it!git rebase -i origin/master Few point that can be happen in developing phase. Another new feature is needed and it need some commits from my new branch 'new-feature' that new feature need new branch and few commits need to push to it and clean from my branch. 7.1 Creating x-new-feature branch on top of 'new-feature' git checkout -b x-new-feature new-feature 7.2 Cleaning commits //revert a commit git revert --no-commit //reverting few steps a back from current HEAD git reset --hard HEAD~2 7.3 Updating the git //Clean new-feature branch git push origin HEAD --force 1. I pull down the latest changes from mastergit checkout mastergit pull origin master2. I make branch to make changes git checkout -b new-feature3. Now I am working on the feature4. I keep my feature branch fresh and up to date with the latest changes in master, using 'rebase'Every once in a while during the development update the feature branch with the latest changes in master.git fetch origingit rebase origin/masterIn the case where other devs are also working on the same shared remote feature branch, also rebase changes coming from it:git rebase origin/new-featureResolving conflicts during the rebase allows me to have always clean merges at the end of the feature development.5. When I am ready I commit my changesgit add -pgit commit -m \"my changes\"6. rebasing keeps my code working, merging easy, and history clean.git fetch origingit rebase origin/new-featuregit rebase origin/masterBelow two points are optional6.1 push my branch for discussion (pull-request)git push origin new-feature6.2 feel free to rebase within my feature branch, my team can handle it!git rebase -i origin/masterAnother new feature is needed and it need some commits from my new branch 'new-feature' that new feature need new branch and few commits need to push to it and clean from my branch.7.1 Creating x-new-feature branch on top of 'new-feature'git checkout -b x-new-feature new-feature7.2 Cleaning commits//revert a commitgit revert --no-commit//reverting few steps a back from current HEADgit reset --hard HEAD~27.3 Updating the git//Clean new-feature branchgit push origin HEAD --force"},{"title":"Know Thy MVN Plugins: Keeping One's Sanity Amidst Open Source Version Hell","tags":"devops","url":"http://ciandcd.github.io/know-thy-mvn-plugins-keeping-ones-sanity-amidst-open-source-version-hell.html","text":"from:http://java.dzone.com/articles/know-thy-mvn-plugins-or Problem #1: Everyone knows that keeping up with versions is tough. This is the reason tools such as OpenLogic come to exist. Some companies pay, some companies develop home grown solutions, some live with the version which are getting older every day. Problem #2: When multiple open source projects once consumed by your product can bring in different versions of the same library. One never knows which will be picked up at run-time and behavior on the developer's box based on the Murphy's Law will be different from the server run-time. This does not have to be such an ordeal. With the power of maven plugins this can be solved relatively easy. Versions Maven Plugin and Maven Enforcer Plugin to the rescue! Versions Maven Plugin will keep versions up-to-date and as you probably guessed from the name Maven Enforcer plugin will be guarding against multiple versions of the maven artifact in the build package produced. Let's see how one introduced enforcer into the mix first. The code below can go in the parent pom.xml of the project, or global parent of the projects if one exists. <project …> … <plugins> … <plugin> <groupId>org.apache.maven.plugins</groupId> <artifactId>maven-enforcer-plugin</artifactId> <version>1.4</version> <executions> <execution> <id>enforce-versions</id> <goals> <goal>enforce</goal> </goals> <configuration> <rules> <requireMavenVersion> <version>[2.2.*,)</version> </requireMavenVersion> <requireJavaVersion> <version>[1.7.*,)</version> </requireJavaVersion> <DependencyConvergence/> </rules> </configuration> </execution> </executions> </plugin> … </plugins> … </project> For each dependency version collision one has to pick the version to keep and exclude the ones that are mismatched. See maven help page for details. If one wants absolute guarantee of the version used, this is the only way to go. An example of excluded dependencies will be: <project …> … <dependencies> … <dependency> <groupId>com.lordofthejars</groupId> <artifactId>nosqlunit-mongodb</artifactId> <version>${nosqlunit.veresion}</version> <scope>test</scope> <exclusions> <exclusion> <groupId>org.slf4j</groupId> <artifactId>slf4j-api</artifactId> </exclusion> <exclusion> <groupId>com.github.fakemongo</groupId> <artifactId>fongo</artifactId> </exclusion> <exclusion> <groupId>org.mongodb</groupId> <artifactId>mongo-java-driver</artifactId> </exclusion> </exclusions> </dependency> … </dependencies> … </project> From the open source developer side , producing two flavors of the package for consumption with and without dependencies can make world a better place. This is the difference of maven scope ‘provided' vs. default scope ‘compile'. Apache Maven Shade Plugin can be used to produce the version with the dependencies to be used there no conflicts can arise along with the version that is artifact version independent and can be used without version conflicts. Once all conflicts are resolved and one version of each artifact is in the final build product guaranteed, one should check if ones project is up to date. It is a good practice to do the check at the beginning of the new version. To check for outdates dependencies, and plugins, and bring those up to date in the controlled manner (manually) one can execute: mvn versions:display-dependency-updates mvn versions:display-plugin-updates mvn versions:display-property-updates As an alternative one can let the plugin update the versions by executing following mvn versions:use-latest-releases mvn versions:update-properties This last mvn command I am going to share is a bonus for the dedicated reader. It allows changing versions across the project and its modules painlessly: mvn versions:set -DgenerateBackupPoms=false​ -DnewVersion=<version>"},{"title":"The Ops Mgr at QCon 2015","tags":"devops","url":"http://ciandcd.github.io/the-ops-mgr-at-qcon-2015.html","text":"From: http://blog.devopsguys.com/2015/06/16/the-ops-mgr-at-qcon-2015/ \"When you're in a startup, the divide between Dev and Ops is normally the width of the desk…it's far easier to collaborate in that small environment. In larger enterprises not only are they in different buildings but they're in different countries with different cultures and different languages…\" The DOG Ops Manager Steve Thair chats to Manuel Pais at QCon 2015. Steve Thair at QCon to hear Steve talk about Enterprise DevOps; taking the first steps on the road to DevOps and cultural change."},{"title":"Steve Thair's QCon Talk – now available online","tags":"devops","url":"http://ciandcd.github.io/steve-thairs-qcon-talk-now-available-online.html","text":"From: http://blog.devopsguys.com/2015/05/20/steve-thairs-qcon-talk-now-available-online/ DevOps and the Need for Speed, the talk from our very own Steve Thair is now available online. You can check it out here . Steve's just spoken at Krakow's Atmosphere Conference . Stay tuned for more, coming soon."},{"title":"Increase your ELK herd with Consul.io","tags":"devops","url":"http://ciandcd.github.io/increase-your-elk-herd-with-consulio.html","text":"From: http://blog.devopsguys.com/2015/05/18/increase-your-elk-herd-with-consul-io/ Originally posted on DevOps Is Common Sense... : At work, I recently had a need to put in place a scalable logging solution based around the ELK stack. Issues with Multicast networking aside, Elasticsearch scales pretty well on its own without the need for any additional overheads, however discovering whether a node is online or not and connecting only to available nodes can be tricky. Scaling Logstash can be tricky, but it basically involves adding more Logstash servers to the mix and pointing them at your Elasticsearch cluster by defining multiple hosts in your Logstash configuration. Kibana (like most web applications) can only have one Elasticsearch host defined in the config, so scaling out Kibana is more difficult. The above raises the question – how do I know which Elasticsearch node to point my configuration at if I don't know whether they are there or not. The answer came in the form of consul.io . If you've not looked at…"},{"title":"DevOpsGuys announce RedGate partnership","tags":"devops","url":"http://ciandcd.github.io/devopsguys-announce-redgate-partnership.html","text":"From: http://blog.devopsguys.com/2015/05/11/devopsguys-partner-redgate/ More and more companies are now considering source control, continuous integration, and automated deployment for their database. To help them adopt each of these stages of Database Lifecycle Management (DLM), Redgate Software has launched a new partner program. Redgate Certified Consultants are now being trained in the USA, Europe and Australia – and many of them will be familiar to SQL Server professionals. The advantages of implementing any stage of DLM are many. Just as with Application Lifecycle Management (ALM), it speeds up the introduction of new features, and makes deployments reliable and error-free. But even though Redgate tools are designed to plug into the tools companies already use for their application d evelopment, questions can arise during the implementation process. As Dan Wood of Northwest Cadence says: \"How can we take a system designed to develop, build and deliver applications and make it work with databases as well? It is a question that has plagued a vast majority of the clients I have worked with over the past few years.\" To address this issue, Redgate's new partner program is training expert consultants like Dan Wood in DLM – and giving them the tools and support they need to help clients on-site, or in training sessions. The list of Certified Consultants is growing and already includes familiar faces like Ike Ellis and Northwest Cadence in the USA, The DevOpsGuys and Skelton Thatcher in the UK, and WARDY IT Solutions in Australia. As John Theron of Redgate points out, the advantages are clear. \"Redgate has spent a lot of time and effort joining the dots in DLM, and making it possible with a suite of dedicated tools, alongside learning materials and resources. The partner program complements this with a group of experts on the ground able to help companies on-site, and provide training in a series of public workshops.\" In places as far apart as Washington, London, San Diego, Philadelphia, Northern Ireland, and Baton Rouge, database professionals are how being trained in source control, continuous integration, and automated deployment for the database. A measure of the success the training is already achieving can be found in the reaction from database professionals like Jim Dorame. A Database Systems Manager for a large scale educational assessment corporation in the Greater Minneapolis area, he reviewed a continuous integration training day on his blog . \"This tool makes the job of the DBA easier as there will be little doubt that the database is in a consistent and correct state. This alone makes me smile, I cannot tell you how many times I've been executing a release and there was a piece missing that caused a failure.\" Further information about the training opportunities available can be found on the Redgate training pages ."},{"title":"Sponsored DOG Walk","tags":"devops","url":"http://ciandcd.github.io/sponsored-dog-walk.html","text":"From: http://blog.devopsguys.com/2015/05/08/sponsored-dog-walk/ The DevOpsGuys team are pulling up our hiking socks to raise cash for SSNAP and Countess Mountbatten Hospice this summer with a 13 mile walk across Wales. TrekFest it's no mean feat. We cross the highest peaks in the Beacons and South Wales including Pen y Fan (886m), Corn Du (873m), Cribyn (795m) and Fan y Big (719m). http://www.trekfest.org.uk/ Click here to donate now through our Just Giving Page Our first charity is special since last year, one of our brave team members was diagnosed with terminal cancer. Unfortunately, and with great sadness we know now they are losing their fight – even after enduring endless rounds of chemotherapy and surgery. The amazing staff at Countess Mountbatten Hospice are proving specialist palliative (end of life) care to many fighting a losing battles against advanced stage cancer. They also support their families and loved ones. We'd love to show our thanks and support by raising money on their behalf. It's with a tear in our eye and sadness in our hearts, that we chose our second charity to support. Last year, little Ollie lost his fight for his life after being born with a heart defect. He was 4 days old. His parents have displayed so much courage during an immensely difficult time during which the amazing doctors and nurses, support by the SSNAP team provided them with much needed support. It is their wish to continue to champion SSNAP, who provide support for the sick newborns and their parents at new born intensive care unit at The John Radcliffe Hospital, Oxford as so, we'll match whatever we raise through our JustGiving page as a donation to SSNAP. These amazing charities are truly special to us here DevOpsGuys. Please support us in raising funds for these brilliant charities who have done so much to support our close friends and employees. The team have signed up to complete the distance in six hours. The trek takes place in the Brecon Beacons and covers the highest peaks in South Wales: Pen y Fan, Corn Du and Fan y Big. The Beacons are a training ground for the SAS so, while the DOGs will have their work cut out for them, they're more than up for the challenge: \"I can't wait to get out there\" says office manager Rhian Owen. \"It's such a great opportunity to work together as a team to achieve personal goals and to raise money for good causes – it's going to be brilliant!\" We've set up a Just Giving page, so you can show your support here. It's a chance to donate to some good causes and get the DevOpsGuys and gals out from behind their screens and into the beautiful Welsh wilderness – come on y'all, dig deep! Click here to donate now through our Just Giving Page"},{"title":"DevOps and the Digital Supply Chain","tags":"devops","url":"http://ciandcd.github.io/devops-and-the-digital-supply-chain.html","text":"From: http://blog.devopsguys.com/2015/05/06/devops-and-the-digital-supply-chain/ What is the \"Digital Supply Chain\" and why is it important to your organisation and to DevOps as a practice? The concept of the \"Digital Supply Chain\" is a different way of looking at the SDLC and the Continuous Delivery \"pipeline\" that we feel makes it easier for traditional organisations to understand the criticality of software delivery (and by extension DevOps) in the modern world. Any organisation that deals with physical goods understands the concept of the supply chain . They are intimately familiar with ideas like supply chain management , supply chain optimisation and, most importantly, they understand the economics of inventory in the supply chain e.g the carrying cost of inventory . So what is the \"Digital Supply Chain\"? The current definitions of the digital supply chain are anchored in the \"New Media\" sector and focus on digital assets like music, video etc \"The \" digital supply chain \" is a \" new media \" term which encompasses the process of the delivery of digital media, be it music or video, by electronic means, from the point of origin (content provider) to destination (consumer).\" – Wikipedia The Wikipedia article references above breaks it down into a number of discrete steps as shown in Figure 1 below. If we contrast this with our SDLC Continuous Delivery Pipeline (Figure 2) we can see that many of the steps are directly analogous – we are creating digital assets (code) which we then \"compress\" (i.e. the Build/Integrate process), which we then subject to Quality Control (Test), we store in a Digital Asset Management system (e.g. like Nexus or Artifactory), we tag it with metadata (e.g. what release/version we're deploying) and when then deploy it out to servers, CDN's, the AppStore or wherever. Once your customers grasp the idea that software is a digital asset and that carrying excess inventory and delays in moving these digital assets along the supply chain is costing them money it can be a lightbulb moment for many organisations. Software assets can depreciate over time. Indeed \" technical debt \" can be looked at as the \"cost of deprecation\" of your software assets over time. Code that is \"stuck\" in your Digital Supply Chain waiting for your next release (as source code in Git, or as artefacts in an artefact repository) represents a capital investment in \"digital assets\" held as \"digital inventory\" and having it sat on the digital shelf in your digital warehouse is costing you money is analogous to the carrying cost of inventory for physical inventory. Sure, the warehousing costs of a digital asset – your latest idea transformed into software code – is fairly trivial compared to the costs of physical warehousing BUT the \" opportunity cost \" is very real. Each digital software asset represents a significant investment in time & money by your designers, developers, testers, project managers etc and it doesn't start generating a return on that investment until it gets to the end of your digital supply chain and into the hands of your customers. DevOps then becomes a way to optimise your digital supply chain to ensure that we: only build the right things (reducing waste and optimising our digital inventory), Supplier management (by improving the relationships between Dev, Test, Ops etc we ensure that we are getting the best from all of the \"suppliers\" in our digital supply chain) improving our logistics to get our digital assets in the hand of our customers (by automating testing, release and deployment to accelerate the movement of the digital assets from left to right) Constantly seeking \"flow\" across the supply chain (the 1st way of DevOps!) Gathering metrics along the supply chain to give us insight into the bottlenecks (the M in the C.A.L.M.S model) So next time you're talking with people in the business try out the \"Digital Supply Chain\" analogy and see if it works for you – we'd love to hear your feedback! -TheOpsMgr"},{"title":"DevOpsGuys at RedGate","tags":"devops","url":"http://ciandcd.github.io/devopsguys-at-redgate.html","text":"from:http://blog.devopsguys.com/2015/05/01/devopsguys-at-redgate/ The DevOpsGuys headed off on a road trip this week to meet with the RedGate team at their amazing offices in Cambridge. As well as working on some workshop training opportunities and guest blog articles (stay tuned to the DevOpsGuys Blog for some RedGate articles coming soon) the teams got together to brainstorm ideas and share skills. We were able to look at some of their newest tools and we're excited to announce that we will be delivering workshops on RedGate DLM tools at various sessions across the country this summer. We've already implemented these tools for a many of our customers and we're delighted to be able to introduce their qualities, in detail, to a wide range of industry professionals as part of an effective, independent DevOps adoption process. The workshops will be running on: May 20 – Automated Database Deployment, London June 26 – Automated Database Deployment, Belfast July 8 – Database Source Control, London July 24 – Database Source Control, Manchester August 20 – Database Continuous Integration, Cardiff Spaces are limited, so register now to take part in a workshop or request a workshop near you."},{"title":"Enterprise COBOL 5.1—Where Tradition Meets Innovation (Mainframe Systems Edition, May 2013)","tags":"devops","url":"http://ciandcd.github.io/enterprise-cobol-51-where-tradition-meets-innovation-mainframe-systems-edition-may-2013.html","text":"From: http://devops.linuxjournal.com/develop-deploy/enterprise-cobol-51%E2%80%94where-tradition-meets-innovation-mainframe-systems-edition-may COBOL is the most ubiquitous programming language for developing business applications. Despite that its specification was created more than 50 years ago, COBOL is still running the world's most critical business applications. Many of us might not realize how much we rely on COBOL applications in our day-to-day lives. Every time we use an ATM, book an airline ticket, process a check or make an insurance claim, we're using a COBOL application to process the transaction, and there's a good chance this application is running on System z."},{"title":"Info-Tech Research Group; Software Test Management Vendor Landscape","tags":"devops","url":"http://ciandcd.github.io/info-tech-research-group-software-test-management-vendor-landscape.html","text":"From: http://devops.linuxjournal.com/develop-deploy/info-tech-research-group-software-test-management-vendor-landscape Info-Tech Research Group Vendor Landscape reports recognize outstanding vendors in the technology marketplace. Assessing vendors by the strength of their offering and their strategy for the enterprise, Info-Tech Research Group Vendor Landscapes pay tribute to the contribution of exceptional vendors in a particular category. For more, download the report today."},{"title":"Use service virtualization to remove testing bottlenecks","tags":"devops","url":"http://ciandcd.github.io/use-service-virtualization-to-remove-testing-bottlenecks.html","text":"From: http://devops.linuxjournal.com/develop-deploy/use-service-virtualization-remove-testing-bottlenecks Discover integration faults early by pushing integration testing left in the software lifecycle. Read this paper to learn how teams can increase agility and test complex interconnected applications earlier in the development process using service virtualization. Shift testing left and get to \"Done Done Done\" in a single iteration reducing project risk and deploying higher quality software faster through continuous integration testing."},{"title":"Info-Tech Vendor Landscape: Application Lifecycle Management 2015 Report","tags":"devops","url":"http://ciandcd.github.io/info-tech-vendor-landscape-application-lifecycle-management-2015-report.html","text":"From: http://devops.linuxjournal.com/collaborative-development/info-tech-vendor-landscape-application-lifecycle-management-2015-report Info-Tech evaluated 14 competitors in the ALM market and IBM was chosen as the champion (see Fig. 1) and the only vendor to offer out of the box, integrated and to the full feature definition all ten evaluated features. Read why Info-Tech recommends \"organizations seeking a premium product to manage complex requirements should add IBM's line of Rational products to their shortlist.\""},{"title":"Scaled Agile Information Kit","tags":"devops","url":"http://ciandcd.github.io/scaled-agile-information-kit.html","text":"From: http://devops.linuxjournal.com/collaborative-development/scaled-agile-information-kit Scaled agile practices deliver on the promise of scaling development methods to the enterprise through a unified approach and addressing the requirements of complex constructs and additional stakeholders of today's organizations. Just ask Nationwide Insurance, which is leveraging scaled agile practices. Learn more about scaled agile and Nationwide's success, and you'll discover how to: -- Utilize a foundational framework and public knowledge base of proven lean and agile practices at enterprise scale -- Reproduce benefits achieved by four real-world companies -- Follow Nationwide's example in slashing downtime and dramatically improving code quality in just three years"},{"title":"Five vital steps for successful software delivery in a chaotic world","tags":"devops","url":"http://ciandcd.github.io/five-vital-steps-for-successful-software-delivery-in-a-chaotic-world.html","text":"From: http://devops.linuxjournal.com/collaborative-development/five-vital-steps-successful-software-delivery-chaotic-world Delivering applications that meet the needs of the business can be a challenge in a complex business climate that is constantly changing. Diverse, multiple heterogeneous environments are the norm, all of which must be maintained and deployed. Multiple tools and technologies connect, support and create work. Processes are inconsistent, which contributes to difficulties with end-to-end lifecycle governance. Waste, rework and technical debt abound. At the same time, organizations are striving to be leaner by eliminating tasks that do not add value and by preserving existing IT investments. Woven throughout almost every one of these challenges is the need for speed and innovation while balancing quality and cost. If you have found yourself in this complex, chaotic world, what should you do about it? What improvements should you consider? What practices should you have in place? This paper covers five steps that can help you address today's challenges and deliver software that yields better business results."},{"title":"Managing the software supply chain","tags":"devops","url":"http://ciandcd.github.io/managing-the-software-supply-chain.html","text":"From: http://devops.linuxjournal.com/collaborative-development/managing-software-supply-chain An approach to governance can help you manage your software supply chain. This approach has three imperatives: balance governance with agility, increase visibility and deliver measurable business value."},{"title":"RDz Lunch and Learn Series Session 1: Streamline and Simplify Application Maintenance Using RDz","tags":"devops","url":"http://ciandcd.github.io/rdz-lunch-and-learn-series-session-1-streamline-and-simplify-application-maintenance-using-rdz.html","text":"From: http://devops.linuxjournal.com/collaborative-development/rdz-lunch-and-learn-series-session-1-streamline-and-simplify-application Join Jonathan Sayles for this complimentary presentation where you will learn how to tackle large, complex application analysis tasks using Rational Developer for System z, and see how cutting edge tools provide you with deep, precise answers to bewildering z/OS application analysis questions. With a single click learn how spaghetti coded programs are structured, how control ended up in specific paragraphs or sections (when you were ready to bet your mortgage they couldn't), how data values circulate among variables, files, databases and relational tables (at a click) - and which statements read vs. update fields. Further? You will learn how RDz's Integrated Debugger can provide windows into your program's run-time behavior that make testing and debugging your maintenance changes as simple as possible."},{"title":"Forrester Study: The Total Economic Impact for SV. & Auto.","tags":"devops","url":"http://ciandcd.github.io/forrester-study-the-total-economic-impact-for-sv-auto.html","text":"From: http://devops.linuxjournal.com/continuous-testing/forrester-study-total-economic-impact-sv-auto Forrester Research has just completed a Total Economic Impact study for a large financial services institution to quantify the actual return on investment for service virtualization. Read Forrester's analysis to learn how service virtualization is eliminating testing bottlenecks and significantly reducing the cost of testing."},{"title":"Info-Tech Quality Management and Test Champion Report","tags":"devops","url":"http://ciandcd.github.io/info-tech-quality-management-and-test-champion-report.html","text":"From: http://devops.linuxjournal.com/continuous-testing/info-tech-quality-management-and-test-champion-report Info-Tech Research Group Vendor Landscape reports recognize outstanding vendors in the technology marketplace. Assessing vendors by the strength of their offering and their strategy for the enterprise, Info-Tech Research Group Vendor Landscapes pay tribute to the contribution of exceptional vendors in a particular category. For more, download the report today."},{"title":"A Rational approach to integ. testing","tags":"devops","url":"http://ciandcd.github.io/a-rational-approach-to-integ-testing.html","text":"From: http://devops.linuxjournal.com/develop-deploy/rational-approach-integ-testing In this challenging environment, a combination of automated integration testing and test virtualization can enable test teams to improve software quality and keep up with the rate of change. This white paper helps address these needs by describing the benefits that can be gained through a proactive and continuous approach to integration testing with IBM® Rational® test automation solutions."},{"title":"Service Virtualization For Dummies","tags":"devops","url":"http://ciandcd.github.io/service-virtualization-for-dummies.html","text":"From: http://devops.linuxjournal.com/develop-deploy/service-virtualization-dummies Discover service virtualization and how it fits into the big picture of software quality. In this book, Service Virtualization For Dummies, IBM Limited Edition written by industry analysts Marcia Kaufman and Judith Hurwitz, learn how to deliver higher quality software by increase the efficiency and effectiveness of your testing processes while reducing testing downtime and testing cost. The book covers the following topics and more: - Understand the changing relationship between IT and business — quicker delivery is required - Examine today's complex applications — mobile, web, social meet middleware, packaged apps, databases, and mainframes. - Discover how to begin with service virtualization — build your business case. - Realize the benefits of service virtualization —enable earlier testing to eliminate surprises and reduce risk."},{"title":"Service Virtualization for Dummies Book","tags":"devops","url":"http://ciandcd.github.io/service-virtualization-for-dummies-book.html","text":"From: http://devops.linuxjournal.com/continuous-testing/service-virtualization-dummies-book Discover service virtualization and how it fits into the big picture of software quality. In this book, Service Virtualization For Dummies, IBM Limited Edition written by industry analysts Marcia Kaufman and Judith Hurwitz, learn how to deliver higher quality software by increase the efficiency and effectiveness of your testing processes while reducing testing downtime and testing cost."},{"title":"The Forrester Wave™: Service Virtualization And Testing Solutions, Q1, 2014","tags":"devops","url":"http://ciandcd.github.io/the-forrester-wavetm-service-virtualization-and-testing-solutions-q1-2014.html","text":"From: http://devops.linuxjournal.com/continuous-testing/forrester-wave%E2%84%A2-service-virtualization-and-testing-solutions-q1-2014 Service virtualization and testing (SVT) solutions provide developers and testers with tools to quickly simulate the services of a complex production environment, mainly for automating regression, integration, and performance tests. Read Forrester's analysis of the leading providers in this space, and learn how this technology is improving continuous testing and helping companies achieve a significant return on their investment."},{"title":"An Insider's Guide to Agile Testing and Service Virtualization","tags":"devops","url":"http://ciandcd.github.io/an-insiders-guide-to-agile-testing-and-service-virtualization.html","text":"From: http://devops.linuxjournal.com/continuous-testing/insider%E2%80%99s-guide-agile-testing-and-service-virtualization Get the essentials about agile testing and service virtualization from several different perspectives – get an analyst's take from Diego Lo Giudice of Forrester Research on how to remove agile testing bottlenecks and ways to calculate your potential return on investment for Service Virtualization. Hear about real customer implementations in the financial services, travel, and healthcare sectors. Learn from experts on how service virtualization is essential for mobile development, testing packaged applications (such as SAP), and more."},{"title":"Visualizations of Continuous Delivery","tags":"ciandcd","url":"http://ciandcd.github.io/visualizations-of-continuous-delivery.html","text":"From: http://continuousdelivery.com/2014/02/visualizations-of-continuous-delivery/ Visualizations of Continuous Delivery Nhan Ngo , a QA engineer at Spotify , made four fabulous visualizations while reading Continuous Delivery . She has very kindly agreed to make them available under a Creative Commons license so feel free to share them, download them, and print them out (click to get a higher resolution version). Thank you Nhan!"},{"title":"The Science Behind the 2013 Puppet Labs DevOps Survey Of Practice","tags":"ciandcd","url":"http://ciandcd.github.io/the-science-behind-the-2013-puppet-labs-devops-survey-of-practice.html","text":"From: http://continuousdelivery.com/2013/12/the-science-behind-the-2013-puppet-labs-devops-survey-of-practice/ The Science Behind the 2013 Puppet Labs DevOps Survey Of Practice By Gene Kim and Jez Humble Last year, we both had the privilege of working with Puppet Labs to develop the 2012 DevOps Survey Of Practice. It was especially exciting for Gene, because we were able to benchmark the performance of over 4000 IT organizations, and to gain an understanding what behaviors result in their incredible performance. This continues research that he has been doing of high performing IT organizations that started for him in 1999. In this blog post, Gene Kim and I will discuss the research hypotheses that we're setting out to test in the 2013 DevOps Survey Of Practice, explain the mechanics of how these types of cross-population studies actually work (so you help this research effort or even start your own), then describe the key findings that came out of the 2012 study. But first off, if you're even remotely interested in DevOps, go take the 2013 Puppet Labs DevOps Survey here ! The survey closes on January 15, 2014, so hurry! It only takes about ten minutes. 2013 DevOps Survey Research Goals Last year's study (which we'll describe in more detail below) found that high performing organizations that were employing DevOps practices were massively outperforming their peers: they were doing 30x more frequent code deploys, and had deployment lead times measured in minutes or hours (versus lower performers, who required weeks, months or quarters to complete their deployments). The high performers also had far better deployment outcomes: their changes and deployments had twice the change success rates, and when the changes failed, they could restore service 12x faster. The goal of the 2013 study is to gain a better understanding of exactly what practices are required to achieve this high performance. Our hypothesis is that the following are required, and we'll be looking to independently evaluate the effect of each of these practices on performance: small teams with high trust that span the entire value stream: Dev, QA, IT Operations and Infosec shared goals and shared pain that span the entire value stream small development batch sizes presence of continuous, automated integration and testing emphasis on creating a culture of learning, experimentation and innovation emphasis on creating resilient systems We are also testing two other hypotheses that one of us (Gene) is especially excited about, because it's something he's wanted to do ever since 1999! Lead time : In plant manufacturing, lead time is the time required to turn raw materials into finished goods. There is a deeply held belief in the Lean community that lead time is the single best predictor of quality, customer satisfaction and employee happiness. We are testing this hypothesis for the DevOps value stream in the 2013 survey instrument. Organizational performance : Last year, we confirmed that DevOps practices correlate with substantially improved IT performance (e.g., deploy frequencies, lead times, change success rates, MTTR). This year, we will be testing whether improved IT performance correlates with improved business performance. In this year's study, we've added inserted three questions that are known to correlate with organizational performance, which is known to correlate with business performance (e.g., competitiveness in the marketplace, return on assets, etc.). Our dream headline would be, \"high performing organizations not only do 30x more frequent code deployments than their peers, but they also outperform the S&P 500 by 3x as measured by shareholder return and return on assets.\" Obviously, there are many other variables that contribute to business performance besides Dev and Ops performance (e.g., profitability, market segment, market share, etc.). However, in our minds, the reliance upon IT performance is obvious: as Chris Little said, \"Every organization is an IT business, regardless of what business they think they're in.\" When IT does poorly, the business will do poorly. And when IT helps the organization win, those organizations will out-perform their competitors in the marketplace. (This hypothesis forms the basis of the hedge fund that Erik wants to create in the last chapter of \"The Phoenix Project: A Novel About IT, DevOps, and Helping Your Business Win\" , where they would make long or short bets, based on the known operating characteristics of the IT organization.) The Theory Behind Cross-Population Studies and Survey Instruments Like last year, this year's DevOps survey is a cross-population study, designed to explore the link between organizational performance and organizational practices and cultural norms. What is a cross-population study? It's a statistical research technique designed to uncover what factors (e.g., practices, cultural norms, etc.) correlate with outcomes (e.g., IT performance). Cross-population studies are often used in medical research to answer questions like, \"is cigarette smoking a significant factor in early mortality?\" Properly designed cross-population studies are considered a much more rigorous approach of testing efficacy of what practices work than say, interviewing people about what they think worked, ROI stories from vendors, or collecting \"known, best practices.\" When doing survey design, we might state our hypotheses in the following form: \"we believe that IT organizations which have high trust have higher IT performance.\" In other words, the higher the trust levels in the IT organization, the higher the performance. We then put this question in the survey instrument, and then analyze the results. If we were to plot the results on a graph, we would put the dependent variable (i.e., performance) on the Y-axis, and the independent variable (i.e., presence of high trust) on the X-axis. We would then test to see if there is a correlation between the two. Shown below is an example of what it looks like when the two variables have low or no correlation, and one that has a significant positive correlation. If we were to find a significant correlation, such as displayed on the right, we could then assert that \"the higher your organization's trust levels, in general, the higher your IT performance.\" (Graph adapted from Wikipedia entry on Correlation and Dependence .) The 2012 DevOps Survey In this section, we will describe the the key findings that came out of the 2012 DevOps Survey, as well as a brief discussion of the research hypotheses that went into the survey design. In the DevOps community, we have long asserted that certain practices enables organizations simultaneously deliver fast flow of features to market, while providing world-class stability, reliability and security. We designed the survey to validate this, and tested a series of technical practices to determine which of them correlated with high performance. The survey ran for 30 days, and we had 4,039 completed respondents. (This is an astonishingly high number, by the way. When Kurt Milne and Gene Kim did similar studies in 2006, each study typically required $200K to do the survey design, gather responses from a couple hundred people, and then perform survey analysis.) You can find the slides that Gene Kim, Jez Humble and James Turnbull presented at the 2013 Velocity Conference here , and the full Puppet Labs infographics and results here . The first surprise was how much the high performing organizations were outperforming their non-high-performing peers: Agility metrics 30x more frequent code deployments 8,000x faster lead time than their peers Reliability metrics 2x the change success rate 12x faster MTTR In other words, they were more agile: they were deploying code 30x more frequently, and the lead time required to go from \"code committed\" to \"successfully running in production\" was completed 8,000x faster — high performers had lead times measured in minutes or hours, while lower performers had lead times measured in weeks, months or even quarters. Not only were the high performers doing more work, but they had far better outcomes: when the high performers deployed changes and code, they were twice as likely to be completed successfully (i.e., without causing a production outage or service impairment), and when the change failed and resulted in an incident, the time required to resolve the incident was 12x faster. We were astonished and delighted with this finding, as it showed not only that it was possible to break the core, chronic conflict, but that it seemed to confirm that just as in manufacturing, agility and reliability go hand in hand. In other words, lead time correlates with both both agility and reliability. (Gene will write more on his personal interpretations of the 2012 DevOps Survey Of Practice in a future post.) Conclusion We hope this gives you a good idea of why we've worked so hard on the 2012 and 2013 DevOps Survey, as well as how to conduct your own cross-population studies. Please let us know if you have any questions or if there's anything we can do for you. And of course, help us understand what in DevOps and Continuous Delivery work by taking 10 minutes to participate in the 2013 Puppet Labs DevOps Survey here by January 15, 2014 ! Thank you! –Gene Kim and Jez Humble"},{"title":"FlowCon 2013 Wrap-Up, With Some Hard Data on Gender Diversity in Tech Conferences.","tags":"ciandcd","url":"http://ciandcd.github.io/flowcon-2013-wrap-up-with-some-hard-data-on-gender-diversity-in-tech-conferences.html","text":"From: http://continuousdelivery.com/2013/12/flowcon-2013-wrap-up/ FlowCon 2013 Wrap-Up, With Some Hard Data on Gender Diversity in Tech Conferences. Thanks to all of you who came along to FlowCon! If you weren't able to make it, you can watch the videos for free thanks to BMC and ThoughtWorks Studios . The slides are also available for downloading. Let me first express my thanks to our producers: Geeta Schmidt and Niley Barros of Trifork and Rebecca Phillips of ThoughtWorks Studios . I also want to thank my fellow PC members Lane Halley , Elisabeth Hendrickson , Gene Kim and John Esser ; our fabulous speakers ; our generous sponsors ; and everyone who came along. The Program The goal of the program committee was to create a conference that represents our industry as we want it to look, not as it is right now. That's an ambitious goal that involves changing the way we think about everything from leadership and governance through product development and design , to IT operations . Not only did our speakers cover all these topics; they also provided real examples of how these changes, along with the cultural changes necessary to support them, have been achieved at enterprise scale. Thus we attacked one of the main objections we hear time and time again — \"that sounds great, but it couldn't work here\". Part of our vision was to provide a platform for people to speak about gnarly, real-life examples that demonstrate that, with sufficient hard work and ingenuity, ideas like continuous delivery, devops, and lean product development can provide significant competitive advantage through higher quality, cost savings, and happier customers, even in traditionally slow-moving and highly-regulated industries with large, complex, heterogeneous systems. Two talks that I am particularly happy to have on record are Gary Gruver's talk on doing continuous delivery for printer firmware at HP, and John Kordyback's talk on doing continuous delivery with mainframes in the financial services industry. Alternatively, if you want a vision of the state of the art of continuous delivery, it would be hard to beat Adrian Cockcroft's opening keynote (the most highly rated talk of the conference) on how Netflix approach building and running systems. Overall, both the individual quality of the talks and the vision they present in concert was incredibly inspiring. Gene Kim comments, \"The FlowCon program was amazing. In my mind, what was presented at FlowCon is what every IT practitioner will be required to know in 10 years time.\" Thank you again to all of our speakers. Data on Gender Diversity Part of representing the industry as we want it to look is changing its composition. Thus another personal goal for me was to gather data to support my hypothesis that taking steps to increase diversity at conferences doesn't mean reducing quality. FlowCon, like the excellent GOTO conferences that Trifork produces, records feedback from participants. Everybody leaving a session can give feedback on whether they thought the talk was good, mediocre or poor by tapping a red, amber or green rectangle on an iPhone on their way out. We then calculate overall satisfaction as follows: satisfaction = (green votes) / (total votes). When we got back all the data, the first thing I did is look at the average (mean) satisfaction for male speakers versus female speakers. It turns out that in both cases the average is between 71% and 72%. First of all, this demonstrates that there was no statistically significant difference in satisfaction between male and female speakers. This is important because it means our steps to increase diversity — including reaching out to a wide network to ensure that 50% of our invited speakers were women — didn't \"lower the bar\". There is also a deeper implication: any claim that the all-white-male conference programs that are so depressingly common in the tech industry are the result of some meritocratic process is BS. They are, rather, the result of not putting in enough effort to seek out high quality speakers from historically discriminated against groups. If our industry were truly meritocratic, the speaker line-up and attendees would resemble the wider population, because we know that there is no biological explanation for the overwhelming proportion of white dudes in our industry. So let's not fool ourselves any more with claims that taking steps to improve diversity is \"reverse discrimination\". Any time we don't take concrete, systematic steps forward we are silently complicit in perpetuating the status quo — which is why it's not good enough when leaders in the tech community ignore the problem. If you ignore the problem, you're part of the problem. Finally, I want to emphasize that what the program committee achieved was not very hard, once we spent some time thinking the problem through, and also that it was insufficient. We had a reasonable level of gender diversity, but the speakers were still overwhelmingly white. I don't have data for the diversity of our audience, but based on observation, there were more white guys than I would see if I walked out of the door onto the streets (and this is in San Francisco, which is far from being representative of the wider population). If you want to educate yourself further on these issues, I suggest watching Ashe Dryden's talk on programming diversity. And if you'd like to become more effective at creating change, check out Linda Rising's closing keynote . Here's to taking small steps every day to make 2014 a marginally, incrementally, better year than 2013 ."},{"title":"How To Create A More Diverse Tech Conference","tags":"ciandcd","url":"http://ciandcd.github.io/how-to-create-a-more-diverse-tech-conference.html","text":"From: http://continuousdelivery.com/2013/09/how-we-got-40-female-speakers-at-flowcon/ How To Create A More Diverse Tech Conference I have been advised by people I trust that it's not a good idea to talk about how you got serious female representation at your conference until after it's over. However the shameful RubyConf \"binders full of men\" debacle and the Neanderthal level of discussion around it has wound me up enough to write this account somewhat prematurely. So here is how we achieved >40% female representation on our speaker roster at FlowCon . Step 0. Care About The Outcome. When John Esser approached me to put together a conference about continuous delivery, devops and lean product development, I thought carefully about it. I've helped put together a conference program before ( QCon SF 2012 ), and that was pretty hard work, so I wanted to be sure I had the correct motivation. One of the things that I have always disliked about tech conferences is being surrounded by a bunch of other straight white guys (nothing personal, some of my best friends are straight white guys). It's a constant reminder of the fact that, due to a number of socioeconomic factors, straight white guys have it easier than others . I wanted to put together a conference which reflects my community as I would like it to look, not as it actually looks. So one of the four values the FlowCon program committee came up with was this: \"Diversity: We believe the technology community – and thus the conference speakers and participants – should reflect the demographics of our customers and the wider world.\" There are two reasons for this. Firstly, we can't effectively change the world through technology without diversity. To find out why, come and see Ashe Dryden talk about how \"diverse communities and workplaces create better products\" . Second, one of the main reasons I like working at ThoughtWorks is that one of the three pillars of our mission is to \"advocate passionately for social and economic justice.\" The fact there are so few women in IT reflects social and economic injustice inherent in our world. Making sure you actually have a mission for your conference is something I learned from helping out with QCon SF . It is a constant reminder of why you're doing it and what's important about it. If you don't have a mission, you're at the mercy of the implicit biases of the organizers. As RubyConf shows, you can't just throw in the \"one weird trick\" of anonymous submissions and expect that it will somehow solve the problem. Everybody on the program committee actually has to care about the outcome, or they won't put in the right amount of work to make it happen. Once you do that, the rest of the steps aren't that hard. Step 1. Make Sure Your Program Committee Is Aligned With Your Mission Once I had an idea about the mission of the conference, I reached out to some people whom I thought would share it. I was lucky enough that Elizabeth Hendrickson , Lane Halley and Gene Kim agreed to join John Esser and me on the program committee. One of the main reasons I asked those particular people, apart from being extremely competent and well-respected in their field, was another conference goal: \"Spanning boundaries: We believe that the best products are created collaboratively by people with a range of skills and experiences.\" The program committee has representation from the UX, testing, operations, product development and programming communities. Step 2. Make Sure Your Invited Speakers Are Aligned With Your Mission. We made the decision to have about half the program be invited speakers. Part of that was about ensuring that we had a solid core program. But it was also a chance for us to put our mission into practice, so that when we put out the call for proposals we had a bunch of confirmed speakers who demonstrated we were serious about our mission. Thus we made sure that the invited speakers were respected boundary spanners, and that 50% of them were women. This involved more work than we would have had to put in had we just invited our friends (a popular strategy for program committees). It was also telling that we got more refusals from women than we got from men due to schedule conflicts. The main factor here was that female speakers are actually in greater demand than men because there are relatively fewer of them. Step 3. The Anonymous Call For Proposals If you jump straight to step 3, it's likely you will suffer the fate of RubyConf and fail. If you use this as your only strategy for increasing representation it won't work. This strategy has been thoroughly discussed by others who have used this approach as part of increasing diversity at their conference. We created a form in Google Docs for people to propose talks. They had to enter their email address, but we mentioned in the form that they should use one that didn't identify them if they wanted their proposal to be more anonymous. Of the 82 people who submitted a talk proposal, 18 (21%) were women as far as we can work out (once the program was confirmed I used Rapportive to reverse-engineer email addresses based on publicly available information). Ultimately, three of the eight people who made it into the final program based on submitted proposals were women. The low female representation through the CFP is the reason our program isn't 50% female. Even getting the 21% of submissions that we did involved reaching out through mailing lists, Twitter, and our networks to encourage women to submit. This step, along with making it clear that you actually care, is essential if you in fact expect women to submit through the anonymous CFP. Observations These four steps resulted in 10 of our 24 speakers being women . I have three main observations coming out of this process: First, unlike increasing the number of women who take programming classes in school or enter the IT industry and don't immediately quit in horror, creating a conference with reasonable female representation is not actually a hard problem. Yes, we put in more work to achieve this goal than we would have had we not cared. But it wasn't significantly more. Conference organizers who claim to care but fail to achieve good representation should quit whining and take real steps to achieve this goal. The community should hold them to higher standards. If the conference speakers are a bunch of straight white guys, the only reason is that the organizers didn't care enough. Second, in the wake of RubyConf, I have been angered but unsurprised to observe the usual chorus about how increasing representation somehow means lowering standards. Not only is this incredibly insulting to the many extraordinary women working in our industry, but it is just false. I dare anyone to look at the kick-ass program we have put together for FlowCon and try and claim that we have somehow lowered standards to achieve great a barely acceptable level of representation. Another thing you will hear is that it is harder to find female speakers on \"hard\" topics such as programming than for \"soft\" ones. I find this claim baffling because in my experience changing organizational culture (considered a \"soft\" topic) is, in my experience, way way harder than knocking out lines of code (even well-factored unit-tested ones). But you'll see on our program that women are covering the whole gamut from organizational change to refactoring to configuration management . Third, it's not all good news. In particular, we have only one non-white speaker. I'll hold my hand up on this – we didn't explicitly set non-white representation as a goal within the program committee, and by the time it became obvious it was a problem (Step 3) it was too late to do anything. This demonstrates why steps 0-2 are important. If we run FlowCon again, we will do better. Meanwhile check out the program , and follow this link to register with a 10% discount. If you need more than a one day conference to come to San Francisco, Balanced Team are running their conference the following two days. End notes Another popular silencing tactic in this discussion is that bringing attention to the level of diversity in a conference is in itself a form of sexism or racism. There's a cartoon on the left which expresses nicely why this is in fact horribly misguided (or you could check out one of the many excellent articles on \"colourblindness\" and racism ). Check out the Geek Feminism blog and wiki for tons of useful information and advice on making things better for women in tech. Also check out the @CallbackWomen and @DevChix Twitter accounts to spread the word for your CFP. Ashe Dryden also wrote an excellent post on creating more diverse conferences. Another important factor when designing a woman-friendly conference is to create an anti-harassment policy. Check out this account of a woman who actually needed to use the anti-harassment policy (trigger alert). UPDATE Of course, this entry is now starting to receive the attention of anonymous trolls. I've left the first one as an example of the idiocy that passes for dialogue in this debate (and from supposedly smart people at that). But forthwith I'll be deleting anonymous or otherwise uncivil posts."},{"title":"Risk Management Theatre: On Show At An Organization Near You","tags":"ciandcd","url":"http://ciandcd.github.io/risk-management-theatre-on-show-at-an-organization-near-you.html","text":"From: http://continuousdelivery.com/2013/08/risk-management-theatre/ How To Create A More Diverse Tech Conference Videos from the Continuous Delivery track at QCon SF 2012 » Risk Management Theatre: On Show At An Organization Near You Translations: 한국말 One of the concepts that will feature in the new book I am working on is \"risk management theatre\". This is the name I coined for the commonly-encountered control apparatus, imposed in a top-down way, which makes life painful for the innocent but can be circumvented by the guilty (the name comes by analogy with security theatre .) Risk management theatre is the outcome of optimizing processes for the case that somebody will do something stupid or bad, because (to quote Bjarte Bogsnes talking about management ), \"there might be someone who who cannot be trusted. The strategy seems to be preventative control on everybody instead of damage control on those few.\" Unfortunately risk management theatre is everywhere in large organizations, and reflects the continuing dominance of the Theory X management paradigm. The alternative to the top-down control approach is what I have called adaptive risk management, informed by human-centred management theories (for example the work of Ohno , Deming , Drucker, Denning and Dweck ) and the study of how complex systems behave, particularly when they drift into failure . Adaptive risk management is based on systems thinking, transparency, experimentation, and fast feedback loops. Here are some examples of the differences between the two approaches. Adaptive risk management (people work to detect problems through improving transparency and feedback, and solve them through improvisation and experimentation) Risk management theatre (management imposes controls and processes which make life painful for the innocent but can be circumvented by the guilty) Continuous code review in which engineers ask a colleague to look over their changes before check-in, technical leads review all check-ins made by their team, and code review tools allow people to comment on each others' work once it is in trunk. Mandatory code review enforced by check-in gates where a tool requires changes to be signed off by somebody else before they can be merged into trunk. This is inefficient and delays feedback on non-trivial regressions (including performance regressions). Fast, automated unit and acceptance tests which inform engineers within minutes (for unit tests) or tens of minutes (for acceptance tests) if they have introduced a known regression into trunk, and which can be run on workstations before commit. Manual testing as a precondition for integration, especially when performed by a different team or in a different location. Like mandatory code review, this delays feedback on the effect of the change on the system as a whole. A deployment pipeline which provides complete traceability of all changes from check-in to release, and which detects and rejects risky changes automatically through a combination of automated tests and manual validations. A comprehensive documentation trail so that in the event of a failure we can discover the human error that is the root cause of failures in the mechanistic, Cartesian paradigm that applies in the domain of systems that are not complex . Situational awareness created through tools which make it easy to monitor, analyze and correlate relevant data. This includes process, business and systems level metrics as well as the discussion threads around events. Segregation of duties which acts as a barrier to knowledge sharing, feedback and collaboration, and reduces the situational awareness which is essential to an effective response in the event of an incident. It's important to emphasize that there are circumstances in which the countermeasures on the right are appropriate. If your delivery and operational processes are chaotic and undisciplined, imposing controls can be an effective way to improve – so long as we understand they are a temporary countermeasure rather than an end in themselves, and provided they are applied with the consent of the people who must work within them. Here are some differences between the two approaches in the field of IT: Adaptive risk management (people work to detect problems through improving transparency and feedback, and solve them through improvisation and experimentation) Risk management theatre (management imposes controls and processes which make life painful for the innocent but can be circumvented by the guilty) Principle-based and dynamic: principles can be applied to situations that were not envisaged when the principles were created. Rule-based and static : when we encounter new technologies and processes (for example, cloud computing) we need to rewrite the rules. Uses transparency to prevent accidents and bad behaviour. When it's easy for anybody to see what anybody else is doing, people are more careful. As Louis Brandeis said, \"Publicity is justly commended as a remedy for social and industrial diseases. Sunlight is said to be the best of disinfectants; electric light the most efficient policeman.\" Uses controls to prevent accidents and bad behaviour. This approach is the default for legislators as a way to prove they have taken action in response to a disaster. But controls limit our ability to adapt quickly to unexpected problems. This introduces a new class of risks, for example over-reliance on emergency change processes because the standard change process is too slow and bureaucratic. Accepts that systems drift into failure. Our systems and the environment are constantly changing, and there will never be sufficient information to make globally rational decisions. Humans solve our problems and we must rely on them to make judgement calls. Assumes humans are the problem. If people always follow the processes correctly, nothing bad can happen. Controls are put in place to manage \"bad apples\". Ignores the fact that process specifications always require interpretation and adaptation in reality. Rewards people for collaboration, experimentation, and system-level improvements. People collaborate to improve system-level metrics such as lead time and time to restore service. No rewards for \"productivity\" on individual or function level. Accepts that locally rational decisions can lead to system level failures. Rewards people based on personal \"productivity\" and local optimization . For example operations people optimizing for stability at the expense of throughput, or developers optimizing for velocity at the expense of quality (even though these are false dichotomies.) Creates a culture of continuous learning and experimentation : People openly discuss mistakes to learn from them and conduct blameless post-mortems after outages or customer service problems with the goal of improving the system. People are encouraged to try things out and experiment (with the expectations that many hypotheses will be invalidated) in order to get better. Creates a culture of fear and mistrust . Encourages finger pointing and lack of ownership for errors, omissions and failure to get things done. As in: If I don't do anything unless someone tells me to, I won't be held responsible for any resulting failure. Failures are a learning opportunity . They occur in controlled circumstances, their effects are appropriately mitigated, and they are encouraged as an opportunity to learn how to improve. Failures are caused by human error (usually a failure to follow some process correctly), and the primary response is to find the person responsible and punish them, and then use further controls and processes as the main strategy to prevent future problems. Risk management theatre is not just painful and a barrier to the adoption of continuous delivery (and indeed to continuous improvement in general). It is actually dangerous, primarily because it creates a culture of fear and mistrust. As Bogsnes says, \"if the entire management model reeks of mistrust and control mechanisms against unwanted behavior, the result might actually be more, not less, of what we try to prevent. The more people are treated as criminals, the more we risk that they will behave as such.\" This kind of organizational culture is a major factor whenever we see people who are scared of losing their jobs, or engage in activities designed to protect themselves in the case that something goes wrong, or attempt to make themselves indispensable through hoarding information. I'm certainly not suggesting that controls, IT governance frameworks, and oversight are bad in and of themselves. Indeed, applied correctly, they are essential for effective risk management. ITIL for example allows for a lightweight change management process that is completely compatible with an adaptive approach to risk management. What's decisive is how these framework are implemented. The way such frameworks are used and applied is determined by—and perpetuates— organizational culture ."},{"title":"Videos from the Continuous Delivery track at QCon SF 2012","tags":"ciandcd","url":"http://ciandcd.github.io/videos-from-the-continuous-delivery-track-at-qcon-sf-2012.html","text":"From: http://continuousdelivery.com/2013/05/videos-from-the-continuous-delivery-track-at-qcon-sf-2012/ Videos from the Continuous Delivery track at QCon SF 2012 At last year's QCon San Francisco I got to curate a track on continuous delivery. One of the goals of the QCon conferences is \"information Robin Hood\" – finding ways to get out into public the secret sauce of high performing organizations. So I set out to find talks that would answer the questions I frequently get asked: can continuous integration, automated testing, and trunk-based development scale? How does continuous delivery affect the way we do product management? What's the business case for continuous delivery? How do you grow a culture that enables it? You'll find the all these questions answered in the talks below, from the leaders who have been at the forefront of continuous delivery at Amazon, Facebook, Google and Etsy. They also discuss the tools they built and the and practices they use to enable continuous delivery. Finally, you get me talking about how you can adopt continuous delivery at your organization. Thanks so much to Jesse Robbins, Frank Harris, Nell Thomas, John Penix and Chuck Rossi for these great talks, and to the folks behind QCon SF for an awesome conference. Jesse Robbins ran ops at Amazon before quitting to co-found Opscode (creators of Chef ). He is also co-founder of Velocity . In his copious spare time, he's a volunteer firefighter. Basically, Jesse is an enormous over-achiever. This is a fabulous – and hilarious – talk that discusses the hardest part of implementing continuous delivery: cultural change. This talk features my favourite devops aphorism: One of the main goals of continuous delivery is to get fast feedback on your hypotheses so you can build the right thing. In this talk Frank Harris and Nell Thomas of Etsy show off a bunch of their tools, including the A/B testing framework they built for running experiments (which uses feature toggles under the hood). They give an example of an experiment they're running right now, and discuss how the ability to gather and analyze data on customer behaviour in real time (see screenshot below) affects the way they do product development. In this talk, John Penix of Google shows off the awesome product he and his team built for continuous integration and cloud-based testing at Google. Teams at Google are free to choose their own development practices and toolchain, but this one has a pretty high uptake. When people ask me if trunk-based development and continuous integration can scale, I like to show them the following slide: In addition to discussing the process he uses to release twice a day, Facebook's lead release engineer Chuck Rossi shows off the extensive toolchain they built to deploy at scale. Highlights include Gatekeeper (screenshot below), which manages who gets to see which features as part of their dark launching process, and their deploy tool which categorizes all proposed patches based on the size of the patch, the amount of discussion around it, and the \"push karma\" of the committers. Amazon, Etsy, Google and Facebook are all primarily software development shops which command enormous amounts of resources. They are, to use Christopher Little's metaphor, unicorns. How can the rest of us adopt continuous delivery? That's the subject of my talk, which describes four case studies of organizations that adopted continuous delivery, with varying degrees of success. One of my favourites – partly because it's embedded software, not a website – is the story of HP's LaserJet Firmware team, who re-architected their software around the principles of continuous delivery. People always want to know the business case for continuous delivery: the FutureSmart team provide one in the book they wrote that discusses how they did it:"},{"title":"Announcing FlowCon","tags":"ciandcd","url":"http://ciandcd.github.io/announcing-flowcon.html","text":"from:http://continuousdelivery.com/2013/05/announcing-flowcon/ Announcing FlowCon I spend quite a lot of time at conferences, and it consistently bothers me that they are so often focused on one particular function: development, testing, UX, systems administration. The point of continuous delivery is to accelerate the rate at which we can learn from each other – and from our customers. That requires everyone involved in the delivery process (including users, product owners and entrepreneurs) to collaborate throughout. So why isn't there a conference which focuses on flow – the emergent property of great teams? So I got together with a bunch of like-minded folks – Elisabeth Hendrickson , Gene Kim , John Esser and Lane Halley – and now there is a conference about creating flow: FlowCon . It's on Friday November 1 in San Francisco , and it's produced by ThoughtWorks and Trifork (creators of the GOTO conferences ). The conference is based around four values: Learning : Our goal is to provide the best possible conference forum for practitioners to learn from each other how to build great products and services. Open Information : We aim to uncover how great products and services are built in real life and make this information freely available to the widest audience possible. Diversity : We believe the technology community – and thus the conference speakers and participants – should reflect the demographics of our customers and the wider world. Spanning boundaries : We believe that the best products and services are created collaboratively by people with a range of skills and experiences. We have put together nearly half of the program , and we're delighted to announce that Adrian Cockcroft , Catherine Courage , Jeff Gothelf and Linda Rising will be giving keynotes. The program is still a work in process (a minimum viable product, if you will). In particular, the after lunch sessions are empty – for a good reason: we want you to speak in those slots . We're looking for people working to create flow in their organization – especially those who: Span multiple roles and work across organizational silos. Work in any of the following areas: a highly regulated environment; a large, traditional enterprise; in the pursuit of social and economic justice. Are willing to share obstacles encountered or mistakes made and how you overcame them – whether cultural or technological. Offer actionable advice \"the rest of us\" can apply today (even if we don't have the resources of Etsy / Amazon / Google). Your talk could be about culture, technology, design, process – the only really important criterion is that it draws on what you've learned about helping to create flow in your organization. If that sounds like you, please submit your proposal . If you know someone who would do a great job, please encourage them to submit. Our submission process is designed to be entirely merit-based, which means that the first round is anonymous. The deadline is midnight Pacific time, Sunday June 23, 2013. Tickets for the conference are now on sale – at $350 if you register before July 31, or $500 if you register afterwards. Whatever your role or domain, you're sure to find inspirational, disruptive thinking that will make you better at creating great products and services. I hope to see you there!"},{"title":"Continuous Delivery","tags":"ciandcd","url":"http://ciandcd.github.io/continuous-delivery.html","text":"From: http://continuousdelivery.com/2013/05/announcing-flowcon/ Announcing FlowCon I spend quite a lot of time at conferences, and it consistently bothers me that they are so often focused on one particular function: development, testing, UX, systems administration. The point of continuous delivery is to accelerate the rate at which we can learn from each other – and from our customers. That requires everyone involved in the delivery process (including users, product owners and entrepreneurs) to collaborate throughout. So why isn't there a conference which focuses on flow – the emergent property of great teams? So I got together with a bunch of like-minded folks – Elisabeth Hendrickson , Gene Kim , John Esser and Lane Halley – and now there is a conference about creating flow: FlowCon . It's on Friday November 1 in San Francisco , and it's produced by ThoughtWorks and Trifork (creators of the GOTO conferences ). The conference is based around four values: Learning : Our goal is to provide the best possible conference forum for practitioners to learn from each other how to build great products and services. Open Information : We aim to uncover how great products and services are built in real life and make this information freely available to the widest audience possible. Diversity : We believe the technology community – and thus the conference speakers and participants – should reflect the demographics of our customers and the wider world. Spanning boundaries : We believe that the best products and services are created collaboratively by people with a range of skills and experiences. We have put together nearly half of the program , and we're delighted to announce that Adrian Cockcroft , Catherine Courage , Jeff Gothelf and Linda Rising will be giving keynotes. The program is still a work in process (a minimum viable product, if you will). In particular, the after lunch sessions are empty – for a good reason: we want you to speak in those slots . We're looking for people working to create flow in their organization – especially those who: Span multiple roles and work across organizational silos. Work in any of the following areas: a highly regulated environment; a large, traditional enterprise; in the pursuit of social and economic justice. Are willing to share obstacles encountered or mistakes made and how you overcame them – whether cultural or technological. Offer actionable advice \"the rest of us\" can apply today (even if we don't have the resources of Etsy / Amazon / Google). Your talk could be about culture, technology, design, process – the only really important criterion is that it draws on what you've learned about helping to create flow in your organization. If that sounds like you, please submit your proposal . If you know someone who would do a great job, please encourage them to submit. Our submission process is designed to be entirely merit-based, which means that the first round is anonymous. The deadline is midnight Pacific time, Sunday June 23, 2013. Tickets for the conference are now on sale – at $350 if you register before July 31, or $500 if you register afterwards. Whatever your role or domain, you're sure to find inspirational, disruptive thinking that will make you better at creating great products and services. I hope to see you there!"},{"title":"Book Review: The Phoenix Project","tags":"ciandcd","url":"http://ciandcd.github.io/book-review-the-phoenix-project.html","text":"From: http://continuousdelivery.com/2013/01/book-review-the-phoenix-project/ Book Review: The Phoenix Project I am not going to do a ton of book reviews on this blog (I have one more planned for next month). I'll only bother posting reviews of books that I believe are both excellent and relevant to Continuous Delivery . This book easily satisfies both criteria. Full disclosure: Gene gave me a draft of this book for free for reviewing purposes. You've probably heard of Gene Kim, Kevin Behr and George Spafford before. They are the three amigos responsible for The Visible Ops Handbook , which can be found in the book pile of every good IT operator. Their new book, The Phoenix Project: A Novel About IT, DevOps, and Helping Your Business Win , follows the format of Eliyahu Goldratt's classic, The Goal . Told from the perspective of newly-minted VP of IT Operations Bill Palmer, it describes the turnaround of failing auto parts company Parts Unlimited. This is to be achieved through the delivery of the eponymous Phoenix Project, a classic \"too big to fail\" software project designed to build a system which will revive the fortunes of the company. To quote (p51): The plot is simple: First, you take an urgent date-driven project, where the shipment date cannot be delayed because of external commitments made to Wall Street or customers. Then you add a bunch of developers who use up all the time in the schedule, leaving no time for testing or operations deployment. And because no one is willing to slip the deployment date, everyone after Development has to take outrageous and unacceptable shortcuts to hit the date. The results are never pretty. Usually, the software product is so unstable and unusable that even the people who were screaming for it end up saying that it's not worth shipping. And it's always IT Operations who still has to stay up all night, rebooting servers hourly to compensate for crappy code, doing whatever heroics are required to hide from the rest of the world just how bad things really are. Part One of the book describes in loving detail the enormous clusterfuck pie that is baked from these ingredients. The pie is spiced with an internal Sarbanes-Oxley audit which reveals 952 control deficiencies, an outage of the payroll processing system, and various other problems that conspire to deepen the woe of the operations group, all of which are clearly drawn from the deep well of the authors' real-life experiences. Apart from the main characters – our hero Bill, his boss Steve, and the evil villain Sarah – The Phoenix Project features a delightful rogues' gallery which anyone working in an enterprise will recognize: Brent Geller, the boy wonder whose encyclopedic knowledge of the company's Byzantine IT systems means that his involvement is necessary to get anything done. Patty McKee, the Director of Support who runs a change management process so bureaucratic that everybody bypasses it. John Pesche, the black binder wielding Chief Information Security Officer whose constant meddling under the guise of improving security has turned him into a pariah. The second part of the book details how the IT group is reborn from the ashes of the Phoenix Project into a high-performing organization that is a strategic partner to the business. This is achieved through the application of a heavy dose of lean thinking (including continuous delivery ) administered by Erik, a mercurial IT and manufacturing guru Steve is courting to join the board. The book does an excellent job of showing – as well as telling – how to apply the concepts (and the effect of doing so) in an enterprise with plenty of technical debt. Perhaps the most eyebrow-raising part of this section is the way in which John has his soul mercilessly crushed to the point where he goes on a multi-day drinking spree before he is rehabilitated towards the end of the book (he is a phoenix too). John's narrative arc is just one example of how the book also succeeds as a novel. It's gripping, with moments of drama and high emotion, as well as some great one-liners. There was even one point when I teared up (bear in mind that I also cried during Forrest Gump – unlike the book's central characters, I did not serve in the armed forces). Nobody who has read The Goal will miss The Phoenix Project's similarity in terms of style and plot. Perhaps my favourite thing about the book's pedagogical style is the way Erik (like Jonah in The Goal) uses the Socratic Method to give Bill the tools to solve his problems by himself. Of course this learning process is fictional, but it means you get to see Bill struggling with the questions and trying things out. It remains to be seen whether readers of the book will be able to apply these techniques as successfully as Bill without a real Erik to guide them. But of course, this is a limitation of any book. If I had one criticism it's that unlike real life, there aren't many experiments in the book that end up making things worse, and it's this process of failing fast, learning from your failures, and coming up with new experiments that is instrumental to a real learning culture. One important point worth noting if you are working in an organization like Parts Unlimited is this: the IT department's rebirth is only possible because of the Titanic proportions of the disaster that unfolds in Part One. For management to truly embrace change, a compelling event or a teachable moment (i.e. a Charlie Foxtrot) is required. Unless your organization faces the same existential threat that Parts Unlimited does, you'll have a much harder time convincing people they should adopt the tools described in the book. Overall, The Phoenix Project is a fantastic read. It's entertaining, cathartic, inspirational and informative. If, like me, you have an enormous backlog of books (and more work in process than you'd like) I suggest giving yourself a break and putting this one to the top of your list. It'll only take you a day or two, and despite its conceptual density it will leave you feeling refreshed and energized with a bunch of new ideas to try out. The Phoenix Project deserves to be read by everyone who works in – or with – IT."},{"title":"On Antifragility in Systems and Organizational Architecture","tags":"ciandcd","url":"http://ciandcd.github.io/on-antifragility-in-systems-and-organizational-architecture.html","text":"From: http://continuousdelivery.com/2013/01/on-antifragility-in-systems-and-organizational-architecture/ On Antifragility in Systems and Organizational Architecture In his new book, Antifragile , Nassim Taleb discusses the behaviour of complex systems and distinguishes three kinds: those that are fragile, those that are robust or resilient, and those that are antifragile. These types of systems differ in how they respond to volatility: \"The fragile wants tranquility, the antifragile grows from disorder, and the robust doesn't care too much.\" (p20) Taleb argues that we want to create systems that are antifragile – that are designed to take advantage of volatility. I think this concept is incredibly powerful when applied to systems and organizational architecture. Why Continuous Delivery Works Taleb shows why the traditional approach of operations – making change hard, since change is risky – is flawed: \"the problem with artificially suppressed volatility is not just that the system tends to become extremely fragile; it is that, at the same time, it exhibits no visible risks… These artificially constrained systems become prone to Black Swans. Such environments eventually experience massive blowups… catching everyone off guard and undoing years of stability or, in almost all cases, ending up far worse than they were in their initial volatile state\" (p105) 1 . This a great explanation of how many attempts to manage risk actually result in risk management theatre – giving the appearance of effective risk management while actually making the system (and the organization) extremely fragile to unexpected events. It also explains why continuous delivery works. The most important heuristic we describe in the book is \"if it hurts, do it more often, and bring the pain forward.\" The effect of following this principle is to exert a constant stress on your delivery and deployment process to reduce its fragility so that releasing becomes a boring, low-risk activity. Antifragile Systems Another of Taleb's key claims is that it is impossible to predict \"Black Swan\" events: \"you cannot say with any reliability that a certain remote event or shock is more likely than another… but you can state with a lot more confidence that an object or a structure is more fragile than another should a certain event happen.\" (p8). Thus we need \"to switch the blame from the inability to see an event coming… to the failure to understand (anti)fragility, namely, ‘why did we build something so fragile to these types of events?'\" (p136). Unlike risk, fragility is actually measurable. How do we measure the fragility of the systems we build? We try to break them, using techniques such as game days and systems like chaos monkey . The systematic application of stress to your systems is essential – not just to ensure your systems are antifragile, but to develop the muscles of the people who create and maintain them through constant practice. After all, it's the combination of the system and the people who build and run it that has the quality of antifragility. In this context, an important quality of legacy systems is their fragility. Legacy systems that aren't touched for a long time will turn into fragile \"works of art\": changing them is considered risky, the number of people who understand the system decreases with time, and their knowledge atrophies from lack of exercise. How do we create antifragile systems? Apply stress to them continuously so we are forced to simplify, homogenise, and automate. Antifragile Organizations We can measure the fragility of an organization by how long it takes before it liquidates its assets. Deloitte's Shift Index shows that the average life expectancy of a Fortune 500 company has declined from around 75 years half a century ago to less than 15 years today. Start-ups are notoriously fragile. But the ones that survive and grow turn into something potentially more dangerous – robust organizations. The problem with robust organizations is that they resist change. They aren't quickly killed by changes to their environment, but they don't adapt to them either – they die slowly. We see this effect all the time – changing the culture of an established organization is incredibly hard. Antifragile organizations are those that have a culture that enables them to learn fast from their environment and adapt to it so they can take advantage of volatility. Here are some characteristics of antifragile organizations: Systems thinking. Everybody in the organization knows the goals of the organization and makes sure their work is directly contributing towards these goals. Theory Y Management. Management needs to assume employees are self-motivated and will be able to learn how to solve problems themselves. Organizations need to make sure they hire antifragile people who will thrive in this environment. As Daniel Pink's Drive points out, giving your employees autonomy, purpose, and the opportunity to learn and master new skills is what stops them from quitting, thus increasing the antifragility of your organization. Continuous experimentation. As described in Toyota Kata , good management knows that the best solutions come from the workers. They create an environment in which practitioners are able to run experiments to learn as rapidly as possible. The feedback loops in command and control organizations are too slow for them to adapt effectively. Disruptive product development. Antifragile organizations aren't content with stress generated by their environment. Like humans exercising, they also try and disrupt themselves (the organizational equivalent of a game day). For example, Amazon cannibalized its own business , creating the Amazon Marketplace and the Kindle. Apple is cannibalizing its Mac business with the iPad. Fragile organizations resist disrupting their own product lines, as Toshiba did at first with flash memory . If you do a good job at this you never need to worry about the competition – you'll always beat them to it. Fragility and Agility As Taleb points out, \"antifragility is desirable in general, but not always, as there are cases in which antifragility will be costly, extremely so. Further, it is hard to consider robustness as always desirable—to quote Nietzsche, one can die from being immortal.\" (p22) Of course working out where on the spectrum you want your systems and your organization to lie is an art, and the great artists are those that know how to build systems, organizations, and products simply, quickly and cheaply so that they are antifragile with respect to our biggest enemy: time. How do they do that? Using the same heuristics described in \"antifragile organizations\", above, which closely mirror the Three Ways of Devops . As I read Antifragile , it reminded me of something I read a number of years ago: Kent Beck and Cynthia Andres' Extreme Programming Explained . The subtitle? Embrace Change. It strikes me that the concept of antifragile is what we were aiming for with agile the whole time: building systems (including human systems – organizations) that benefit from volatility. Endnotes Thanks to Badrinath Janakiraman for feedback on an earlier draft of this post. 1 He is talking about financial markets, which are rather less fragile than IT systems, hence his rather generous \"years of stability\""}]}